### YamlMime:ModuleUnit
uid: learn.perform-basic-data-transformation-in-azure-databricks.4-knowledge-check
title: Knowledge check
metadata:
  title: Knowledge check
  description: Test your knowledge by answering questions about skills you learned from the lab.
  ms.date: 02/20/2019
  author: barlanmsft
  ms.author: barlan
  ms.topic: interactive-tutorial
  ms.prod: learning-azure
durationInMinutes: 5
content: |
  [!include[](includes/4-knowledge-check.md)]
quiz:
  questions:
    - content: 'How do you specify parameters when reading data?'
      choices:
        - content: Using .option() during your read allows you to pass key/value pairs specifying aspects of your read
          explanation: Using .option() during your read allows you to pass key/value pairs specifying aspects of your read. For instance, options for reading CSV data include header, delimiter, and inferSchema.
          isCorrect: true
        - content: Using .parameter() during your read allows you to pass key/value pairs specifying aspects of your read
          isCorrect: false
          explanation: Incorrect method name
        - content: Using .keys() during your read allows you to pass key/value pairs specifying aspects of your read
          isCorrect: false
          explanation: Incorrect method name
    - content: 'How do you connect your Spark cluster to the Azure Blob?'
      choices:
        - content: By mounting it
          explanation: Mounts require Azure credentials such as SAS keys and give access to a virtually infinite store for your data
          isCorrect: true
        - content: By calling the .connect() function on the Spark Cluster.
          isCorrect: false
          explanation: Incorrect, no such method
        - content: By calling the .connect() function on the Azure Blob
          isCorrect: false
          explanation: Incorrect, no such method
    - content: 'By default, how are corrupt records dealt with using spark.read.json()?'
      choices:
        - content: They appear in a column called "_corrupt_record"
          explanation: These are the records that Spark can't read (e.g. when characters are missing from a JSON string)
          isCorrect: true
        - content: They get deleted automatically
          isCorrect: false
          explanation: Incorrect, they don't get deleted.
        - content: They throw an exception and exit the read operation
          isCorrect: false
          explanation: Incorrect, no exception is thrown.
    