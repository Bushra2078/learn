After you've clearly defined what you want to know, you can evaluate the data you have and the data you might need to collect. From there, you can prepare your data in a way that supports the discovery you're interested in.

## Step 2: Gather the Data

With constraints, scopes, and prioritization of data based on subject matter expertise, you can begin to gather useful data. This step can present its own challenges. In the case of growing lettuce, you could grow ten heads of lettuce under slightly different conditions, and then determine which conditions yield the best results.

In the case of rocket launches, comparison experiments are not as easy. You can run simulations, but they are based on data. Some of that data comes from failed rocket launches that were attempted under negative circumstances. Otherwise how would you *really* know that they are negative circumstances? We can use other information (like from airplanes or basic physics) to determine constraints, but it isn't ethical or economical to undertake a test launch under each unique circumstance to be able to determine with certainty the safest circumstances. Furthermore, many conditions, like weather, can't be controlled.

## Step 2.5: Clean and manipulate the data

At first impression, you might be suspicious to learn that one step in machine learning is to "manipulate" the data. However, in this case, manipulate doesn't mean that the data is modified to get a desireable outcome. It means that the data is made to be the most accurate representation of the truth.

For example, with your lettuce garden, you might run a study that focuses on the moisture in the soil. You collect moisture readings every hour to be able to see what trends yield what results. But the moisture sensor broke on Tuesday at 2:55 PM. You noticed and fixed it before the 4:00 PM reading was taken, but you lost one sensor reading (at 3:00 PM). It's reasonable to "manipulate" the data by calculating the average of the 2:00 PM reading and the 4:00 PM reading, and then using that as the reading for missing 3:00 PM time. Now let's say your sensor went out on Tuesday at 2:55 PM, but you didn't notice until Wednesday at 2:55 PM. In this case, it might make more sense to "clean" your data by removing that day from the analysis altogether.

In the case of rocket launches, there's a lot of missing data. NASA is likely to have access to better data that is publicly available. NASA has  subject matter expertise of people involved in the nuances of rocket launches and weather. They also have access to all of their previous experiments and analyses. By contrast, you'll  rely mostly on accessible weather data (temperature, precipitation, cloudiness), and focus in on the days that launches happened. The realistic result is that this project will be less accurate than NASA's predictions. Because you only have examples of successful launches, the machine learning model you train will be skewed towards good conditions, and won't be great at identifying bad conditions.