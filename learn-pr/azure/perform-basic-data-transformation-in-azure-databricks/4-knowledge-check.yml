### YamlMime:ModuleUnit
uid: learn.perform-basic-data-transformation-in-azure-databricks.4-knowledge-check
title: Knowledge check
metadata:
  title: Knowledge check
  description: Test your knowledge by answering questions about skills you learned from the lab.
  ms.date: 02/19/2019
  author: barlanmsft
  ms.author: barlan
  ms.topic: interactive-tutorial
  ms.prod: learning-azure
durationInMinutes: 10
content: |
  [!include[](includes/4-knowledge-check.md)]
quiz:
  title: 'Knowledge Check'
  questions:
    - content: 'How do you specify parameters when reading data?'
      choices:
        - content: Using .option() during your read allows you to pass key/value pairs specifying aspects of your read
          explanation: Using .option() during your read allows you to pass key/value pairs specifying aspects of your read. For instance, options for reading CSV data include header, delimiter, and inferSchema.
          isCorrect: true
        - content: Using .parameter() during your read allows you to pass key/value pairs specifying aspects of your read
          isCorrect: false
          explanation: Incorrect method name
        - content: Using .keys() during your read allows you to pass key/value pairs specifying aspects of your read
          isCorrect: false
          explanation: Incorrect method name
    - content: 'How do you connect your Spark cluster to the Azure Blob?'
      choices:
        - content: By mounting it
          explanation: Mounts require Azure credentials such as SAS keys and give access to a virtually infinite store for your data
          isCorrect: true
        - content: By calling the .connect() function on the Spark Cluster.
          isCorrect: false
          explanation: Incorrect, no such method
        - content: By calling the .connect() function on the Azure Blob
          isCorrect: false
          explanation: Incorrect, no such method
    - content: 'How does Spark connect to databases like MySQL, Hive and other data stores?'
      choices:
        - content: JDBC
          explanation: JDBC stands for Java Database Connectivity, and is a Java API for connecting to databases such as MySQL, Hive, and other data stores.
          isCorrect: true
        - content: ODBC
          isCorrect: false
          explanation: Incorrect, ODBC is not an option
        - content: Using the REST API Layer
          isCorrect: false
          explanation: Incorrect, not available
    - content: 'By default, how are corrupt records dealt with using spark.read.json()?'
      choices:
        - content: They appear in a column called "_corrupt_record"
          explanation: These are the records that Spark can't read (e.g. when characters are missing from a JSON string)
          isCorrect: true
        - content: They get deleted automatically
          isCorrect: false
          explanation: Incorrect, they don't get deleted.
        - content: They throw an exception and exit the read operation
          isCorrect: false
          explanation: Incorrect, no exception is thrown.
    - content: 'What is the recommended storage format to use with Spark?'
      choices:
        - content: Apache Parquet
          explanation: Apache Parquet is a highly optimized solution for data storage and is the recommended option for storage where possible
          isCorrect: true
        - content: JSON
          isCorrect: false
          explanation: Incorrect, not an optimal format.
        - content: XML
          isCorrect: false
          explanation: Incorrect, not an optimal format.