The previous units were largely focused on **what** your storage solution was doing. This unit will focus on **where** your data is located. We will discuss *hybrid* file access considerations and how to approach them.

## Overview of Hybrid File Access ##

You have decided to run a workload in Azure that is currently running in your datacenter. Your compute environment is access data on your NAS, which is serving NFSv3 ops to your workload. It's been running there for years but perhaps your NAS environment is reaching the end of its cycle and rather than replace it you're considering a long-term migration into the cloud.

There is a period of time between this decision and what I'll call "cloud steady-state" where you have determined your Azure strategy and established your baseline account/subscription/security setup. Now the hard part...moving workloads!

The buildout of your HPC cluster and its management plane are outside the scope of this module, so we will assume that you have determined which virtual machine types your cluster will run, how many, and so on. 

Further we will assume for the moment that your objective is to run the workload "as-is"; that is without modifying the logic or access methods in any meaningful way. The implication is that your code expects to see data at directory paths in the cluster members' local filesystem.

The first objective is to understand what data is required, and from where it is currently sourced. All of your data may be located in a single directory on a single NAS environment, or it may be spread out across different environments.

The next objective is to determine *how much* data is required to run the workload. Is the source data a couple of Gigabytes or is it hundreds of Terabytes?

Finally, you have to determine how the data is presented in Azure Compute. Is it served locally to each HPC cluster machine, or is it shared by a cloud-based NAS solution?

## Remote data access considerations ##

You have a genomic workload that you'd like to run in Azure. Your data is generated by on-premises sequencers to a local NAS environment. On-premises researchers consume the data for various different uses. Perhaps those researchers also wish to consume the results of the genomic workload you intend to run in Azure, but some of those researchers use on-premises compute and workstations to do so. Let's also assume that fresh genomic data is generated on a regular basis, and so you have a limited interval to execute the workload before the data needs replaced/refreshed.

The challenge then is to present the data to Azure Compute, in a cost-effective, timely fashion, while preserving on-premises access to that data.

This is an example of the considerations you may face when attempting to run a new HPC workload in Azure. The main questions you must answer are:

    - Can source data be moved to Azure without retaining a copy on-premises?
    - Can results data be saved in Azure storage without retaining a copy on-premises?
    - Do on-premises users require concurrent access to the source or results data?
    - If they do, are they able to operate on the data in Azure, or are they requiring it be stored on-premises?

If the data must be kept on-premises, how much data must be copied to Azure for the workload? How long between a fresh dataset must be copied?

Further, you must consider network connectivity to Azure. Do you only have Internet access to Azure? If so that might be ok depending on the size of the data to be copied/transferred and the amount of time between refreshes. Perhaps you have a large amount of data to copy each time. You may require a Wide-Area Network connection to Azure using Azure ExpressRoute, which would provide much more bandwidth to copy/transfer the data.

If you have an ExpressRoute to Azure already, the next consideration is how much of that ExpressRoute is available to your data copy operation. If the link is heavily saturated you may need to consider time-of-day when transferring data, or perhaps a larger ExpressRoute connection to accommodate large data transfers.

If the data is moved to Azure, you may need to consider how it will be secured. For example, you may have a NFS environment on-premises that leverages a directory service that helps to extend permissions to your users. If you intend to copy this security to Azure, you will need to decide whether or not you require a directory service as part of the Azure buildout. On the other hand, if your workload is restricted to the HPC cluster and the results would be transferred back to your local environment, you may be able to omit these additional components.

Next, we consider the methods of accessing the data: caching, copying or synchronizing.


## Caching vs Copying vs Sync ##

We will discuss the general approaches you can use to put data into Azure. The focus of this transfer is on **active** data; we will not discuss data archival and backup. The data being transferred in our discussion is presumed to be the *working set* of a HPC job. In a life sciences HPC environment, data might include source data such as raw genomic data, binaries used to process that data, or supplemental data such as reference genomes. It is to be processed immediately upon arrival or not long after. It must also be stored on media with the appropriate performance profile in terms of IOPS, latency and throughput...and cost. By contrast, archive/backup data is most often transferred to the lowest storage cost solution possible, which is not intended for high performance access.

The main methods of transferring active data are **caching**, **copying** and **synchronizing**. We will discuss the pros and cons of each approach. We will start with copying.

**Copying** data is the most common approach to data movement. The data is copied in various ways depending on the tool you use. You must consider the size of the files, the number of files, the amount of available throughput to transfer the data, and the amount of time you have to do the transfer. Using a basic copy tool such as ```cp``` is all you need if you are transferring a handful of reasonably sized files to a remote destination. You would likely want to use ```scp``` instead of ```cp``` if you are transferring over insecure networks, as scp provides encryption over an Secure Shell (SSH) connection.

There are many approaches to optimizing copy operations for your HPC environment, depending on where you intend to copy the data. If you are copying files directly to each HPC machine, you could schedule individual copy operations on each node, for example. 

One issue with copying across WAN links involves the quantity of files/folders being copied.  If you are copying a large number of small files, you will want to combine the use of copy with an archive such as ```tar``` to remove the metadata overhead from the WAN link. Copy the .tar file to Azure and then copy the data to the machines.

Another issue with copying involves the risk of interruption. If, for example, you're trying to copy a very large file and there are transmit errors, using ```cp``` won't suffice because it cannot resume the copy from where it left off.

A final concern with copying data is that your copy can become *stale*. For example, you may copy a data set to Azure, and in the meantime an on-premises user may have updated one or more of the source files. You must therefore figure out a process for ensuring you are using the correct data.

**Synchronizing** data is a form of copying, but is more sophisticated. Tools such as ```rsync``` add the ability to not only copy data from a source, but can synchronize the data between the source and destination. ```Rsync``` will check to make sure the files are up to date based on the file size and modification dates. Synchronizing allows you to minimize the possibility of using stale files.

Using ```rsync``` has recovery capability. For example, in the case where you're trying to copy a large file and encountering transmit issues, ```rsync``` has the ability to *resume* from where it left off.

```Rsync``` is free and easy to implement. It has extensive capabilities beyond the scope of this unit. It allows you to establish a synchronized file system in Azure, based on your on-premises data.

There are limitations with ```rsync``` we should mention here as well.  First, the tool is *single-threaded*; it can only run one operation at a time and cannot parallelize its access of the data. The copy utility ```cp``` is also single-threaded. Performance of these tools is therefore not optimized for large scale copy/sync operations involving large amounts of data and a short window of time. You must also RUN the tool to sync, which means that you must add complexity to your environment to ensure that it is running according to your timeframe requirements. You may wish to schedule a script including ```rsync```, for example. This also means that you'll need to add logging of your script, in case there are issues. It also implies you must watch for the issues. As you can infer, the level of complexity can grow quickly.

If you are running a commercial NAS solution, there are server-level synchronization tools that can be purchased that are far more sophisticated and offer multi-threaded performance. Once enabled and configured such tools are always operating, synchronizing data between one more more sources and destinations.

Copying and Synchronizing are both full copies of source data. This may be fine for smaller data sets or file sizes, but can introduce significant delays if the source data consists of many large files. The larger the amount of data, the longer the transfer will take. In many cases, your HPC workload may not require the entirety of a given set of files and in fact may only require access to specific areas of files. This is where caching can help.

**Caching** data is a third approach for putting data into Azure. Caching refers to the retrieval and presentation of file data via a cache. The cache can be located on individual local clients, or can be a distributed cache serving all HPC machines. Caches are normally used to minimize *latency*, and so placing a cache at a latency boundary is an optimal approach to serving data. For example, you can cache data requests across a WAN connection, by placing a distributed cache in Azure compute, connected to on-premises storage across the WAN link.

In the context of this module, we are referring specifically to **file caching**, where the cache itself is fielding requests from machines, retrieving the data from a back-end storage environment (such as a NFS NAS environment), and presenting that data to the clients.

The power of the cache is two-fold.  First, caches **do not retrieve whole files**; the cache retrieves requested byte ranges of files, rather than entire files, based on client requests for those byte ranges. This approach to retrieval minimizes performance penalties for retrieving the entirety of a large file when only a small section of the file is required.

Further, caches **optimize repeat access of frequently requested data**. Once a byte range is in the cache, subsequent requests for that data are very fast; the only slow retrieval is the initial retrieval. You can realize significant benefits when running a large number of HPC clients/threads that are accessing a common set of files. 

Caching offers an additional advantage for hybrid scenarios. Because the data is only transiently stored in Azure (in the cache), and only during the operation of the HPC workload, you can reduce the amount of logistical overhead involved in more concrete data movement to Azure. Concerns of data privacy and security can be isolated to the cache and the HPC machines themselves. 

Finally, certain caching solutions offer what is called an *attribute check*. Similar to synchronization, the cache will periodically check the attributes of the file located in the source, and retrieve byte ranges where the file modification is greater at the source. This ensures that you're HPC environment is always operating with the freshest data.