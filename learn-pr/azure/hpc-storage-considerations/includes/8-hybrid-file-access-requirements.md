The previous units were largely focused on *what* your storage solution is doing. This unit will focus on *where* your data is located. We'll discuss hybrid file access considerations and how to approach them.

## Overview of hybrid file access ##

You've decided to run an HPC workload in Azure that's currently running in your datacenter. Your compute environment accesses data on your NAS, which is serving NFSv3 operations to your workload. It's been running there for years, but maybe your NAS environment is reaching the end of its cycle. Rather than replace it, you're considering a long-term migration to the cloud.

After you make this decision but before full cloud deployment of your HPC workload, you determine your Azure strategy and establish your baseline account/subscription/security setup. Now the hard part: moving your HPC workloads!

The buildout of your HPC cluster and its management plane are outside the scope of this module. We'll assume you have determined which virtual machine types and quantities your cluster will run.

For now, we'll also assume that your objective is to run the workload as is. That is, you won't modify the logic or access methods currently deployed on-premises. The implication is that your code expects data to be at directory paths in the cluster members' local file systems.

The first objective is to understand what data is required and where it's sourced from. Your data might be in a single directory in a single NAS environment, or it might be spread out across various environments.

The next objective is to determine how much data is required to run the workload. Is the source data a couple of gigabytes, or is it hundreds of terabytes?

Finally, you have to determine how the data is presented in Azure compute. Is it served locally to each HPC cluster machine, or is it shared by a cloud-based NAS solution?

## Remote data access considerations ##

You have a genomics workload that you want to run in Azure. Your data is generated by on-premises gene sequencers to a local NAS environment. On-premises researchers consume the data for various uses. The researchers might also want to consume the results of the HPC workload you intend to run in Azure. But some of them use on-premises workstations to do so. Let's also assume that fresh genomic data is generated regularly. So you have a limited interval to run the current workload before the data needs replaced/refreshed.

The challenge is to present the data to Azure compute in a cost-effective, timely way while preserving on-premises access to it.

Here are some of the main questions to ask when you're trying to run HPC workloads in Azure:

- Can we move source data to Azure without retaining a copy on-premises?
- Can we save results data in Azure storage without retaining a copy on-premises?
- Do on-premises users need concurrent access to the source or results data?
- If they do, are they able to operate on the data in Azure, or do they need the data to be stored on-premises?

If the data needs to be kept on-premises, how much data must be copied to Azure for the workload? How long do you have after data is processed before you need to process a new set of data? Will your workload run in that timeframe?

You also need to consider network connectivity to Azure. Do you have only internet access to Azure? That limitation might be OK, depending on the size of the data to be copied/transferred and the amount of time between refreshes. Maybe you have a large amount of data to copy each time. You might need a wide-area network (WAN) connection to Azure that uses Azure ExpressRoute, which would provide more bandwidth to copy/transfer the data.

If you already have an ExpressRoute connection to Azure, here's the next consideration: how much of the connection is available to your data-copy operation? If the link is heavily saturated, you might need to consider the  time of day when you transfer data. Or you might want to configure a larger ExpressRoute connection to accommodate large data transfers.

If you move the data to Azure, you might need to consider how you'll secure it. For example, you might have an on-premises NFS environment that uses a directory service that helps to extend permissions to your users. If you plan to copy this security to Azure, you'll need to decide whether you need a directory service as part of the Azure buildout. On the other hand, if your workload is restricted to the HPC cluster and the results would be transferred back to your local environment, you may be able to omit these requirements.

Next, we consider the methods of accessing the data: caching, copying or synchronizing.

## Caching vs copying vs sync ##

We will discuss the general approaches you can use to put data into Azure. The focus of this transfer is on **active** data; we will not discuss data archival and backup. The data being transferred in our discussion is presumed to be the *working set* of an HPC workload. In a life sciences HPC environment, data might include source data such as raw genomic data, binaries used to process that data, or supplemental data such as reference genomes. It is to be processed immediately upon arrival or not long after. The data must also be stored on media with the appropriate performance profile in terms of IOPS, latency, throughput, and cost. By contrast, archive/backup data is most often transferred to the lowest storage cost solution possible, which is not intended for high-performance access.

The main methods of transferring active data are **caching**, **copying** and **synchronizing**. We will discuss the pros and cons of each approach. We will start with copying.

**Copying** data is the most common approach to data movement. The data is copied in various ways depending on the tool you use. Consider the size of the files, the number of files, the amount of available throughput to transfer the data, and the amount of time you have to do the transfer. Using a basic copy tool such as ```cp``` is all you need if you are transferring a handful of reasonably sized files to a remote destination. You would likely want to use ```scp``` instead of ```cp``` if you are transferring over insecure networks, as scp provides encryption over a Secure Shell (SSH) connection.

There are many approaches to optimizing copy operations, depending on where you intend to copy the data. If you are copying files directly to each HPC machine, you could schedule individual copy operations on each node, for example. 

One issue with copying across WAN links involves the quantity of files/folders being copied.  If you are copying a large number of small files, you will want to combine the use of copy with an archive such as ```tar``` to remove the metadata overhead from the WAN link. Copy the .tar file to Azure and then copy the data to the machines.

Another issue with copying involves the risk of interruption. For example, you're trying to copy a large file and there are transmit errors, using ```cp``` won't suffice because it cannot resume the copy from where it left off.

A final concern with copying data is that your copy can become *stale*. For example, you may copy a data set to Azure, and in the meantime an on-premises user may have updated one or more of the source files. You must figure out a process for ensuring you are using the correct data.

**Synchronizing** data is a form of copying, but is more sophisticated. Tools such as ```rsync``` add the ability to not only copy data from a source, but can synchronize the data between the source and destination. ```Rsync``` will check to make sure the files are up to date based on the file size and modification dates. Synchronizing allows you to minimize the possibility of using stale files.

Using ```rsync``` has recovery capability. For example, in the case where you're trying to copy a large file and encountering transmit issues, ```rsync``` has the ability to *resume* from where it left off.

```Rsync``` is free and easy to implement. It has extensive capabilities beyond the scope of this unit. It allows you to establish a synchronized file system in Azure, based on your on-premises data.

There are limitations with ```rsync``` we should mention here as well.  First, the tool is *single-threaded*; it can only run one operation at a time and cannot parallelize its access of the data. The copy utility ```cp``` is also single-threaded. Performance of these tools is therefore not optimized for large-scale copy/sync operations involving large amounts of data and a short window of time. You must also RUN the tool to sync, which means added complexity to your environment to ensure that it is running according to your timeframe requirements. You may wish to schedule a script including ```rsync```, for example. This approach means that you'll need to add logging of your script, in case there are issues. It also implies you must watch for the issues. The level of complexity can grow quickly.

If you are running a commercial NAS solution, there are server-level synchronization tools that can be purchased that are far more sophisticated and offer multi-threaded performance. Once enabled and configured such tools are always operating, synchronizing data between one or more sources and destinations.

Copying and Synchronizing transmit full copies of source data. Full file transmission may be fine for smaller data sets or file sizes, but can introduce significant delays if the source data consists of many large files. The larger the amount of data, the longer the transfer will take. And while synchronization will ensure that you are only appending new files to the cloud, those files must still be transmitted in their entirety. In many cases, your HPC workload may not require the entirety of a given set of files and in fact may only require access to specific areas of files.

**Caching** data is a third approach for putting data into Azure. Caching refers to the retrieval and presentation of file data via a cache. The cache can be located on individual local clients, or can be a distributed cache serving all HPC machines. Caches are normally used to minimize *latency*, and so placing a cache at a latency boundary is an optimal approach to serving data. For example, you can cache data requests across a WAN connection, by placing a distributed cache in Azure compute, connected to on-premises storage across the WAN link.

In the context of this module, we are referring specifically to **file caching**, where the cache itself is fielding requests from machines, retrieving the data from a back-end storage environment (such as an NFS NAS environment), and presenting that data to the clients.

The power of the cache is two-fold.  First, caches **do not retrieve whole files**; the cache retrieves requested a subset, or byte range, of files, rather than entire files, based on client requests for those byte ranges. This approach to retrieval minimizes performance penalties for retrieving the entirety of a large file when only a small section of the file is required.

Further, caches **optimize repeat access of frequently requested data**. Once a byte range is in the cache, subsequent requests for that data are fast; the only slow retrieval is the initial retrieval. You can realize significant benefits when running a large number of HPC clients/threads that are accessing a common set of files. 

Caching offers an another advantage for hybrid scenarios. Because the data is only transiently stored in Azure (in the cache), and only during the operation of the HPC workload, you can reduce the logistical overhead involved in more concrete data movement to Azure. Concerns of data privacy and security can be isolated to the cache and the HPC machines themselves. 

Finally, certain caching solutions offer what is called an *attribute check*. Similar to synchronization, the cache will periodically check the attributes of the file located in the source, and retrieve byte ranges where the file modification is greater at the source. This architecture ensures that you're HPC environment is always operating with the freshest data.

![A diagram showing WAN caching between Azure compute and on-premises storage](../media/wan-caching.png)