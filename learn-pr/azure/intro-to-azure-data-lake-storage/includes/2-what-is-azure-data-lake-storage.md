Let's begin with a quick overview of Azure Data Lake Storage and its core features. This overview should help you decide whether Azure Data Lake Storage is worth considering as a solution for your company's big data storage requirements.

## What is a data lake?

A *data lake* is a single, centralized repository where you can store all your data, both structured and unstructured. A data lake enables your organization to quickly and more easily store, access, and analyze a wide variety of data in a single location. With a data lake, you don't need to conform your data to fit an existing structure. Instead, you can store your data in its raw or native format, usually as files or as binary large objects (blobs).

When evaluating whether a data lake is the correct solution for your company, you should consider several elements as described in the following table.

| **Element**        | **Description**  |
| --- | --- |
| **Data speed**      | A data lake must be able to ingest data at any speed: from the occasional file, to large relational data imports, to real-time data generated by web server logs or IoT devices. |
| **Data scalability** | A data lake might be required to store massive amounts of data that arrive in real time. Thus, the storage must be highly scalable to keep up with demand. |
| **Data availability** | After the data is stored in a data lake, it must be readily available via browsing, searching, and indexing. |
| **Data security**      | Most data lakes store crucial data assets, including line-of-business (LOB) data, company-developed apps, and productivity output. The data lake requires robust security to protect these assets.    |
| **Data analytics** | A data lake must store data in a way that enables users to use their preferred tools to analyze the data in place. Business analysts, data scientists, and AI modelers need to use their own tools to derive business intelligence, insights, trends, and forecasts. |
| | |

## Azure Data Lake Storage definition

*Azure Data Lake Storage* is a cloud-based, enterprise data lake solution. It's engineered to store massive amounts of data in any format, and to facilitate big data analytical workloads. You use it to capture data of any type and ingestion speed in a single location for easy access and analysis using various frameworks.

> [!NOTE]
> The current implementation of Azure's data lake storage service is Azure Data Lake Storage Gen2. You might notice references to the previous implementation, Azure Data Lake Storage Gen1, which is scheduled to be retired on February 29, 2024.

To better understand Azure Data Lake Storage, you can examine its following characteristics:

- Data storage
- Data access
- Data costs
- Data performance
- Data security
- Data redundancy
- Data scalability
- Data analysis

### Data storage

Azure Data Lake Storage can store any type of data by using the data's native format. You don't need to define a schema or perform any type of transformation on the data before ingesting the data. Azure Data Lake Storage doesn't need to do any special handling of data based on the type of data it stores. With support for any data format and massive data sizes, Azure Data Lake Storage can work with structured, semi-structured, and unstructured data.

### Data access

Azure Data Lake Storage is primarily designed to work with Hadoop and all frameworks that use the Apache Hadoop Distributed File System (HDFS) as their data access layer. Hadoop distributions include the Azure Blob File System (ABFS) driver, which enables many applications and frameworks to access Azure Blob Storage data directly.

*Hierarchical namespace* is a key feature that enables Azure Data Lake Storage to provide high-performance data access at object storage scale and price. You can use this feature to organize all the objects and files within your Azure Data Lake Storage account into a hierarchy of directories and nested subdirectories. In other words, your Azure Data Lake Storage data is organized in much the same way that files are organized on your computer.

> [!IMPORTANT]
> Hierarchical namespace is *not* enabled by default. When you create a storage account, you must select the **Enable Hierarchical Namespace** checkbox. Note, as well, that you can't enable this feature on existing storage accounts, only on new storage accounts.

### Data costs

Azure Data Lake Storage is priced at Azure Blob Storage levels. It builds on Azure Blob Storage capabilities such as Automated Lifecycle Policy Management and Object Level tiering to manage big data storage costs. An Azure Data Lake Storage account uses a hierarchical namespace, which provides the scalability and cost-effectiveness of object storage.

### Data performance

The hierarchical namespace capability of Azure Data Lake Storage allows for efficient access and navigation. This architecture means that data processing requires fewer computational resources, reducing both the speed and cost of accessing data.

Azure Data Lake Storage supports high throughput for input/output intensive analytics and data movement. To get the best performance from Azure Data Lake Storage, you should use all available *throughput*; that is, the amount of data that can be read or written per second. Azure Data Lake Storage achieves throughput maximization by performing as many reads and writes in parallel as possible.

### Data security

The Azure Data Lake Storage access control model supports both Azure role-based access control (RBAC) and Portable Operating System Interface for UNIX (POSIX) access control lists (ACLs). There are also a few extra security settings that are specific to Azure Data Lake Storage. You can set permissions either at the directory level or at the file level. All stored data is encrypted at rest by using either Microsoft-managed or customer-managed encryption keys.

You can configure security settings in several ways, including by using the following apps and frameworks:

- Azure Storage Explorer
- Azure Command-Line Interface (Azure CLI)
- Java
- JavaScript (Node.js)
- Microsoft .NET Framework
- PowerShell
- Python
- REST API

### Data redundancy

Azure Data Lake Storage utilizes Azure Blob replication models. These models provide data redundancy in a single datacenter with locally redundant storage (LRS). They also provide a secondary region by using geo-redundant storage (GRS), which helps ensure that your data is always available and protected if there's an outage.

### Data scalability

Azure Data Lake Storage offers massive storage and accepts numerous data types for analytics. It doesn't impose any limits on account sizes, file sizes, or the amount of data that can be stored in the data lake. Individual blobs and files can have sizes that range from a few kilobytes (KBs) to a few petabytes (PBs). This design means that Azure Data Lake Storage can easily and quickly scale up to meet the most demanding workloads. It can also just as easily scale back down when demand drops.

### Data analysis

Data analysis frameworks that use HDFS as their data access layer can directly access Azure Data Lake Storage data through ABFS. The Apache Spark analytics engine and the Presto SQL query engine are examples of such frameworks.

In Azure, Data Lake Storage integrates with the following frameworks for analysis:

- Azure HDInsight
- Azure Machine Learning
- Azure Synapse Analytics
- Microsoft Power BI

> [!NOTE]
> Azure Data Lake Storage is also integrated into a massive and mature analytics ecosystem associated with Azure Blob Storage.

## How to prevent data silos

Data silos occur when different data sources are stored in separate locations, each of which can only be accessed by a specific application or framework. For example, a business analyst or data scientist who wants to analyze both sales data and web server logs must complete the following general steps:

1. Access one set of data.
1. Query the data.
1. Download the query results.
1. Repeat steps 1 through 3 for each set of data.
1. Process the downloaded data in a way that makes it possible to analyze the two datasets together.
1. Analyze the processed data.

These steps form a time-consuming, convoluted, and complex process. However, if all the required data is stored in Azure Data Lake Storage, the business analyst or data scientist can use their tool of choice&mdash;such as Power BI or Azure HDInsight&mdash;to work directly with all the data they need. Azure Data Lake Storage leaves it up to the individual analytic framework to interpret the data and define a schema at the time of  analysis.
