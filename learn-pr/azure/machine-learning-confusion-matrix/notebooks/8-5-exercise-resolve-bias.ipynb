{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70d0b5f9",
   "metadata": {},
   "source": [
    "# Exercise: Imbalanced data model bias\n",
    "\n",
    "* Please make sure unit 3 is done first so you can copy+paste critical code and we can be consistent in our message\n",
    "* Please also read previous content in the word doc\n",
    "* Make a model using default settings that predicts hiker or non-hiker (ie binary classification). There a  rough example down the bottom of 2c dataset generation\n",
    "* We want to build some confusion matrices for biased data/models. Again, there's an example in 2c dataset generation. The related code is ugly though, so please try to be tidier than I have. You might want to copy accuracy-related metrics from the 2b Unit 3\n",
    "* Demonstrate it all looks ok on the snow_objects.csv data, but when it needs to process data that is differently distributed (snow_objects_balanced), it doesn't do well. \n",
    "* We want to change weighting of the labels in the cost function to avoid bias \n",
    "* OR\n",
    "* We want to resample our data to deal with the imbalances\n",
    "(whichever is easier). If the unit is not too long, do both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eec3f24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/snow_objects.csv\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/snow_objects_balanced.csv\n",
    "\n",
    "#Import the data from the .csv file\n",
    "dataset = pandas.read_csv('snow_objects.csv', delimiter=\"\\t\")\n",
    "\n",
    "#Let's have a look at the data\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2abc945",
   "metadata": {},
   "source": [
    "Recall that we have an imbalanced frequency of labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444ec79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphing # custom graphing code. See our GitHub repo for details\n",
    "\n",
    "# Plot a histogram with counts for each label\n",
    "graphing.multiple_histogram(dataset, label_x=\"label\", label_group=\"label\", title=\"Label distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8c9da0",
   "metadata": {},
   "source": [
    "--- Let's build a binary classification model for this dataset: given a set of features, it should predict whether the object is either a \"hiker\" or a \"non-hiker\":\n",
    "\n",
    "--- talk about adding a new label... mention \"feature engineering\"?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa7670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new label with true/false values to our dataset\n",
    "dataset[\"is_hiker\"] = dataset.label == \"hiker\"\n",
    "\n",
    "# Plot frequency for new label\n",
    "graphing.multiple_histogram(balanced_dataset, label_x=\"is_hiker\", label_group=\"is_hiker\", title=\"Distribution for new binary label 'is_hiker'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25702363",
   "metadata": {},
   "source": [
    "Still imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a994b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Custom function that measures accuracy on different sets\n",
    "# We will use this in different parts of the exercise\n",
    "# TODO: make this accept several different datasets and loop to generate results\n",
    "def assess_accuracy(model, label=\"label\"):\n",
    "    \"\"\"\n",
    "    Asesses model accuracy on different sets\n",
    "    \"\"\"\n",
    "\n",
    "    def sub(source):   \n",
    "        actual = source[label]        \n",
    "        predictions = model.predict(source[features])\n",
    "        acc = accuracy_score(actual, predictions)\n",
    "        return acc\n",
    "\n",
    "    train_acc = sub(train)\n",
    "    test_acc = sub(test)\n",
    "\n",
    "    return train_acc, test_acc\n",
    "\n",
    "# Split the dataset in an 70/30 train/test ratio. \n",
    "train, test = train_test_split(dataset, test_size=0.3, random_state=1, shuffle=True)\n",
    "\n",
    "# define a random fores model\n",
    "model = RandomForestClassifier(n_estimators=1, random_state=1, verbose=False)\n",
    "\n",
    "# Define which features are to be used (leave color out for now)\n",
    "features = [\"size\", \"roughness\", \"motion\"]\n",
    "\n",
    "# Train the model using binary label\n",
    "model.fit(train[features], train.is_hiker)\n",
    "\n",
    "print(\"Train accuracy:\", assess_accuracy(model, \"is_hiker\")[0])\n",
    "print(\"Test accuracy:\", assess_accuracy(model, \"is_hiker\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8fd19b",
   "metadata": {},
   "source": [
    "--- metrics are very high, but remember accuracy is not an absolute measure of success\n",
    "\n",
    "--- lets build a confusion matrix for the model we just trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce575015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn has a very convenient utility to build confusion matrices\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Calculate the model's accuracy on the TEST set\n",
    "actual = test.is_hiker\n",
    "predictions = model.predict(test[features])\n",
    "\n",
    "# Build and print our confusion matrix, using the actual values and predictions \n",
    "# from the test set, calculated in previous cells\n",
    "cm = confusion_matrix(actual, predictions, normalize=None)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fcc1b1",
   "metadata": {},
   "source": [
    "--- small break for explanation... plot this thing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f5a67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use plotly to create plots and charts\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Create the list of unique labels in the test set, to use in our plot\n",
    "# I.e., ['True', 'False',]\n",
    "unique_targets = sorted(list(test[\"is_hiker\"].unique()))\n",
    "\n",
    "# Convert values to lower case so the plot code can count the outcomes\n",
    "x = y = [str(s).lower() for s in unique_targets]\n",
    "\n",
    "# Plot the matrix above as a heatmap with annotations (values) in its cells\n",
    "fig = ff.create_annotated_heatmap(cm, x, y)\n",
    "\n",
    "# Set titles and ordering\n",
    "fig.update_layout(  title_text=\"<b>Confusion matrix</b>\", \n",
    "                    yaxis = dict(categoryorder = \"category descending\")\n",
    "                    )\n",
    "\n",
    "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
    "                        x=0.5,\n",
    "                        y=-0.15,\n",
    "                        showarrow=False,\n",
    "                        text=\"Predicted label\",\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\"))\n",
    "\n",
    "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
    "                        x=-0.15,\n",
    "                        y=0.5,\n",
    "                        showarrow=False,\n",
    "                        text=\"Actual label\",\n",
    "                        textangle=-90,\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\"))\n",
    "\n",
    "# We need margins so the titles fit\n",
    "fig.update_layout(margin=dict(t=80, r=20, l=120, b=50))\n",
    "fig['data'][0]['showscale'] = True\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e23a90",
   "metadata": {},
   "source": [
    "--- Beautiful! NOw explain why this is not good... recall explanation in 2c u3\n",
    "\n",
    "What happens if we used this model to make predictions on balanced sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213f63ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and print umbiased set\n",
    "#Import the data from the .csv file\n",
    "balanced_dataset = pandas.read_csv('snow_objects_balanced.csv', delimiter=\"\\t\")\n",
    "\n",
    "#Let's have a look at the data\n",
    "balanced_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c6166b",
   "metadata": {},
   "source": [
    "--- plot distribution to prove it is balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a945b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add a new label with true/false values to our dataset\n",
    "balanced_dataset[\"is_hiker\"] = dataset.label == \"hiker\"\n",
    "\n",
    "graphing.multiple_histogram(balanced_dataset, label_x=\"is_hiker\", label_group=\"is_hiker\", title=\"Label distribution in balanced dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ef3ebc",
   "metadata": {},
   "source": [
    "As you can see, all labels have the same frequency now.\n",
    "\n",
    "Let's run predictions on this set using the previously trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92acc9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "actual = test.is_hiker\n",
    "predictions = model.predict(test[features])\n",
    "\n",
    "# Build and print our confusion matrix, using the actual values and predictions \n",
    "# from the test set, calculated in previous cells\n",
    "cm = confusion_matrix(actual, predictions, normalize=None)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b692998e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot new confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842be7ad",
   "metadata": {},
   "source": [
    "## Weighted labels to balance dataset\n",
    "\n",
    "Explain concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1ebefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance dataset using weighted classes\n",
    "\n",
    "# Retrain\n",
    "\n",
    "# Re assess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43e6270",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sumary"
   ]
  }
 ],
 "metadata": {
    "kernelspec": {
      "name": "conda-env-py37_default-py",
      "language": "python",
      "display_name": "py37_default"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "conda-env-py37_default-py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
  }