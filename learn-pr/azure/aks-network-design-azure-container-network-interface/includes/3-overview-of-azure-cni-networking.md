Kubernetes supports various plugins, allowing you to add new features and replace or enhance existing cluster behavior. The Container Network Interface (CNI) is a specification that allows developers to create plugins to configure container networking. Kubernetes leverages the CNI specification, enabling the use of CNI plugins on your clusters.

## Azure CNI plugins

The [Azure CNI plugin](/azure/aks/configure-azure-cni) enables collaboration between containers and Azure virtual networks (VNets). Using Azure CNI in a Kubernetes cluster allows pods to be assigned IP addresses from an Azure virtual network. The pod can then communicate on that virtual network just like any other device. It can connect to other pods, peered networks, on-premises networks using a VPN or ExpressRoute, or to other Azure services using Private Link.

Aside from the traditional Azure CNI plugin, AKS supports the following additional CNI plugins:

* Azure CNI Overlay
* Azure CNI Powered by Cilium
* Azure CNI for dynamic allocation of IPs and enhanced subnet support
* Bring your own (BYO) CNI

| Plugin | Description | When to use |
| ------ | ------------ | ----------- |
| [Azure CNI Overlay](/azure/aks/azure-cni-overlay) | Cluster nodes are deployed into an Azure VNet subnet. Pods are assigned IP addresses from a private CIDR logically different from the VNet hosting the nodes. Pod and node traffic within the cluster use an Overlay network. NAT uses the node's IP address to reach resources outside of the cluster. | • You want to scale to a large number of pods, but you have limited IP address space in your VNet.<br/> • Most of the pod communication is within the cluster.<br/> • You don't need advanced AKS features, such as virtual nodes.|
| [Azure CNI Powered by Cilium](/azure/aks/azure-cni-powered-by-cilium) | Combines the Azure CNI control plane with the Cilium data plane. Cilium enforces network policies to allow or deny traffic between pods, so you don't need to use a separate network policy engine. You can choose between two different methods for assigning pod IPs: via an overlay network or a virtual network. | • You need support for larger clusters.<br/> • You want faster service routing, more efficient network policy enforcement, and better observability of cluster traffic.<br/> • You want to leverage the functionalities of the traditional Azure CNI and Azure CNI Overlay plugins with high-performance networking and security. |
| [Azure CNI for dynamic allocation of IPs and enhanced subnet support](/azure/aks/configure-azure-cni-dynamic-ip-allocation) | Leverages the functionalities of the traditional Azure CNI plugin and extends upon them to allocate pod IPs from subnets separate from the subnet hosting the AKS cluster. IPs are dynamically allocated to cluster pods from the pod subnet. Node and pod subnets can be scaled independently and share pod subnets across multiple node pools or clusters in the same VNet. Since pods have a separate subnet, you can configure separate VNet policies for them that differ from the node policies. | • You want the flexibility to scale node and pod subnets independently.<br/>  • You need support for larger clusters without sacrificing performance.<br/> • You want to configure separate VNet policies for pods. |
| [BYO CNI](/azure/aks/use-byo-cni) | AKS clusters are deployed without a preinstalled CNI plugin. From there, you can install your preferred [Azure supported third-party CNI plugin](/azure/aks/concepts-network). Keep in mind that Microsoft support can't assist with CNI-related issues in clusters deployed with BYO CNI. | • You want to use the same CNI plugin in AKS that you use in your on-premises Kubernetes environment.<br/> • You want to use advanced functionalities available in the supported third-party plugins. |
