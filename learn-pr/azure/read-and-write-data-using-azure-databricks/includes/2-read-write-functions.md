Let's discuss the  reading and writing capabilities of Spark notebooks in detail. Databricks provides various built-in functions that allows you to query, process, and analyze a large volume of data. 

- **Querying files** - You can use DataFrames to query large data files. DataFrames are derived from data structures known as Resilient Distributed Datasets (RDDs). RDDs and DataFrames are immutable distributed collections of data.

  - _Resilient_: RDDs are fault tolerant, so if part of your operation fails, Spark quickly recovers the lost computation.
  - _Distributed_: RDDs are distributed across networked machines known as a cluster.
  - _DataFrame_: A data structure where data is organized into named columns, like a table in a relational database, but with richer optimizations under the hood.

  Once created, a DataFrame object comes with methods attached to it. These methods are operations that you can perform on DataFrames such as filtering, counting, aggregating and many others. Spark provides a number of built-in functions, many of which can be used directly with DataFrames to filter through the data and derive specific results.

- **Joins and aggregation** - Several built-in functions of Databricks allow you to aggregate your data in different ways. You can use DataFrames to join completely different sets of data to find a correlation between them.
- **Accessing data** - Databricks allows several ways to access your data. You can access data stored in an existing file, by uploading a data file from your local system, or by mounting an Azure blob to Databricks file system. Databricks accepts data stored in different file formats such as parquet and csv.
- **Working with hierarchial data** - DataFrames makes it easier to query hierarchial data such as nested JSON records. You can query nested structured data or data containing array columns.
- **Data Lake** - Apache Spark and Databricks make it easy to access and work with files stored in Data lakes, such as Azure Data Lake Storage (ADLS). Data lake stores vast amount of data in its native format such as XML, JSON, CSV, and Parquet. You can use Databricks to perform exploratory data analysis (EDA) to gain insights from a Data Lake. Spark DataFrames can be used to read directly from raw files contained in a Data Lake and then execute queries to join and aggregate the data.
- **Azure Data Lake Storage Gen2** - Azure Data Lake Storage Gen2 (ADLS Gen2) is a set of capabilities dedicated to big data analytics, built on Azure Blob storage. Data Lake Storage Gen2 is the result of converging the capabilities of Microsoft's two existing storage services, Azure Blob storage and Azure Data Lake Storage Gen1. Features from Azure Data Lake Storage Gen1, such as file system semantics, directory, and file level security and scale are combined with the low-cost, tiered storage, and high availability/disaster recovery capabilities from Azure Blob storage. Databricks allows you to provision an ADLS Gen2 instance and use the Azure Blob File System (ABFS) driver built into the Databricks Runtime to query data stored in ADLS Gen2.
- **Key Vault-backed secret scopes** - Azure Databricks has two types of secret scopes: Key Vault-backed and Databricks-backed. These secret scopes allow you to store secrets, such as database connection strings, securely. If someone tries to output a secret to a notebook, it is replaced by [REDACTED]. This helps prevent someone from viewing the secret or accidentally leaking it when displaying or sharing the notebook. 

In this module, we'll look at the steps for configuring a Key Vault-backed secret scope. We'll also see how to create the Key Vault, Azure SQL Database, and Cosmos DB resources, and how to create a Key Vault-backed secret scope.