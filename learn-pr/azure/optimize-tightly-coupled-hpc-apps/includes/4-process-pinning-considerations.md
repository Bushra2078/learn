## Why pin processes and threads?

The primary reasons why it's always recommended to always pin processes to specific cores is to help achieve maximum performance and get more consistent performance from run to run.

How does process pinning help with performance and consistency?

- Process pinning maximizes memory bandwidth by placing or pinning processes in locations that utilize all memory channels and distribute all memory channels equally among the cores.

- Process pinning improves floating-point performance by guaranteeing that each process is on its own core and eliminating the possibility that two processes landing on the same core.

- Process pinning optimizes data movement among the processes, by placing processes that communicate in Non-Uniform Memory Access (NUMA) domain nodes, which guarantees they have the lowest latency and highest bandwidth.

- Process pinning reduces OS overhead and gives you more consistent results because the OS cannot move processes to different cores or NUMA domains.

## Where do you pin processes and threads?

To determine where is the right location to pin processes and threads, we first need to understand the processor and memory topology and specifically the number and location of the NUMA domains.

The **lstopo-no-graphics** utility (from the hwloc RPM) and the **Intel Memory Latency Checker (MLC)** are useful tools to determine the processor and memory topology. For example: How many NUMA domains does the VM have? Which cores are members of each NUMA domain? What is the latency and bandwidth for processes in each NUMA domain as they communicate with each other?

The following image displays the HB120_v2 NUMA domain latency map generated by Intel MLC. The lower the latency between NUMA domains, the faster the communication will be between them. The following illustration clearly shows that HB120_v2 has 30 NUMA domains, which NUMA domains are on which socket, and which NUMA domains can be grouped together to achieve the lowest data transfer and communication latency.

[![HB120_v2 NUMA domain latency map](../media/4-hb120_v2-numa-map.png)](../media/4-hb120_v2-numa-map-large.png#lightbox)

Intel processors have six memory channels and AMD EPYC processors have eight memory channels. We need to make sure we use all memory channels to maximize the available memory bandwidth. We can do this by spreading the parallel processes evenly among the NUMA node domains. For hybrid parallel applications keep process/thread grouping in the same NUMA domains, ideally sharing the same l3cache and make sure that the total thread count does not exceed the total number of cores.

The following image illustrates an HC44 SKU with 2 NUMA domains and 44 cores.

![HC44 NUMA domains](../media/4-hc44-numa-domains.png)

The following image illustrates an HB60 SKU with 15 NUMA domains and 60 cores.

![HB60 NUMA domains](../media/4-hb60-numa-domains.png)

## Memory bandwidth bound applications

If you have an application that is bound by memory bandwidth, you may get better performance on the VM by reducing the number of parallel processes and threads in each NUMA node domain, thereby providing more memory bandwidth per process and possibly a reduction in wall-time.

For example, if you're using HB120_v2 SKU with 30 NUMA node domains, you could try running 1, 2 and 3 processes and threads per NUMA node domain (for example: 30, 60 and 90 processes and threads per VM) and see which configuration gives the best performance.
