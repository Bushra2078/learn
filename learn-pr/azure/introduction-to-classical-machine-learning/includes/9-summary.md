Well done for getting through all of that! Letâ€™s have a quick recap of what we've covered.

* Supervised learning is a kind of learning by example: a model makes predictions, they're compared to expected labels, and the model is then updated to produce better results.
* A cost function is a mathematical way to describe what we want a model to learn. These calculate large numbers when a model isn't making good predictions, and small numbers when it's performing well.
* Gradient descent is an optimization algorithm: a way of calculating how to improve a model, given a cost function and some data.
* Step size (learning rate) changes how quickly and how well gradient descent performs.
