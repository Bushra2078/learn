

With Azure Synapse Analytics pipelines, you can orchestrate data transfer and transformation activities and build data integration solutions across multiple systems. When working with analytical data in a data lake, Apache Spark provides a scalable, distributed processing platform that you can use to process huge volumes of data efficiently.

The **Synapse Notebook** activity enables you to run data processing code in Spark notebooks as a task in a pipeline. This makes it possible to automate big data processing and integrate it into *extract, transform, and load* (ETL) workloads.