In the context of analytics, data streams are event data generated by sensors or other sources that can be analyzed by another technology. Analyzing a data stream is typically done to measure the state change of a component or to capture information on an area of interest. The intent of collecting the sensor data and analyzing it is to:

- Continuously analyze data to detect issues and understand or respond to them.
- Understand component or system behavior under various conditions to fuel further enhancements of said component or system.
- Trigger specific actions when certain thresholds are identified.

In today's world, data streams are ubiquitous. Various industries harness the latent knowledge in data streams to improve efficiencies and further innovation. Examples of use cases that analyze data streams include:

:::row:::
    :::column:::
- Stock market trends
- Monitoring data of water pipelines, and electrical transmission and distribution systems by utility companies
- Mechanical component health monitoring data in automotive and automobile industries
- Monitoring data from industrial and manufacturing equipment
    :::column-end:::
    :::column:::
- Sensor data in transportation such as traffic management and highway toll lanes
- Patient health monitoring data in the healthcare industry
- Satellite data in the space industry
- Fraud detection in the banking and finance industries
- Sentiment analysis of social media posts
    :::column-end:::
:::row-end:::

Streaming data can also be collected over time, persisted in storage as static data, and compute run over it on demand to perform streaming analysis. Static data processing can require a lot of storage.  In contrast, live data streams have relative lower storage requirements but require higher compute power because compute is run in sliding windows over continuously incoming data; with insights then delivered, and the data discarded once the analysis completes.