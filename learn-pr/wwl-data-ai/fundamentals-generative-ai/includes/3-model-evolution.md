Between 2010 and 2020, language modeling approaches increased in sophistication. One of the key advances in natural language processing was the development of embedding techniques in which text is broken down into tokens (words, phrases, or even partial words) and each token is assigned a vector value. In 2013, this approach was encapsulated in the Word2Vec algorithm, which uses a deep learning model to analyze a large corpus of text and assign vector values to each discrete word.

Improvements in model design progressed models like Word2Vec to large language models like BERT and GPT. Commonly used large language models in Microsoft generative AI technologies include GPT-3 models for text and code-generation and GPT 4 models, next generation GPT optimized for chat interactions. These models can be used to design new models for specific purposes, such as DALL-E, which is based on GPT but designed for image generation. 

To get a sense of a magnitude of difference between models, the GPT-3 model contains 175B parameters (weights) while GPT-4 contains trillions. These models are developed by OpenAI, a company that partners closely with Microsoft to develop LLMs and other AI solutions. OpenAI is best known as the company behind ChatGPT, an AI chatbot app based on a GPT LLM. Microsoft's partnership with OpenAI enables Azure OpenAI users to access the latest language model innovations.

## Considerations 

There are several concepts useful for understanding the constraints to language modeling and its evolution. 

When making business decisions about what type of model to use, it's important to understand how time and compute needs factor into machine learning training. 

In order to produce an effective machine learning model, the model needs to be trained with a substantial amount of cleaned data. The 'learning' portion of training requires a computer to identify an algorithm that best fits the data. The complexity of the task the model needs to solve for and the desired level of model performance all factor into the time required to run through possible solutions for a best fit algorithm.

