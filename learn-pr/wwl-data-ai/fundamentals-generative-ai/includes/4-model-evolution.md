<<<<<<< HEAD
=======
Between 2010 and 2020, language modeling approaches increased in sophistication from models like Word2Vec to large language models like BERT and GPT. Commonly used large language models in Microsoft generative AI technologies include GPT 3 models for text and code-generation and GPT 4 models, next generation GPT optimized for chat interactions. Through transfer learning, these models can be used to design new models for specific purposes, such as DALL-E, which is based on GPT but designed for image generation. 

To get a sense of a magnitude of difference between models, the GPT-3 model contains 175B parameters (weights) while GPT-4 contains trillions. These models are developed by OpenAI, a company that partners closely with Microsoft to develop LLMs and other AI solutions. OpenAI is best known as the company behind ChatGPT, an AI chatbot app based on a GPT LLM. 

## Considerations 

There are several concepts useful for understanding the constraints to language modeling and its evolution. 

When making business decisions about what type of model to use, it's important to understand how time and compute needs factor into machine learning training. 

In order to produce an effective machine learning model, the model needs to be trained with a substantial amount of cleaned data. The 'learning' portion of training requires a computer to identify an algorithm that best fits the data. The complexity of the task the model needs to solve for and the desired level of model performance all factor into the time required to run through possible solutions for a best fit algorithm.

>>>>>>> d150372fa754121bcd7a350f5cc3bad59ffc3b83
