The quality of responses that a generative AI application returns not only depends on the model itself, but on the types of prompts it is given. Both engineers who design applications and consumers who use those applications can improve the quality of responses from generative AI by considering prompt structure. You will often hear of the term *prompt engineering* used to describe the process of prompt improvement. 

Prompts are ways we tell an application what we want it to do. An engineer can add instructions for what the program will do with prompts. For example, engineers may build a generative AI application for teachers to create multiple-choice questions related to text students read. During the development of hte application, engineers can add additional rules for what the program should do with the prompts it receives. One way to do this is by including **grounding data**, data specific to the user, to contextualize a prompt. Another way to do this is by including instructions on the voice in which to respond - friendly, formal, academic etc.  

Consumers of the application can also control the response of generative AI applications by using specific language and leading the prompt with instructive language such as "tell me", "create", "list", "explain".   

Submitting clear, specific prompts produces better results. You can see some examples [here](https://learn.microsoft.com/en-us/training/modules/apply-prompt-engineering-azure-openai/3-write-effective-prompts).




