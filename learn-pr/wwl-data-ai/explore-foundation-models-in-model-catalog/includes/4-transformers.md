The latest breakthrough in **Natural Language Processing** (**NLP**) is owed to the development of the **Transformer** architecture.

Transformers were introduced in the [*Attention is all you need* paper by Vaswani, et al. from 2017](https://arxiv.org/abs/1706.03762?azure-portal=true). The transformer architecture introduced two concepts that advanced the field of NLP: **positional encoding** and **multi-headed attention**.

## Understand positional encoding


## Understand multi-headed attention


## Explore the Transformer architecture
