The Computer Vision service and API in the Microsoft Azure Cognitive Services offerings, provides pre-built algorithms that can process images that you upload. The service can return specific information about the content of the image supplied.

## Azure resources for Computer Vision

The first step towards using the Computer Vision service is to create a resource for it in your Azure subscription. You can use either of the following resource types:

- **Computer Vision**: A specific resource for the Computer Vision service. Use this resource type if you don't intend to use any other cognitive services, or if you want to track utilization and costs for your Computer Vision resource separately.
- **Cognitive Services**: A general cognitive services resource that includes Computer Vision along with many other cognitive services; such as Text Analytics, Translator Text, and others. Use this resource type if you plan to use multiple cognitive services and want to simplify administration and development.

Whichever type of resource you choose to create, it will provide two pieces of information that you will need to use it:

- A **key** that is used to authenticate client applications.
- An **endpoint** that provides the HTTP address at which your resource can be accessed.

> [!NOTE]
> If you create a Cognitive Services resource, client applications use the same key and endpoint regardless of the specific service they are using.

## Analyzing an image for insights

Exploring the many features that Computer Vision offers as pre-built functionality, will help you understand the variety of information that the service can provide.

### Describing an image

Computer Vision has the ability to analyze an image, evaluate the objects that are detected, and then generate a human-readable phrase or sentence that can describe what was detected in the image. Depending on the image contents, you may find multiple results, or phrases, returned.  Each returned phrase will have a confidence score next to it, based on how confident the algorithm is in the supplied description.  The highest confidence phrases will be listed first, or at the top of the returned results.

To help you understand this concept, consider the following image of the Empire State building in New York. The returned phrases are listed below the image in the order of confidence.

![black and white aerial picture of New York City with Empire State Building](../media/black-white-buildings.png)

- A black and white photo of a city
- A black and white photo of a large city
- A large white building in a city

### Tagging visual features

The image descriptions generated by Computer Vision are based on a set of thousands of recognizable objects from which it can compare to suggest *tags* for the image.  These tags encompass living things, scenery, and even actions.  The tagging is capable of identifying main subjects in the image, which may be the largest object in the image, but is also capable if detecting other recognizable objects in the background.  The tagging is also capable of identifying settings for the image such as knowing if the image has indoor or outdoor scenes.

For example, the tags returned for the Empire State building image include:

- skyscraper
- tower
- building

### Detecting objects

The object detection capability is similar to tagging, in that the service can identify common objects; but rather than tagging, or providing tags for the recognized objects only, this service can also return what is known as bounding box coordinates.  Not only will you get the type of object, but you will also receive a set of coordinates that indicate the top, left, width, and height of the object detected, which you can use to identify the location and size of the object in the lime, like this:

![black and white aerial picture of New York City with Empire State Building](../media/black-white-buildings-objects.png)

If you provide an image that contains a family in a park with a dog, for example, the object detection would return the dog as an identified object and list the coordinates for the dog's location in the image but it will also return a person, or people object, with all the coordinates where the people were detected.  In this way, you can know if the image contains multiple instances of a certain object, and where in the image they are located.

Consider a security system that uses cameras for perimeter security.  If you have motion detection enabled, you don't want to be alerted every time a squirrel or a small animal wanders past the area of view.  Instead, you would expect the system to recognize people and alert you only when a person is detected.  The bounding box coordinates could help the system to move and focus the camera on that specific area of the video, or even take a picture of only the person.

### Detecting brands

This feature provides the ability to identify commercial brands.  The service has an existing database of thousands of globally recognized logos from commercial brands of products.

When you call this service and pass it an image, it will perform a detection task and determine if any of the identified objects in the image are recognized brands.  Recall that it compares the brands against its database of popular brands spanning clothing, consumer electronics, and many more categories.  If a known brand is detected, the service will return a response that contains the brand name, a confidence score (from 0 to 1 indicating how positive the identification is), and a bounding box (coordinates) for where in the image the detected brand was found.

### Categorizing an image

Computer Vision can actually categorize an entire image.  The service uses a parent/child hierarchy with a "current" limited set of categories.  When analyzing an image, detected objects are compared to the existing categories to determine the best way to provide the categorization.  As an example, one of the parent categories is **people_**. the image of a person on a roof with have a category of **people_**.

![person on a commercial building roof](../media/woman-roof.png)

You would see a slightly different categorization returned for the following image, **people_group**, because there are multiple people in the image:

![family picture with father, mother, son, and daughter](../media/family-photo.png)

Review the 86-category list [here](https://docs.microsoft.com/azure/cognitive-services/computer-vision/category-taxonomy).

### Detecting faces

There is a dedicated Face API, Computer Vision includes a subset of that functionality to aid in face detection.  The capabilities include detecting the face but also includes the ability to determine age, gender, and a bounding box rectangle for the location of the face(s).  For more detailed information on the detected faces, use the [Face Service](https://docs.microsoft.com/azure/cognitive-services/face/) instead.

### Detecting image types

The ability to detect the content type of images, is handled by the Analyze Image API in Computer Vision.  The content type of an image refers to the image being clip art or a line drawing.  If clip art is detected the service will return a value from 0 to 3 indicating how likely the image is of the type *clip art*.  The values are shown here to help you understand the results:

- 0 indicates the service doesn't think the image is clip art
- 1 indicates ambiguity.  In other words, the service isn't sure if the image type is clip art.  Typically there may not be sufficient information in the image to make a determination
- 2 would represent normal clip art
- 3, being the highest value, is used to represent good clip art (good meaning a good representation of the clip art category)

For line drawings, the service will only return a boolean value, either 0 or 1, indicating if it believes the image is a line drawing.  The value 0 indicates a false, meaning the service hasn't detected the image as being a line drawing.

### Detecting domain-specific content

This feature relies on models that been trained on specialized data. The service currently supports two domain-models, celebrities and landmarks.  Celebrities are supported for images that are classified in the **people_** category.   The service offers two methods, scoped analysis or enhanced categorization.  Let's evaluate each method.

#### Scoped analysis

You can analyze an image using only the chosen domain-specific model by calling the ```Models/<model>/Analyze``` API.  You replace the ```<model>``` portion with the domain you are looking at such as ```celebrities```.  If a known celebrity is detected in the image, the name of the celebrity is returned along with a confidence score (between 0 and 1).  Like other Cognitive Services, the confidence score indicates how positive the service is on the indicated value.  The higher the value, the more positive.

#### Enhanced categorization analysis

You can also use domain-specific models to supplement general image analysis by specifying domain-specific models in the details parameter of the Analyze API call.

In this case, the [86-category taxonomy](https://docs.microsoft.com/azure/cognitive-services/computer-vision/concept-categorizing-images) classifier is called first. If any of the detected categories have a matching domain-specific model, the image is passed through that model as well and the results will include the additional information.  

As an example, if we passed in an image that contains Microsoft CEO, Satya Nadella, the results returned would indicate the category of **people_** with its confidence score, and then under that category we would find the **detail** section listing **celebrities**, Satya's name, and then a confidence score for this detail.

```json
"categories":[
  {
    "name":"abstract_",
    "score":0.00390625
  },
  {
    "name":"people_",
    "score":0.83984375,
    "detail":{
      "celebrities":[
        {
          "name":"Satya Nadella",
          "faceRectangle":{
            "left":597,
            "top":162,
            "width":248,
            "height":248
          },
          "confidence":0.999028444
        }
      ]
    }
  }
]
```

### Detecting color scheme

When you pass an image to the Detect Color API, Computer Vision will analyze the image for three main attributes. The attributes are:

- dominant foreground color
- dominant background color
- dominant colors for whole image.  

The colors are limited to the following 12 colors:

- black
- brown
- blue
- gray
- green
- orange
- pink
- purple
- red
- teal
- white
- yellow

The following image is an example that was sent to the API.  

![image of a daisy with green, blurred background](../media/flower.png)

The dominant colors returned are listed as:

- Foreground: Black
- Background: White
- Colors: Black, White, Green

This API can also return accent colors, using their RGB values such as #474A84.  RGB color generators on the Internet can help you decipher these values if you're curious about them.  The API uses a boolean value indicating if the image is black and white or color.  True indicates the image is black and white.

### Generating a thumbnail

You can pass images to the Computer Vision service and have it generate reduced-size representations of the images, known as thumbnails.  Thumbnails help conserve space and you might find them used in a grocery store scenario where thumbnail images are used to represent fruit categories.  Clicking the thumbnail image might lead to another page with a larger image and detailed descriptions and prices, for example.

The Computer Vision thumbnail generation algorithm works as follows:

1. Remove distracting elements from the image and identify the area of interest (the area of the image in which the main subject appears).
2. Crop the image based on the identified area of interest.
3. Change the aspect ratio to fit the target thumbnail dimensions.

## Other features

Computer Vision can extract text from images using Optical Character Recognition (OCR) and provide some content moderation.  These additional features are not covered in this content.  For more information, visit the [OCR page](https://docs.microsoft.com/azure/cognitive-services/computer-vision/concept-recognizing-text#ocr-optical-character-recognition-api) for text extraction or the [adult content page](https://docs.microsoft.com/azure/cognitive-services/computer-vision/concept-detecting-adult-content) for content moderation.
