Synapse Studio comes with an integrated notebook experience. 
The notebooks in Synapse studio, are a web interface that enables you to create, edit, or transform data in the files. 
It is based on a live code experience, including visualizations and narrative text. 

If you'd like to experiment with your data and gain some insights about the data, notebooks are a good way to start and validate some of the ideas you might have. 

The look and feel of the integrated notebook experience is similar to, for example,  the jupyter notebooks in Azure Machine Learning Service or other IDEs you might use and interact with on your data. 

If you navigate to the Synapse studio environment, you can find the notebooks in the Development Hub of the studio experience. 
To access the studio environment, you can navigate to the Azure Synapse Analytics Workspace and launch the studio. 
You'll also find that there are some notebook examples available through the Knowledge Center. 

The notebooks allow you to write multiple languages in one notebook by using the magic commands expressed by %%<Name of Language>

The visual aspects of the notebooks are
* Support for Language Syntax highlight
* Syntax error
* Syntax code completionâ€‹
* Export results 

Within the notebook environment of the Azure Synapse Analytics Studio, you have the possibility tc create temporary tables across the multiple languages you might use.  

In order to ingest data through notebooks you can use a linked service from the workspace, to, for example,  an Azure Data Lake storage where then the keys and access are automatically passed through to the storage account where you have stored the file that you want to ingest or read out into a spark DataFrame.

You can access data in the primary storage account directly. There's no need to provide the secret keys. In the Data tab on the left hand-side in the Synapse Workspace, right-click on a file and select New notebook to see a new notebook with data extractor autogenerated.

With an Azure Synapse Studio notebook, you can:

* Get easily started.
* Keep data secure with built-in enterprise security features.
* Analyze data across raw formats (CSV, txt, JSON, etc.), processed file formats (parquet, Delta Lake, ORC, etc.), and SQL tabular data files against Spark and SQL.
* Be productive with enhanced authoring capabilities and built-in data visualization.

