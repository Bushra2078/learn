
The societal implications of AI and the responsibility of organizations to anticipate and mitigate unintended consequences of AI technology are significant. Considering this responsibility, organizations are finding the need to create internal policies and practices to guide their AI efforts, whether they are deploying third-party AI solutions or developing their own. We'll hear from a few executives on how they have formed responsible AI principles in their organizations.

**In this module, you will learn to:**

* Describe the importance of engaging with AI in a responsible manner.
* Identify six guiding principles to develop and use AI responsibly.
* Describe different approaches to responsible AI governance from experienced executives

We also recognize that as organizations and as a society, our steps towards responsible AI will need to continually evolve to reflect new innovations and lessons from both our mistakes and accomplishments. The processes, tools, and resources mentioned in these units can be a starting point from which organizations create their own AI strategy.

As the use of AI increases across the private and public sectors, it is essential that we continue to foster open dialogue among businesses, governments, Non-Governmental Organizations (NGOs), academic researchers, and all other interested individuals and organizations. Organizations that embrace AI early have an important role to play in promoting the responsible use of AI and preparing society for its impacts. Their firsthand experience in dealing with the ethical challenges of AI will be crucial knowledge for later adopters and those individuals who are trying to study or regulate AI technology.

Next, letâ€™s explore the guiding principles of responsible AI by listening to Brad Smith, President and Chief Legal Officer at Microsoft, as Smith shares some key learnings from our own AI journey.
