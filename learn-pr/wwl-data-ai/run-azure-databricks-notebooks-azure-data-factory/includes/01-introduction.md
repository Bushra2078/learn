Azure Databricks enables data engineers to use code in notebooks to ingest and process data. While notebooks are designed to be used interactively, you can use them to encapsulate activities in a data ingestion or processing pipeline that is orchestrated using Azure Data Factory.

In this module, you'll learn how to:

- Describe how Azure Databricks notebooks can be run in a pipeline.
- Create an Azure Data Factory linked service for Azure Databricks.
- Use a Notebook activity in a pipeline.
- Pass parameters to a notebook.

> [!NOTE]
> While this module focuses on using Azure Databricks notebooks in an Azure Data Factory pipeline, the same principles and techniques apply when using an Azure Synapse Analytics pipeline.
