In this module, you learned how to:

- Deploy a model as a real-time inferencing service. You registered a trained model, defined both an inference and deployment configuration, and finally, deployed the model.
- Consume a real-time inferencing service by using both the Azure Machine Learning SDK and REST endpoint.
- Troubleshoot a service deployment by checking the service state, reviewing service logs, and deploying to a local container.
