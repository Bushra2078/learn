Imagine you're working as a data engineer for a Fortune 500 enterprise. Your organization has been using Azure Databricks heavily for large-scale data processing and data science workloads for a couple of years. Given this high workload and exponential data growth that comes with it, it has become apparent that your Databricks environment might not be configured using best practices. Each job takes increasingly longer to execute with each passing month. Your organization's technical leadership has tasked your team with researching best practices and to put those principles in place.

Planning, deploying, and running Azure Databricks (ADB) at scale requires one to make many architectural decisions.

While each ADB deployment is unique to an organization's needs, some patterns are common across most successful ADB projects. Unsurprisingly, these patterns are also in-line with modern Cloud-centric development best practices.

This module summarizes these patterns into prescriptive and actionable best practices for Azure Databricks. In it, you will find guidance on administering the workspace, applying security best practices, using tools and integrations, tweaking the Databricks Runtime, configuring high availability and disaster recovery (HA/DR), and creating and managing clusters.

The audience for this module include system architects, field engineers, and development teams that use Azure Databricks.

## Learning objectives 

In this module, you will learn best practices in the following categories:

- Workspace administration
- Security
- Tools & integration
- Databricks runtime
- HA/DR
- Clusters

## Prerequisites

None