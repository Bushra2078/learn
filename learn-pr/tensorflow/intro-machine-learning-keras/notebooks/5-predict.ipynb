{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import requests\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-08-24 23:50:34.471189: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assuming we achieved pretty good accuracy during the training and testing phases, we can now use the trained model for inference &mdash; in other words, to predict the classification of images that the network has never seen before.\n",
        "\n",
        "But before we move on, we'll load the code you've already seen in previous notebooks:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "!wget -Nq https://raw.githubusercontent.com/MicrosoftDocs/tensorflow-learning-path/main/intro-keras/kintro.py\n",
        "from kintro import *"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember that in the previous notebook, after successfully training our network, we saved the model. Let's load it back into memory."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "  model = tf.keras.models.load_model('outputs/model')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making a prediction is easy &mdash; we simply call the model's `predict` method and pass one or more images. Let's start by geting the image we'll use for prediction, and let's display it:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "url = 'https://raw.githubusercontent.com/MicrosoftDocs/tensorflow-learning-path/main/intro-keras/predict-image.png'\n",
        "\n",
        "with Image.open(requests.get(url, stream=True).raw) as image:\n",
        "  X = np.asarray(image).reshape((-1, 28, 28)) / 255.0\n",
        "\n",
        "plt.figure()\n",
        "plt.axis('off')\n",
        "plt.imshow(X.squeeze(), cmap='gray')\n",
        "plt.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"231.84pt\" version=\"1.1\" viewBox=\"0 0 231.84 231.84\" width=\"231.84pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-24T23:51:31.979370</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 231.84 \nL 231.84 231.84 \nL 231.84 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g clip-path=\"url(#p264a84222d)\">\n    <image height=\"218\" id=\"image0c9cc0f62e\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"7.2\" xlink:href=\"data:image/png;base64,\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\" y=\"-6.64\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p264a84222d\">\n   <rect height=\"217.44\" width=\"217.44\" x=\"7.2\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "image/png": "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"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, we have an image of an ankle boot. In the code below, we load the image, call `predict` to get its class index, and map that index to the class name.  "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "predicted_vector = model.predict(X)\n",
        "predicted_index = np.argmax(predicted_vector)\n",
        "predicted_name = labels_map[predicted_index]\n",
        "\n",
        "print(f'Predicted class: {predicted_name}')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that you can also get probabilities of the input image being of a certain class, in which case we need to normalize the output of our network using `softmax` to get probabilities. Here are the predictions for our image: "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "probs = tf.nn.softmax(predicted_vector.reshape((-1,)))\n",
        "for i,p in enumerate(probs):\n",
        "    print(f'{labels_map[i]} -> {p:.3f}')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T-Shirt -> 0.102\n",
            "Trouser -> 0.002\n",
            "Pullover -> 0.001\n",
            "Dress -> 0.870\n",
            "Coat -> 0.001\n",
            "Sandal -> 0.000\n",
            "Shirt -> 0.023\n",
            "Sneaker -> 0.000\n",
            "Bag -> 0.001\n",
            "Ankle Boot -> 0.000\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you need to compute probabilities often, you can specify `activation='softmax'` for the final `Dense` layer of your network. In this case the network would give you probabilities as output, and you need to omit `use_logits=True` in the `SparseCategoricalCrossentropy` loss function. "
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "a7d8d32a02de2fe32a77a4e581138922e011c09664b6c2991156e76c4176efab"
    },
    "kernelspec": {
      "display_name": "py37_tensorflow",
      "language": "python",
      "name": "conda-env-py37_tensorflow-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}