Conversational AI bots often have diverse requirements for their users. For example, you may want a bot to check with a human operator before communicating certain information to a user. As users trust is hard to gain, and easy to lose it, it’s important to ensure AI bots are fair and secure.

Suppose you work at a healthcare company that produces bots for patients to ask questions they might not feel comfortable asking a doctor. You have a moral and legal obligation to keep patient information secret and not give out incorrect healthcare advice. A patient provides the bot with enough information that the likelihood of a cancer diagnosis is high. While designing this healthcare bot, you must decide how to keep the data provided by the patient private and secure, while also flagging similar situations where the case that must be reviewed by a qualified healthcare professional.

To help, Microsoft has created special guidelines for the development of responsible Conversational AI.

This module reviews the Microsoft guidelines for safe and fair development of responsible Conversational AI. You will review guidelines and see how you can keep users trust when your bot encounters difficult situations.

You will then review how to ensure your bot is diverse, and actions you can take to prevent unintended consequences.

**In this module, you will learn to:**

* Identify potential biases that your bot may contain
* Ensure your bot respects users privacy
* Discuss situations where human supervision may be necessary

Let’s explore the Microsoft guidelines that will help guide you to build fair and secure Conversational bots.
