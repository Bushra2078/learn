Conversational AI bots often have diverse requirements. For example, you may need a bot to check with a human operator before communicating personal financial information to a user, and restrict third-party access to that information. As user's trust is hard to gain, and easy to lose it, it’s important to ensure bots are designed to be fair and secure.

Suppose you work at a healthcare company, and you want to develop a bot for patients to ask questions they don't feel comfortable discussing with a doctor. You have ethical and legal obligations to keep patient information secret, and to not give out dangerous healthcare advice. A patient provides the bot with information that gives a high probability of a serious infection. While designing this healthcare bot, you must decide how to flag similar situations that must be reviewed by a qualified healthcare professional, while keeping patient data private and secure.

To help, Microsoft has created special guidelines for the development of responsible Conversational AI.

This module reviews the Microsoft guidelines for safe and fair development of responsible Conversational AI. You will review guidelines and see how you can keep users trust when your bot encounters difficult situations.

You will then review how to ensure your bot is diverse, and actions you can take to prevent unintended consequences.

## Learning objectives

In this module, you will:

* Identify potential biases that your bot may contain
* Ensure your bot respects users privacy
* Analyze situations where human supervision may be necessary

## Prerequisites

None.

Let’s explore the Microsoft guidelines that will help guide you to build fair and secure Conversational bots.
