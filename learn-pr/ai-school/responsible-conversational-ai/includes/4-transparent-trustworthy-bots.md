*Make sure you understand why your bot exists — and understand the consequences of using it.*

Before you write a single line of code, you need to understand clearly how your bot will benefit the end user. Take into account the needs of other stakeholders, even those stakeholders who aren’t directly interacting with the bot.

You also need to understand exactly how your bot will benefit the bot’s owner, be that you, or the customer you're developing the bot for.

To do this, you need to define the purpose of the bot. Can its purpose be performed responsibly? If not, don’t use a bot — or go back to the drawing board. If so, you need to ensure that you have access to relevant expertise that will enable your bot to carry out its function.

The next important consideration is whether the bot will be put to a use that is consequential. For a basic understanding, ask yourself: will using this bot impact on services like healthcare, education, employment, or finances? Will using the bot have other meaningful or significant impacts on the user’s daily life? Things like financial planning, or healthcare diagnoses, may require human judgment, empathy, or expertise, even in the early stages of engagement.

To make a decision about how Conversational AI should be deployed in these cases — or whether it should be used at all — you will need to assess and consider what it would mean for your bot to have an error, or fail, at this sort of task. If the cost is too high, or error is too likely, it may be that what you’re looking at isn’t a suitable use case for Conversational AI.

## Make sure your users know they’re working with a bot

The Turing test is a famous thought experiment, named after computing pioneer Alan Turing. It's often thought to be a test as to whether a machine can think - but that’s not quite right. The idea of a Turing test is to see if a machine can win the “imitation game” — to fool a human into thinking that the bot they're talking to is in fact another human.

In today’s world, machines that use Conversational AI are capable of passing this kind of test. Conversational AI services and frameworks, such as Microsoft’s Language Understanding (LUIS), have comprehensive natural language capabilities and can easily be programmed to have the appearance of a human personality. If a real person is unaware they’re talking to a bot, they might easily believe — at least temporarily — that they are talking to another human being.

This is ethically problematic for a number of reasons, one of which is that it undermines trust in bots. If a bot is not “honest” about the fact that it is a bot, a user that discovers the truth might feel upset or even frightened or threatened by the fact they at first thought they were talking to a human. Perhaps worse, if a human user never realizes they were talking to a bot, it raises questions over all the actions taken by the human user, as it is likely they would have acted differently had they known they were talking to a bot.

For bots to be helpful to people, they must be able to be trusted. To be trusted, the developers of bots must make sure that users know that they are interacting with a computer program, and not a human being. There are a number of design choices that can be made that encourage this understanding, without undermining user experience.

## Let users know the purpose of the bot

This design choice allows users to interact with the bot in a satisfying way that helps them achieve their aims. The more up-front and specific you are with the user about what the bot is intended to do, the better the user’s experience will tend to be. They’ll have an easier time finding and using it, and they’ll feel confident that it represents you, or your business.

## Assess user satisfaction

You should always aim to improve your bot’s performance, to help it better serve users. One way to do this is by assessing the bot’s performance. Good measurements to take include whether the user feels that the bot served its stated purpose, and the user’s sense of well-being and comfort while they’re talking to the bot. User satisfaction can easily be measured through a quick yes/no or Net Promoter Score (NPS) style survey after an interaction reaches its conclusion, and while the user’s sense of well-being or comfort can be measured in a number of ways, one of which might be running a sentiment analysis on the words that the human user uses during the interaction. Human users often treat bots extremely differently to how they treat human operators. For instance, expressing frustration by abusing or swearing at an unhelpful bot is common! If you track this (in a way that doesn’t violate user privacy!) you'll have an excellent working understanding of how well your bot is serving its users.

Bots are enormously powerful tools, but those that design and deploy them have an equally enormous responsibility to make sure that they are used well. If your bot is built with transparency in mind, it will gain user trust more easily. They’ll be more likely to use and recommend it, and you’ll find it easier to improve your bot as the user-base grows.
