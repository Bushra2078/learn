Congratulations on building an audio binary classification speech model!  

We've covered the basics of building an audio machine learning. From understanding how an analog audio turns to digital sound, to creating spectrogram images of our wave files. We used the PyTorch Speech Commands dataset, parsed the classes down to `yes` and `no`, then looked at ways we can understand and visualize audio data patterns. From there, we took the spectrograms, created images and used a convolutional neural network to build our model.

You can expand on this knowledge by looking at other datasets/sounds, the `MFCC` transformer and build your model.

Be sure to check out the other machine learning modules with [PyTorch Fundamentals Learning Path](/learn/paths/pytorch-fundamentals), and learn more:

[!include[](../../../includes/open-link-in-new-tab-note.md)]

* [Introduction to PyTorch](/learn/modules/intro-machine-learning-pytorch)
* [Computer Vision with PyTorch](/learn/modules/intro-computer-vision-pytorch)
* [Natural Language Processing with PyTorch](/learn/modules/intro-natural-language-processing-pytorch)

Happy learning!
