{
 "cells": [
  {
   "attachments": {
    "4-embedding-1.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "In our previous example, we operated on high-dimensional bag-of-words vectors with the length of `vocab_size`, and we were explicitly converting from low-dimensional positional representation vectors into sparse one-hot representations. \n",
    "\n",
    "![Diagram show high-dimensional vector to embedding vector](../images/4-embedding-1.png)\n",
    "\n",
    "The drawbacks of the one-hot encoded vectors representation are:\n",
    "- It is not memory-efficient.  The length of each word vector equals the length of the vocabulary size.  For example, if the vocab length is 10,000, then each word in the input sentence or text sequence will have a vector length of 10,000.  If 1 sentence has 8 words, then the total length of the vectors will be 80,0000.  \n",
    "- Each word is treated independently from each other. For example, the vector does not express any relationship or semantic similarity between words.\n",
    "\n",
    "In this unit, we will continue exploring the **News AG** dataset. To begin, let's load the data and get some definitions from the previous unit.  In addition, we will allocation our training and testing datasets; word vocabulary size; and the category of our word classes: _World_, _Sports_, _Business_ and _Sci/Tech_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torchtext torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/torchnlp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "import numpy as np\n",
    "from torchnlp import *\n",
    "from torchinfo import summary\n",
    "train_dataset, test_dataset, classes, vocab = load_dataset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing word sequences\n",
    "\n",
    "To better understand how embedding represent the relationship of words in a vector, let's look at how word sequences are tokenized.  We'll use our training dataset to view the first 2 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence = train_dataset[0][1]\n",
    "second_sentence = train_dataset[1][1]\n",
    "\n",
    "print(f'First Sentence in dataset:\\n{first_sentence}')\n",
    "print(\"Length:\", len(train_dataset[0][1]))\n",
    "print(f'\\nSecond Sentence in dataset:\\n{second_sentence}')\n",
    "print(\"Length: \", len(train_dataset[1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use PyTorch's `get_tokenizer` to split words and spaces in the sentence apart.  In our case, we'll use `basic_english` for the tokenizer to understand the language structure. This will return a string list of the text and characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "f_tokens = tokenizer(first_sentence)\n",
    "s_tokens = tokenizer(second_sentence)\n",
    "\n",
    "print(f'\\nfirst token list:\\n{f_tokens}')\n",
    "print(f'\\nsecond token list:\\n{s_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how each work maps to the in the vocabulary, we'll achieve this by looping through each word in the list to lookup it's index number in `vocab`.  Each word or character is display with it's corresponding index.  For example, 'the' appears several times in both sentence and it's unique index in the vocab is the number 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lookup = [list((vocab[w], w)) for w in f_tokens]\n",
    "print(f'\\nIndex lockup in 1st sentence:\\n{word_lookup}')\n",
    "\n",
    "word_lookup = [list((vocab[w], w)) for w in s_tokens]\n",
    "print(f'\\nIndex lockup in 2nd sentence:\\n{word_lookup}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with variable sequence size\n",
    "\n",
    "When working with words, you are going to have text sequences or sentences that are of difference length.  This can be problematic in training the word embeddings neural network. For consistency in the word embedding and improve training performance, we would have to apply some padding. This can be done using the `torch.nn.functional.pad` on a tokenizers dataset. It added zeros values to the empty indexs at the end of the vector.\n",
    "\n",
    "![Image showing padding.](../images/3-embedding-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padify(b):\n",
    "    # b is the list of tuples of length batch_size\n",
    "    #   - first element of a tuple = label, \n",
    "    #   - second = feature (text sequence)\n",
    "    # build vectorized sequence\n",
    "    v = [encode(x[1]) for x in b]\n",
    "    # first, compute max length of a sequence in this minibatch\n",
    "    l = max(map(len,v))\n",
    "    return ( # tuple of two tensors - labels and features\n",
    "        torch.LongTensor([t[0]-1 for t in b]),\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the sentences in the our dataset to change into a tokenize vector.  As you will see, the text sequence have different lengths.  Well will apply padding so all the text sequence will have a fixed length.  This approach is used when you have a large set of text sequences in your dataset.\n",
    "\n",
    "- Display the length of the 1st and 2nd sentence.  As you can see book have difference length.  \n",
    "- The fixed length of the dataset tensor is the lenth of the longest sentence length in the entire dataset.\n",
    "- Zeros are added to the empty indexes in the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "labels, features = padify(train_dataset)  \n",
    "print(f'features: {features}')\n",
    "\n",
    "print(f'\\nlength of first sentence: {len(f_tokens)}')\n",
    "print(f'length of second sentence: {len(s_tokens)}')\n",
    "print(f'size of features: {features.size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What is embedding?\n",
    "\n",
    "The idea of **embedding** is the process of mapping words into vectors, which reflects the **_semantic meaning of a word_**. The length of its vectors are the embedding dimensions size. We will later discuss how to build meaningful word embeddings, but for now let's just think of embeddings as a way to lower dimensionality of a word vector. \n",
    "\n",
    "So, embedding layer would take a word as an input, and produce an output vector of specified `embedding_size`. In a sense, it is very similar to `Linear` layer, but instead of taking one-hot encoded vector, it will be able to take a word number as an input.\n",
    "\n",
    "By using embedding layer as a first layer in our network, we can switch from bag-or-words to **embedding bag** model, where we first convert each word in our text into corresponding embedding, and then compute some aggregate function over all those embeddings, such as `sum`, `average` or `max`.  \n",
    "\n",
    "![Image showing an embedding classifier for five sequence words.](./images/embedding-classifier-example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classifier neural network will start with embedding layer, then aggregation layer, and linear classifier on top of it:\n",
    "- `vocab_size` are the size of the total number of words we have in our vocabulary.\n",
    "- `embed_dim` are the length of the words features that show relationships with the input vectors passed in the network.\n",
    "- `num_class` are the number of news categories we are trying to classify this input text sequences into (e.g. World, Sports, Business, Sci/Tech) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x,dim=1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training embedding classifier\n",
    "\n",
    "Now that we have defined proper dataloader, we'll use the `collate_fn` to apply the padify function to the datasets as the training dataset is being loaded in each batch.  As a result, the padding while the data is being loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=padify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train the model using the training function we have defined in the previous unit to run the embedding network.  The training makes sure the word relationships are represented in vector.  The results serve as a vector lookup store based on the unique index tokens from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = EmbedClassifier(vocab_size,32,len(classes)).to(device)\n",
    "train_epoch(net,train_loader, lr=1, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: We are only training for 25k records here (less than one full epoch) for the sake of time, but you can continue training, write a function to train for several epochs, and experiment with learning rate parameter to achieve higher accuracy. You should be able to go to the accuracy of about 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a lock of the EmbedClassifier architecture.   Here you can see, the summary shows the shape of the tensor which includes the vocab size, number of the reduce embedding dimension at 32 and finally the fix size of the padded input word sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO:  format paraments for model summary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "summary(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EmbeddingBag Layer and Variable-Length Sequence Representation\n",
    "\n",
    "In the previous architecture, we needed to pad all sequences to the same length in order to fit them into a minibatch. This is not the most efficient way to represent variable length sequences - another apporach would be to use **offset** vector, which would hold offsets of all sequences stored in one large vector.\n",
    "\n",
    "![Image showing an offset sequence representation](./images/offset-sequence-representation.png)\n",
    "\n",
    "> **Note**: On the picture above, we show a sequence of characters, but in our example we are working with sequences of words. However, the general principle of representing sequences with offset vector remains the same.\n",
    "\n",
    "To work with offset representation, we use [`EmbeddingBag`](https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html) layer. It is similar to `Embedding`, but it takes content vector and offset vector as input, and it also includes averaging layer, which can be `mean`, `sum` or `max`.\n",
    "\n",
    "Here is modified network that uses `EmbeddingBag`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.EmbeddingBag(vocab_size, embed_dim)\n",
    "        self.fc = torch.nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, text, off):\n",
    "        x = self.embedding(text, off)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the dataset for training, we need to provide a conversion function that will prepare the offset vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offsetify(b):\n",
    "    # first, compute data tensor from all sequences\n",
    "    x = [torch.tensor(encode(t[1])) for t in b]\n",
    "    # now, compute the offsets by accumulating the tensor of sequence lengths\n",
    "    o = [0] + [len(t) for t in x]\n",
    "    o = torch.tensor(o[:-1]).cumsum(dim=0)\n",
    "    return ( \n",
    "        torch.LongTensor([t[0]-1 for t in b]), # labels\n",
    "        torch.cat(x), # text \n",
    "        o\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=offsetify, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The offset vector is calculated by first combining the sentences indices into one tensor sequence, then extracting the starting index location of each sentences in the seqence. For example: \n",
    "- The length of the first sentence in the dataset is 29.  Meaning the first index of the offset will be `0`.\n",
    "- The length of the second sentence in the data is 42.  Meaning the second index of the offset will be `29`, where 1st sentence ended. \n",
    "- Since the sentences combine into one, that means third index of the offset will be 29 + 42 = `71`, where the 2nd sentence ended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, features, offset = offsetify(train_dataset)  \n",
    "print(f'offset: {offset}')\n",
    "print(f'\\nlength of first sentence: {len(f_tokens)}')\n",
    "print(f'length of second sentence: {len(s_tokens)}')\n",
    "print(f'size of data vector: {features.size()}')\n",
    "print(f'size of offset vector: {offset.size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that unlike in all previous examples, our network now accepts two parameters: data vector and offset vector, which are of different sizes. Sililarly, our data loader also provides us with 3 values instead of 2: both text and offset vectors are provided as features. Therefore, we need to slightly adjust our training function to take care of that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = EmbedClassifier(vocab_size,32,len(classes)).to(device)\n",
    "\n",
    "def train_epoch_emb(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.CrossEntropyLoss(),epoch_size=None, report_freq=200):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    loss_fn = loss_fn.to(device)\n",
    "    net.train()\n",
    "    total_loss,acc,count,i = 0,0,0,0\n",
    "    for labels,text,off in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        labels,text,off = labels.to(device), text.to(device), off.to(device)\n",
    "        out = net(text, off)\n",
    "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss\n",
    "        _,predicted = torch.max(out,1)\n",
    "        acc+=(predicted==labels).sum()\n",
    "        count+=len(labels)\n",
    "        i+=1\n",
    "        if i%report_freq==0:\n",
    "            print(f\"{count}: acc={acc.item()/count}\")\n",
    "        if epoch_size and count>epoch_size:\n",
    "            break\n",
    "    return total_loss.item()/count, acc.item()/count\n",
    "\n",
    "\n",
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Semantic Embeddings: Word2Vec\n",
    "\n",
    "In our previous example, the model embedding layer learnt to map words to vector representation, however, this representation did not have much semantical meaning. It would be nice to learn such vector representation, that similar words or symonims would correspond to vectors that are close to each other in terms of some vector distance (eg. euclidian distance).\n",
    "\n",
    "To do that, we need to pre-train our embedding model on a large collection of text in a specific way. One of the first ways to train semantic embeddings is called **Word2Vec**.  It help map the probably of a word, base of the contexts from texts in the sequence.  It is based on two main architectures that are used to produce a distributed representation of words:\n",
    "\n",
    " - **Continuous bag-of-words** (CBoW) — in this architecture, we train the model to predict a word from surrounding context. Given the ngram $(W_{-2},W_{-1},W_0,W_1,W_2)$, the goal of the model is to predict $W_0$ from $(W_{-2},W_{-1},W_1,W_2)$.  For example:  **_\"I like my hot dog on a __\"_**.  Here the predict word would be **_\"bun\"_**.\n",
    " - **Continuous skip-gram** is opposite to CBoW. The model uses surrounding window of context words to predict the current word.  For example: you can predict **_dog_** to be more associated with the word **_veterinary_**.\n",
    "\n",
    "CBoW is faster, while skip-gram is slower, but does a better job of representing infrequent words.\n",
    "\n",
    "![Image showing both CBoW and Skip-Gram algorithms to convert words to vectors.](./images/example-algorithms-for-converting-words-to-vectors.png)\n",
    "\n",
    "To experiment with word2vec embedding pre-trained on Google News dataset, we can use **gensim** library. Below we find the words most similar to 'neural'\n",
    "\n",
    "> **Note:** When you first create word vectors, downloading them can take some time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w,p in w2v.most_similar('dog'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also extract vector embeddings from the word, to be used in training classification model (we only show first 20 components of the vector for clarity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.word_vec('play')[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great thing about semantical embeddings is that you can manipulate vector encoding to change the semantics. For example, we can ask to find a word, whose vector representation would be as close as possible to words *king* and *woman*, and as far away from the word *man*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both CBOW and Skip-Grams are “predictive” embeddings, in that they only take local contexts into account. Word2Vec does not take advantage of global context. \n",
    "\n",
    "**FastText** and **GloVe** are other word embeddings techniques that predict the probably of words appearing together.  However, for the scope of this model, we'll focus on gensim. You can play with the example by changing embeddings to FastText and GloVe, since gensim supports "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pre-Trained Embeddings in PyTorch\n",
    "\n",
    "We can modify the example above to pre-populate the matrix in our embedding layer with semantical embeddings, such as Word2Vec. We need to take into account that vocabularies of pre-trained embedding and our text corpus will likely not match, so we will initialize weights for the missing words with random values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed_size = len(w2v.get_vector('hello'))\n",
    "print(f'Embedding size: {embed_size}')\n",
    "\n",
    "net = EmbedClassifier(vocab_size,embed_size,len(classes))\n",
    "\n",
    "print('Populating matrix, this will take some time...',end='')\n",
    "found, not_found = 0,0\n",
    "for i,w in enumerate(vocab.itos):\n",
    "    try:\n",
    "        net.embedding.weight[i].data = torch.tensor(w2v.get_vector(w))\n",
    "        found+=1\n",
    "    except:\n",
    "        net.embedding.weight[i].data = torch.normal(0.0,1.0,(embed_size,))\n",
    "        not_found+=1\n",
    "\n",
    "print(f\"Done, found {found} words, {not_found} words missing\")\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our model. Note that the time it takes to train the model is significantly larger than in the previous example, due to larger embedding layer size, and thus much higher number of parameters. Also, because of this, we may need to train our model on more examples if we want to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case we do not see huge increase in accuracy, which is likely to quite different vocalularies. \n",
    "To overcome the problem of different vocabularies, we can use one of the following solutions:\n",
    "* Re-train word2vec model on our vocabulary\n",
    "* Load our dataset with the vocabulary from the pre-trained word2vec model. Vocabulary used to load the dataset can be specified during loading.\n",
    "\n",
    "The latter approach seems easiter, especially because PyTorch `torchtext` framework contains built-in support for embeddings. We can, for example, instantiate GloVe-based vocabulary in the following manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = torchtext.vocab.GloVe(name='6B', dim=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loaded vocabulary has the following basic operations:\n",
    "* `vocab.stoi` dictionary allows us to convert word into its dictionary index\n",
    "* `vocab.itos` does the opposite - converts number into word\n",
    "* `vocab.vectors` is the array of embedding vectors, so to get the embedding of a word we need to use `vocab.vectors[vocab.stoi[s]]`\n",
    "\n",
    "Here is the example of manipulating embeddings to demonstrate the equation **kind-man+woman = queen** (I had to tweak the coefficient a bit to make it work):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the vector corresponding to kind-man+woman\n",
    "qvec = vocab.vectors[vocab.stoi['king']]-vocab.vectors[vocab.stoi['man']]+1.3*vocab.vectors[vocab.stoi['woman']]\n",
    "# find the index of the closest embedding vector \n",
    "d = torch.sum((vocab.vectors-qvec)**2,dim=1)\n",
    "min_idx = torch.argmin(d)\n",
    "# find the corresponding word\n",
    "vocab.itos[min_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the classifier using those embeddings, we first need to encode our dataset using GloVe vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offsetify(b):\n",
    "    # first, compute data tensor from all sequences\n",
    "    x = [torch.tensor(encode(t[1],voc=vocab)) for t in b] # pass the instance of vocab to encode function!\n",
    "    # now, compute the offsets by accumulating the tensor of sequence lengths\n",
    "    o = [0] + [len(t) for t in x]\n",
    "    o = torch.tensor(o[:-1]).cumsum(dim=0)\n",
    "    return ( \n",
    "        torch.LongTensor([t[0]-1 for t in b]), # labels\n",
    "        torch.cat(x), # text \n",
    "        o\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen above, all vector embeddings are stored in `vocab.vectors` matrix. It makes it super-easy to load those weights into weights of embedding layer using simple copying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = EmbedClassifier(len(vocab),len(vocab.vectors[0]),len(classes))\n",
    "net.embedding.weight.data = vocab.vectors\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our model and see if we get better results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, collate_fn=offsetify, shuffle=True)\n",
    "train_epoch_emb(net,train_loader, lr=4, epoch_size=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the reasons we are not seeing significant increase in accuracy is due to the fact that some words from our dataset are missing in the pre-trained GloVe vocabulary, and thus they are essentially ignored. To overcome this fact, we can train our own embeddings on our dataset. \n",
    "\n",
    "\n",
    "## Training your own embeddings\n",
    "\n",
    "In our examples, we have been using pre-trained semantic embeddings, but it is interesting to see how those embeddings can be trained using either CBoW, or Skip-gram architectures. This exercise goes beyond this module, but those interested can reference Word Embeddings tutorials on Pytorch's website. Also, **gensim** framework can be used with Pytorch to train most commonly used embeddings in a few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Embeddings\n",
    "\n",
    "One key limitation of tradition pretrained embedding representaitons such as Word2Vec is the problem of word sense disambigioution. While pretrained embeddings can capture some of the meaning of words in context, every possible meaning of a word is encoded into the same embedding. This can cause problems in downstream models, since many words such as the word 'play' have different meanings depending on the context they are used in.\n",
    "\n",
    "For example word 'play' in those two different sentences have quite different meaning:\n",
    "- I went to a **play** at the theatre.\n",
    "- John wants to **play** with his friends.\n",
    "\n",
    "The pretrained embeddings above represent both of these meanings of the word 'play' in the same embedding. To overcome this limitation, we need to build embeddings based on the **language model**, which is trained on a large corpus of text, and *knows* how words can be put together in different contexts. Discussing contextual embeddings is out of scope for this tutorial, but we will come back to them when talking about language models in the next unit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
