
Organizations that practice DevOps want to view mistakes, and errors with a goal of *learning*. Having blameless postmortems on outages and accidents are part of goal.

## A Blameless Post-Mortem

Having a *Just Culture* means that you’re making an effort to balance safety and accountability. It means that by investigating mistakes in a way that focuses on the situational aspects of a failure’s mechanism and the decision-making process of individuals proximate to the failure, an organization can come out safer than it would normally be if it had simply punished the actors involved as a remediation.

Having a *blameless* post-mortem process means that engineers whose actions have contributed to an accident can give a detailed account of:

- What actions they took at what time
- What effects they observed
- Expectations they had
- Assumptions they had made
- Their understanding of timeline of events as they occurred

It is assured that they can give this detailed account **without fear of punishment or retribution**.

An engineer who thinks they’re going to be reprimanded has no incentive to give the details necessary to get an understanding of the mechanism, pathology, and operation of the failure. This lack of understanding of how the accident occurred all but guarantees that it *will* repeat. If not with the original engineer, with another one in the future.

> "We must strive to understand that accidents don’t happen because people gamble and lose.
Accidents happen because the person believes that:
…what is about to happen is not possible,
…or what is about to happen has no connection to what they are doing,
…or that the possibility of getting the intended outcome is well worth whatever risk there is."
>
>&mdash; <cite>Erik Hollnagel</cite>

## Allowing Engineers to Own Their Own Stories

A funny thing happens when engineers make mistakes and feel safe when giving details about it: they are not only willing to be held accountable, they are also enthusiastic in helping the rest of the company avoid the same error in the future. They are, after all, the most expert in their own error. They ought to be heavily involved in coming up with remediation items.

### What do we do to enable a "Just Culture"?

- Encourage learning by having these blameless postmortems on outages and accidents.
- The goal is to understand *how* an accident could have happened, in order to better equip ourselves from it happening in the future
- Gather details from multiple perspectives on failures, and don’t punish people for making mistakes.
- Instead of punishing engineers, we instead give them the requisite authority to improve safety by allowing them to give detailed accounts of their contributions to failures.
- Enable and encourage people who do make mistakes to be the experts on educating the rest of the organization how not to make them in the future.
- Accept that there is always a discretionary space where humans can decide to make actions or not, and that the judgement of those decisions lie in hindsight.
- Accept that the [Hindsight Bias](https://en.wikipedia.org/wiki/Hindsight_bias?azure-portal=true) will continue to cloud our assessment of past events, and work hard to eliminate it.
- Accept that the [Fundamental Attribution Error](https://en.wikipedia.org/wiki/Fundamental_attribution_error?azure-portal=true) is also difficult to escape, so we focus on the environment and circumstances people are working in when investigating accidents.
- Strive to make sure that the blunt end of the organization understands how work is actually getting done (as opposed to how they imagine it’s getting done, via Gantt charts and procedures) on the sharp end.
- The sharp end is relied upon to inform the organization where the line is between appropriate and inappropriate behavior. This isn’t something that the blunt end can come up with on its own.

Failure happens. In order to understand how failures happen, we first have to understand our reactions to failure.
