Watch this second video from Susan Etlinger as she predicts how governments could approach AI in the coming years.

> [!VIDEO https://www.microsoft.com/videoplayer/embed/RE3Pcge]

In this video, you heard Susan Etlinger explain why it’s important to combine conversations about AI responsibility and AI innovation in order to increase progress. Etlinger discussed the opportunity to address some of society’s biggest challenges by examining bias in AI and by building purposeful business models that are based on trust.

Etlinger recognizes that establishing responsible AI practices to guide innovation is a process that isn't perfect in the beginning, but, through collaboration and iteration, eventually becomes common practice.

## Actions your organization can take

To help you consider how to apply governance and external engagements in your own organization, we developed the following recommendations:

### Establishing an AI governance system

1. **Choose a governance structure** that best fits your organization’s AI maturity, unique characteristics, culture, and business objectives.
1. **Outline the specific role of your governance system** within your organization. Responsibilities could include developing and enforcing policies, providing advice, and mitigating risks associated with AI systems.
1. **Allocate sufficient budget and resources** to give your governance system the power to enforce responsible AI policies.
1. **Adapt your governance system(s)** over time as your objectives and capabilities change.
1. **Train employees** on the policies, standards, and best practices that your governance system establishes.
1. **Engage in public and private partnerships** to advance the responsible use of AI. As the use of AI increases across the private and public sectors, we must continue to foster open dialogue among businesses, governments, NGOs (Non-Governmental Organizations), academic researchers, and all other interested individuals and organizations.
1. **Leverage lessons learned from internal AI deployments** to inform broader social policies or regulations related to AI.

### Tips for using third-party AI systems

* Research the third party’s stance on responsible design before purchasing out-of-the-box AI solutions to ensure they were designed in a manner consistent with your principles, policies, and standards.
* Include your principles in your request for proposal so the solution can be designed with your principles in mind.
* Create guidelines on how to safely operate and monitor the system and train your employees on these guidelines before deploying the system.
* Rigorously test the system to ensure it operates as intended and in a manner consistent with your principles, policies, and standards.

### Tips for building first party AI systems

* **Consider having your governance system review or provide advice before the release of any new AI system**, especially for sensitive use cases.
* **Create processes for employees to analyze an AI system’s** purpose, technical capabilities, reliability, and use case prior to its release.
* **Provide clear guidelines to ensure your ethical principles are reflected in an AI system** if you're developing AI systems in-house. Support your developers by using industry-established guidelines or developing your own, especially for AI systems that raise complex ethical or human rights considerations.
* **Consider integrating internal guidelines into project management processes**, such as a checklist aligned to the phases of a data science project.
* **Leverage tools and resources** to make it easier for developers to spot and design against potentially harmful issues like biases, safety and privacy gaps, and exclusionary practices.

Now, let’s wrap up everything you’ve learned about governing responsible AI with a knowledge check.
