## Abstract

As discussed previously, each organization will have its own guiding principles for responsible AI. But ultimately, those principles need to be part of a larger strategy to be effective.

Businesses and government agencies alike are using their guiding principles as a foundation for a system of internal oversight, or “governance,” that provides guardrails for their AI solutions. Here, we’ll share a few examples of how governments around the world are approaching AI governance.

## Introduction

Governments play a key role in encouraging AI innovation and responsibility throughout society. They are accountable to citizens to ensure that AI systems across industries do not violate anti-discrimination, data protection, and data privacy laws. Governments have the power to create new laws and invest in research regarding trustworthy AI systems. They are uniquely positioned to bring together stakeholders across business and academia, as well as across borders.

But governments are not just a referee or coach when it comes to AI in society – they are also a player. Governments around the world are increasingly using AI themselves, from bots that engage citizens to AI models that detect tax compliance trends. As they do so, they are seeing a need to create internal guidelines to ensure those systems are designed and managed responsibly.

Managing internal AI solutions is especially important for government agencies. First of all, governments collect a much higher concentration of sensitive, personal data than corporations, heightening the importance of data security and prudence with AI applications. Plus, unlike some private enterprises, government regulations and services often have a significant impact on people’s lives, and governments are accountable to citizens rather than shareholders.

The oversight, or “governance,” of AI might include developing and enforcing policies for testing, documenting, and managing AI systems, or training requirements for the employees who design or use AI systems. For governments, one of the most interesting questions is about where the authority to develop and enforce these policies should be placed:

* What kind of structure should the governance system have (e.g., executive body, committee, appointed or elected members, etc.)
  * Should a centralized body set responsible AI policies for all agencies, or should individual agencies have some autonomy to develop their own standards?
* How should AI projects be funded?
* How should the governance system apply to solutions that are purchased from third-party AI vendors?
* What mechanisms should the office have to enforce responsible AI policy?

Here, we’ll share a few examples of how governments around the world are approaching these questions. Governments are some of the largest and most complex organizations on the planet, so it should come as no surprise that there’s no “one size fits all” governance model for engaging with AI responsibly. We recognize that local, regional, and federal agencies will need to tailor their AI governance systems to their priorities and culture, and their systems must be flexible enough to evolve as the AI landscape rapidly changes.

## Canada

Canada has been one of the first nations to implement a clear governance model for responsible AI. We interviewed Cathy Cobey, Global Trusted AI Advisory Leader at EY and member of Canada’s CIO Strategy Council, to learn more. Cathy Cobey explained that Alex Benay, the former Chief Information Officer of Canada and Deputy Minister at the Treasury Board of Canada Secretariat, has conducted extensive work on responsible AI use both within and beyond the public sector. Through the CIO Strategic Council, he assembled thought leaders from across the country to develop voluntary shared standards for automated decision systems.

When it came to internal oversight, the Treasury Board issued a directive requiring agencies and departments to meet ethical standards when designing automated decision-making systems. To receive funding for AI projects, federal agencies must now complete a mandatory impact assessment to determine whether or not their desired use of AI is compliant with national ethical standards. Third-party AI vendors are required to disclose how they would help agencies build their AI system within a trusted and ethical framework in order to qualify as a vendor.

While Benay was a visionary leader in this space, his approach illustrates the benefits of a centralized structure. Through a central office, like the Treasury Board in this case, leaders can set responsible AI policies for the entire government and then rigorously enforce them across all departments. The benefits of this model are that it delivers consistent guidance to all agencies, enables quick decision-making with clear lines of accountability, and allows governments to implement policies at scale and enforce them at every stage of the AI lifecycle.

## The United States

The United States has taken a similar approach to AI governance. We interviewed Monika Wilczak, Managing Director in AI at EY Advisory, to learn more. With over 10 years of experience working in data science and AI, Wilczak has seen the landscape evolve first-hand. Monika Wilczak recounts that in early 2019, the US President signed an executive order tasking the National Institute of Standards and Technology (NIST), part of the Department of Commerce, to create a plan for developing technical standards and tools in support of trustworthy AI systems. While still in progress, NIST is working on standards to ensure AI technologies are accurate, reliable, resilient, objective, secure, explainable, safe, and accountable.

The U.S. strategy differs from Canada in that funding is not directly tied to meeting these standards. Rather, the executive order directs federal agencies to prioritize AI investments in their R&D missions. As Wilczak explains, the goal is to give agencies the freedom to explore how AI can be used to solve their unique challenges.

With innovation happening in every corner of the government, inter-agency collaboration has the potential to accelerate the spread of responsible AI best practices. Coupled with standards like those from the NIST, there is the opportunity to foster consistency and trustworthiness across the government as a whole.

## India

India’s policy-makers intend to use AI to solve the country’s pressing social challenges such as income inequality and food insecurity, so they are cautious about overregulating it. NITI Aayog, a government think tank established with the aim of achieving the Sustainable Development Goals, created a national strategy for AI that emphasizes an “AI for all” approach. Anna Roy, advisor to NITI Aayog, says India prefers to keep an “open mind” when it comes to the ethics of AI and focuses on funding academic and commercial research.<sup>1</sup>

India chose to implement a responsible AI governance model that provides their departments with key guiding principles for engaging with AI responsibly, then empowers them to execute their own projects as they see fit. They believe deeply that India can play an important role in testing AI use cases across healthcare, education, agriculture, and more that will fulfill inclusive AI criteria under the “AI for all” mandate.

Because they see the challenges of AI as common to all countries and large corporations alike, NITI Aayog has called for the development of international standards for ethical AI.

## Partnering with other stakeholders

It’s often helpful to partner with stakeholders outside of the government when creating AI governance systems.

As previously noted by Susan Etlinger, Industry Analyst with Altimeter, A Prophet Company, it’s important to consult with technology experts like data scientists and machine learning engineers who can speak to the capabilities and limitations of AI. Leading think tanks, policy advisors, and non-governmental organizations should also be included—especially those that focus on underserved communities. It’s helpful to loop in researchers from academic institutions who represent a variety of disciplines such as sociology, economics, philosophy, applied ethics, law, humanities, social psychology, and neuroscience. Finally, it can be helpful to work with business leaders who are utilizing AI at scale and organizations such as the [Organization for Economic Co-operation and Development (OECD)](https://www.oecd.org), [Partnership on AI (PAI)](https://www.partnershiponai.org), and [World Economic Forum (WEF)](https://www.weforum.org).

There is one more important voice for governments to consider: their citizens. Many governments, such as Canada and the UK, are making their responsible AI policies, non-private training data, and source code public, thereby enabling citizens to access, review, and provide their perspective on them. Public feedback gives governments the opportunity to tailor their governance model to reflect citizen priorities. For example, some countries may choose to prioritize rapid innovation over personal privacy, while others may place more emphasis on protecting human rights.

Next, let’s learn more about how governments can approach policy discussions regarding responsible AI.

> [!TIP]
> For more on how enterprises and other organizations are governing their use of responsible AI, see this module: [Identify governing practices for responsible AI](https://review.docs.microsoft.com/learn/modules/responsible-ai-governing-practices/index?branch=pr-en-us-5765).
