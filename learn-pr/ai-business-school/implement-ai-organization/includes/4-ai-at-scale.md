A new approach to AI is creating a revolutionary new class of machine learning models.

Traditionally, machine learning models have been trained for a specific task. Individual models can translate between languages, recognize objects in an image or video, extract key points from text, or make predictions based on quantitative data. Most AI solutions are built using one or several of these purpose-built models.

But recently, researchers have been training AI models that can execute multiple tasks.

## Multitasking AI models are the future of AI

:::row:::
:::column span="2":::
These multitasking models are called “large” or “massive” because they have billions of “parameters,” or connections between nodes in the model’s neural network. To illustrate just how powerful they are: models with only millions of parameters achieved human parity in object recognition, speech recognition, and translation. These cutting-edge models will jumpstart a whole new breed of AI applications that weren’t possible before.

According to a 2018 OpenAI analysis,<sup>3</sup> from 2012 to 2018 the amount of compute used in the largest AI training runs grew more than 300,000 times with a 3.5-month doubling time. Just in the year 2020, the size of natural language generation (NLG) models has increased exponentially—from less than 20 billion parameters to 175 billion.

As you can imagine, training these large, multi-tasking models requires expertise, years of work, and massive amounts of training data. It also requires the most advanced supercomputing infrastructure and techniques for training the models across many pieces of hardware.
:::column-end:::
:::column span="2":::
:::image type="complex" source="../media/implement-ai-organization-1.png" alt-text="Illustration of natural language generation (NLG) model growth.":::
The y-axis represents number of parameters. It has three intervals: 25 billion, 100 billion, and 175 billion. The x-axis represents time and has the following years: 2018, 2019, and 2020. The graph shows exponential growth of parameters from 2018 to 2020, with a trend line curving from the bottom left of the graph to the top right. The trend line is near zero in 2018 and starts moving upward slightly in 2019. At the beginning of 2020, it’s just under 25 billion. This is marked with a circle with the text “Jan 2020.” The line then shoots upward to 175 billion by the middle of 2020, which is marked by a circle with the text “May 2020.”
:::image-end:::
:::column-end:::
:::row-end:::

## Microsoft AI at Scale initiative

Through our AI at Scale initiative, we’re providing organizations with access to large multi-tasking AI models and the supercomputing resources needed to create them. Our goal is to dramatically reduce the barriers and costs associated with accessing next-generation AI capabilities.

:::row:::
:::column span="2":::
In collaboration with OpenAI, we’ve built a supercomputer designed specifically for training large AI models.

We’ve also developed our own family of large AI models, called the Microsoft Turing models. These include the world’s largest publicly available language model, the *Microsoft Turing model for natural language generation*, which has 17 billion parameters. This model has a deep understanding of grammar, context, and intended meaning. It had set new benchmarks in many different language-related tasks, such as summarization, contextual prediction, and question answering.
:::column-end:::
:::column span="2":::
:::image type="complex" source="../media/implement-ai-organization-2.png" alt-text="Graphic with the title “Leverage AI at Scale.”":::
Within the graphic there are three different icons: In the top left there is a symbol showing circles connected by lines, with the text “Turing models” underneath it. In the top right, there is a cube icon that looks like a box, with the text “Microsoft products” underneath it. Finally, in the bottom center is an icon showing servers and a computer, with the text “supercomputer” underneath it.
:::image-end:::
:::column-end:::
:::row-end:::

## Three ways to take advantage

By making this breakthrough technology available to all, we hope to empower organizations, developers, and employees to unlock innovation that was previously impossible.

### Create your own multi-tasking AI models with our supercomputer

If your organization is ready to create your own large AI models, you can harness the power of our supercomputer through Azure. With more than 285,000 CPU cores, 10,000 GPUs, and the latest high-bandwidth network, it ranks in the top five supercomputers in the world.

We’re also providing the tools you need to train large AI models on the supercomputer efficiently and cost-effectively. DeepSpeed, an open-source Pytorch library, provides techniques for training large models 10 times faster. The open-source engine ONNX Runtime enables highly efficient model training. We’ll continue to create optimization techniques that push the boundaries of scaling and efficiency and offer them through our Azure Machine Learning platform.

### Innovate on top of some of the most sophisticated models in the world

If you’re interested in using the Microsoft Turing models for your own AI solutions, we’ve made them open source for organizations and developers to access. Our language understanding and language generation models are available today.

You can use the Turing models as they are or customize them for your own scenarios. After a model has been trained for general tasks, it can be fine-tuned for more specific tasks by training with additional data. For example, a company could take a pre-trained language model and teach it more specific vocabulary by training it with industry- and company-specific text. This same process is used to customize smaller models as well, but it’s even more beneficial for large models. We provide open- source guidance for customizing large models in the Azure Machine Learning platform.

With access to this world class technology, your technical teams will be able to fast-track the AI development cycle and deliver business value at an unprecedented pace and quality.

### Bring transformational capabilities to everyone with Turing models built into Microsoft products

Finally, every organization can benefit from Turing models today, because they’re baked into our products. We’ve used the  *Turing model for natural language generation* to improve many tasks across Bing, Office, Dynamics, and other productivity products. For example:

* Microsoft Word users can now get answers to questions about document content. For example, if you’re looking at a biography of Alan Turing in Word, you can ask “where did he get his PhD?”
* Outlook users can reply to emails with the click of a button using AI-generated suggested replies.
* In SharePoint, users can see AI-generated document summaries by hovering over the file name, so they can find what they’re looking for faster.
* The Dynamics 365 Virtual Agent for Customer Service easily identifies and resolves common support issues using the Turing model.

These advances in Microsoft products help everyone be more productive, stay connected, and drive greater impact.

## Imagine what’s possible with AI at Scale and an AI-ready culture

As discussed in the module “Understand the importance of building an AI-ready culture,” taking full advantage of AI requires cultural shifts within an organization. When developing the Microsoft Turing models, we found it essential to have product teams collaborate and share data instead of working independently. Our progress wouldn’t have been possible without shifting our approach from a siloed to a collaborative culture.

With our AI at Scale technology and an AI-ready culture, you can reach new heights in your AI journey. Later in this module, we'll dive deeper into how you can assign AI-related responsibilities in your organization.

Next, let’s hear more about AI at Scale from Luis Vargas, Partner Technical Advisor to the Microsoft CTO.
