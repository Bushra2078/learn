<!-- Original file: C:\Users\Mark\Desktop\CMU\v_5_3\content\_u02_data_centers\_u02_m01_historical_perspective\x-oli-workbook_page\_u02_m01_4_cloud_datacenters.xml -->
##  Challenges and Requirements for Cloud Data Center Design
With the advent of cloud computing, it becomes critical for data center designers to address the cloud's current and evolving needs. In this paradigm, data centers physically host all the cloud services that are delivered to users. In turn, the cloud services are abstracted from the underlying physical resources on varying scales (over a private IP network [i.e., a private cloud] or over the Internet [i.e., a public cloud]), on demand, and for potentially millions of subscribers. Data center design requirements vary according to use, size, and desired functionalities. The cloud model redefines the way data center assets are designed and consumed. Cloud-based services and scale impose new requirements on data centers whereby traffic flows vary, I/O bandwidth and performance demands are significantly increased, and new security concerns are induced. We describe some of the challenges that cloud computing puts on data centers and identify associated requirements for designing cloud-centric data centers.

##  Scalability
With cloud computing, there is an ever-growing need for expansion and high capacity. For that sake, cloud data centers are typically designed around virtual machines (or instances), which are the units of computing in the cloud paradigm. In contrast to enterprise data centers, cloud data centers offer services to potentially millions of users. To address increasing user demands for services on the cloud, virtualization techniques are usually adopted. With virtualization, data center operators can automatically provision and deprovision virtual machines (VMs) as required, without adding or reconfiguring physical devices. Today. it is not uncommon to provision 20 or more VMs per rack-mount or blade server. Clearly, this load can greatly stress server's resources (e.g., CPU, RAM, and network cards). In addition, this can dramatically increase the number of logical servers that operate over the physical data center network. For instance, with a rack of 64 servers and 20 VMs per a server, a cloud provider would require as many as 1200 IP subnets and VLANs (in networking, a LAN can be segmented into different broadcast domains, each referred to as a VLAN). Furthermore, with only 10 racks, 12,000 IP subnets and VLANs will be needed. This demand creates a major problem because it exceeds the limit (i.e., 4094) of usable VLANs specified by IEEE 802.1Q standard, let alone straining physical switches/routers. Cloud data center providers need to find solutions for such problems.

On the other hand, even with maximal use of virtualization techniques, at a point in time it will be necessary to add physical capacity to support growth. As such, cloud data centers need to be based on modular designs in order to support the easy addition of physical capacity without disrupting applications and services. Designers should specify chassis capacity to support long-term growth so that data center operators can include additional components to the chassis as necessary.

##  Network Topologies
![Figure 2.5: Traditional hierarchical, tree-style data center network topology.](..\media\tree_style_data_center_network_topology.png)
_Figure 2.5: Traditional hierarchical, tree-style data center network topology._

Most of today's data center networks are based on hierarchical, tree-style designs consisting of three main tiers: an access tier, an aggregation tier, and a core tier. Figure 2.5 shows a sample of a traditional tree-style network topology. First, the access tier is made up of cost-effective Ethernet switches connecting rack servers and IP-based storage devices (typically 10/100Mbps or 1GbE connections). Second, multiple access switches are connected via Ethernet (typically 1/10GbE connections) to a single aggregation switch. Third, a set of aggregation switches are connected to a layer of core switches. Because layer 2 VLANs do not involve IP routing, they are typically implemented across access and aggregation tiers. Conversely, layer 3 routing is implemented at core switches that forward traffic between aggregation switches, to an intranet, and to the Internet. A salient point is that the bandwidth between two servers is dependent on their relative locations in the network topology. For instance, nodes that are on the same rack have higher bandwidth between them as opposed to nodes that are off rack.

Indeed, the network is a key component in cloud data centers. Hierarchical topologies as depicted in Figure 2.5 do not truly suit clouds because they enforce inter-server traffic to traverse multiple switch layers, each adding to latency. Latency in this context refers to the delays incurred by the number of switches traversed, required processing and buffering. Minimal delays in clouds can result in poor user performance perception and loss of productivity. Hence, flatter network topologies with fewer layers to accommodate delay- and volume-intensive traffic are typically required for cloud data centers.

##  Greater Utilization and Resiliency
Usually, contemporary tree-style data center networks rely on some variant of the Spanning Tree Protocol (STP) for resiliency. STP is a data link management protocol that ensures a loop-free topology when switches/bridges are interconnected via multiple paths. STP allows only one active path across two switches, with the rest being set inactive (assuming many paths are available). On an active path failure, STP automatically selects another available inactive path instead of the failed one. This selection might take STP several seconds, which could turn unsuitable for delay-sensitive cloud applications (e.g., Web conferencing). Furthermore, setting idle backup paths is not the best choice for cloud data centers, especially with the exponential increase in user demands. Cloud data centers require more streamlined and resilient network designs that make full use of network resources and recover from failures in milliseconds to meet demands speedily and utilize resources efficiently.

##  Secure Multitenant Environment
![Figure 2.6: Workload-to-workload communications in a virtualized environment.](..\media\communications_virtualized_environment.png)
_Figure 2.6: Workload-to-workload communications in a virtualized environment._

In modern data centers, workloads (e.g., databases, user applications, Web hosting) are typically deployed on distinct physical servers, with workload-to-workload communications occurring over physical connections. Accordingly, securing users can be achieved by conventional network-based intrusion detection/prevention systems. On the other hand, in cloud data centers, multiple VMs can be provisioned on a single rack server, with each belonging to a different user. Thus, workload-to-workload communications can occur within the same server over virtual connections in a manner completely transparent to existing security systems (see Fig. 2.6). Therefore, cloud data centers need to isolate users, protect virtual resources, and secure intra-server communications.

##  Virtual Machine Mobility
Cloud data centers can host user VMs at servers in one rack, across racks but in the same data center, or at servers across data centers. A cloud can encompass multiple data centers (e.g., Amazon allows users to provision virtualized instances across many data centers). For executing routine maintenance, balancing loads, and tolerating faults, clouds need to periodically and seamlessly migrate VMs between physical servers without impacting user services and applications. This migration does not only require expanding the layer three domain (the domain in which IP routing is required [e.g., WAN]) to move VMs across data centers, but it also requires extending the layer two domain (the domain in which no routing occurs and only broadcasting is employed [e.g., LAN]) in order to span multiple data centers.

##  Fast and Highly Available Storage Transport
Storage in a cloud data center must support VM mobility and be continually available. VMs that are migrated need to maintain communication with their storage systems. One way to get around this is to move VMs with pertaining storage/data. Clearly, this would require highly available, low-latency, and bandwidth-intensive cloud data center connectivity. Multiple storage models (e.g., Storage over IP [SoIP], Fiber Channel over Ethernet (FCoE), traditional Fiber Channel) are in use today and would further require high-performance and highly available cloud networks. We discuss cloud storage, its challenges, and protocols in the unit on cloud storage.

In summary, data centers tailored for clouds would require the following:


- Modular designs to support exponential growth and the easy addition of physical capacity without any service disruption.
- Flatter network topologies with fewer layers and less equipment and cabling to accommodate delay- and volume-intensive traffic.
- More efficient and resilient network designs that make full use of network resources and recover from failures in milliseconds.
- Capability to fully isolate cloud users, protect virtual resources, and secure intra-server communications.
- VM mobility to execute routine maintenance, achieve load balancing, and tolerate faults seamlessly and speedily.
- Twenty-four seven, 365 days a year service availability and ability to keep migratory VMs connected to their storage systems.
##  Addressing Requirements for Cloud Data Centers
Data center designers can address the previously discussed requirements at the infrastructure layer, the virtualization layer, or both. For instance, the scalability requirement needs to be addressed at both layers, whereas secure multitenancy can be mainly addressed at the virtualization layer. We study virtualization in detail in the unit "Resource Sharing and Virtualization." In this unit, we are concerned with the infrastructure layer. Accordingly, we present only some of the IT devices (e.g., switches, routers), platforms, and protocols that can contribute to satisfying the requirements for cloud data centers. Because this is a cloud computing course, we do not discuss how the devices, protocols, and platforms work but focus on the benefits they bring to cloud data centers.

To start, data center planners can consider Multiprotocol Label Switching (MPLS) to address cloud infrastructure requirements. MPLS is a highly scalable mechanism that directs data from one server to another based on short path labels rather than long network addresses. Specifically, MPLS labels data packets and enables packet forwarding without examining the contents of packets, which makes packet forwarding quite fast because it avoids routing table lookups and solely depends on packet labels. Consequently, MPLS allows creating end-to-end circuits across any type of transport medium, a key requirement for server-to-server communication across cloud data centers.

As previously discussed, in cloud computing, different types of traffic are imposed, and variant bandwidth requirements are induced. Without considering MPLS, separate layer two networks might be necessary to build, which clearly is not a scalable solution in cloud data centers and can greatly increase both capital and operational expenses. By contrast, with MPLS, networks can be shared via creating virtual network connections called label switched paths (LSPs). Furthermore, quality of traffic flows over the LSPs can be flexibly controlled. Such traffic control facilitates end-to-end quality of service (QoS) and provides fast network convergence (approximately 50ms) in case of link failures, a remarkable improvement over STP. As a result, the transparency of network failures can be highly improved, and service disruptions can be reduced, which are other key requirements for greater resiliency on cloud data centers.

MPLS also allows enabling virtual private LAN service (VPLS) to extend layer two connectivity across multiple data centers. VPLS is a virtual private network (VPN) technology. VPN is typically utilized to implement secure connections between LANs located at different sites (i.e., data centers) using public Internet links. VPLS allows different LANs at different data centers to communicate as if they are one LAN, a key requirement for streamlining VM mobility. VM mobility can also benefit from the aforementioned traffic controlling capability offered by MPLS.

##  Summary
Several vendors are now marketing cloud-centric networking and compute products to address several of the design goals stated earlier. Most of these products are variations on what you would find in a traditional data center, with a shifted focus on density and concurrent users. A data center designer who wants to support private or public clouds has a growing catalog of products from which to choose.

Data center design has evolved rapidly over the last 5 years; this trend is not slowing down. The data centers 5 or 10 years from now will most likely look very different than the data centers of today. Data center designers are addressing new requirements while improving efficiency and the TCO to deliver a cloud service. With the advent of pre-engineered modular data centers, soon the facilities will become interchangeable components, much like the IT equipment itself. To meet the growing consumer demand for new cloud-based services, as well as address IT infrastructure needs of existing enterprises, we will see more and more data centers, with each generation more efficient than its predecessor. 