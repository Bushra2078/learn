<!-- Original file: C:\Users\Mark\Desktop\CMU-source\v_5_3\content\_u05_distributed_programming_analytics_engines\_u05_m02_mapReduce\x-oli-workbook_page\_u05_m02_2_programming_model.xml -->
##  Hadoop MapReduce Programming Model

Hadoop presents MapReduce as an analytics engine and, "under the hood," employs the Hadoop Distributed File System (HDFS). HDFS mimics the Google File System (GFS) and partitions input datasets into fixed-size chunks ( _blocks_), distributing them on participating cluster nodes. By default 64MB, each HDFS block can be configured differently by users. Jobs can subsequently process HDFS blocks in parallel at distributed machines, thus exploiting the parallelism enabled by partitioning datasets. MapReduce breaks jobs into multiple tasks denoted as map and reduce tasks. All map tasks are encapsulated in what is known as the map phase, and reduce tasks are encompassed in what is called the reduce phase. The map phase can have one or many map tasks, and the reduce phase can have zero or many reduce tasks. When a MapReduce job includes no reduce tasks, it is referred to as "reduce-less."
![ Figure 5.15: A full, simplified view of the phases, stages, tasks, data input, data output, and data flow in the MapReduce analytics engine ](../media/mapReduce_analytics_engine.png)

_Figure 5.15: A full, simplified view of the phases, stages, tasks, data input, data output, and data flow in the MapReduce analytics engine_


Figure 5.15 demonstrates a full, although simplified, view of the MapReduce analytics engine. Map tasks operate on distributed HDFS blocks, and reduce tasks operate on map tasks' output, denoted as intermediate output, or partitions. Each map task processes one or many distinct HDFS blocks (more on this shortly), and each reduce task processes one or many partitions. In a typical MapReduce program, the map task that is run across all the input HDFS blocks is the same and the reduce task run across all the partitions is also the same. Therefore, within a specific Map or Reduce phase, MapReduce jobs can considered to be in the single program, multiple data (SPMD) category (see the section ). 

Map and reduce tasks consume different data, operating independently and in parallel only in their respective phases. That is, tasks in the same phase never communicate (send or receive messages), and the only communication in MapReduce occurs explicitly (through the help of the MapReduce framework) between different tasks in different phases. Specifically, map tasks generate new partitions in the map phase, and the Hadoop engine itself transfers partitions (over the network) to the reduce tasks in the reduce phase in a process called _shuffling_. The rationale behind such a strategy is that Hadoop would not scale to large clusters (hundreds or thousands of nodes) if tasks were allowed to communicate arbitrarily. Instead, all communication occurs only between the map and the reduce phases and under the full control of the engine itself (not the tasks). Therefore a typical program running in the MapReduce variant can be considered to be a special case of the of the Message Passing model, as tasks do not have access to a common shared memory, but rely on messages being passed by the framework between the Map and Reduce synchronisation barrier. 

### References

1. _Hadoop ().  http://hadoop.apache.org/docs/r1.0.4/hdfs_design.html_
2. _S. Ghemawat, H. Gobioff, and S. T. Leung (Oct. 2003). The Google File System SOSP_
3. _S. Chen and S. W. Schlosser (May 2008). Map-Reduce Meets Wider Varieties of Applications Intel Research Pittsburgh, Tech. Rep. IRP-TR-08-05_