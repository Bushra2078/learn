<!-- Original file: C:\Users\Mark\Desktop\CMU-source\v_5_3\content\_u05_distributed_programming_analytics_engines\_u05_m02_mapReduce\x-oli-workbook_page\_u05_m02_5_computation_architectural_model.xml -->
##  The Computation Model

MapReduce jobs, like all distributed programs, can embody either a synchronous or asynchronous computation model (see the section _Synchronous and Asynchronous Distributed Programs_). During each phase and stage, MapReduce tasks execute numerous computations that depend on results from the previous phase or stage and can proceed only after those data arrive. For example, a reduce task cannot begin before all required partitions arrive from the shuffle and the merge and sort stages. Furthermore, in a phase, tasks do not communicate with each other, and interactions occur only at the end of a stage or phase. Any synchronous system must guarantee this interaction property, and MapReduce presents a good example of that computation model. 
![Figure 5.18: A simplified example of the master-slave, tree-style architecture employed by Hadoop MapReduce. The master is denoted as JobTracker and each slave is called TaskTracker. As shown, the JobTracker and the TaskTrackers communicate over the network using a heartbeat mechanism.](../media/master-slave.png)

_Figure 5.18: A simplified example of the master-slave, tree-style architecture employed by Hadoop MapReduce. The master is denoted as JobTracker and each slave is called TaskTracker. As shown, the JobTracker and the TaskTrackers communicate over the network using a heartbeat mechanism._

##  The Architectural Model

As Figure 5.18 shows, MapReduce uses a master-slave architecture. The master node is called _JobTracker_ (JT), and each slave is called _TaskTracker_ (TT). The JT and TTs communicate over the cluster network via a periodic heartbeat mechanism. By default, TTs send messages (heartbeats) to the JT every three seconds, and the JT replies<sup>6</sup> with a new map or reduce task or with a different message. The JT uses these heartbeats to detect task failures (see the section _Fault Tolerance_). Each TT has, by default, two map slots and two reduce slots at which corresponding tasks can execute. This slot allocation determines the maximum number of map and reduce tasks (degree of task parallelism) that can run simultaneously in the TT. 

Hadoop assumes a hierarchical, tree-style network topology with rack and core switches, as shown in Figure 5.18. TTs are spread over different racks and may reside in one or several data centers. Between any two TTs, communication bandwidth depends on their relative locations in the network topology. For instance, TTs on the same rack can interact with each other much faster than with off-rack counterparts. Measuring bandwidth between any two TTs is difficult in practice, so Hadoop employs a simple, distance-based approach, representing TT network positions as strings (e.g., `TaskTracker5`'s location in Figure 5.18 is `/CoreSwtich/RackSwitch1/TaskTracker5`). Hadoop assumes a unit distance between any TT and its parent switch, so the total distance between any two TTs can be calculated by simply adding up the distances to their closest common ancestor. In our example, `Total-Distance(/CoreSwitch/RackSwitch2/TaskTracker1, /CoreSwitch/RackSwitch2/JobTracker) = 4`. 

<sup>6</sup>The JT does not reply to every heartbeat sent by a TT. TTs can send heartbeats just to indicate that they are still alive. If a TT includes in its heartbeat a request (e.g., a request for a map or a reduce task), the JT replies with a heartbeat that satisfies the TT's request (e.g., a map or a reduce task). 

### References

1. _T. White (2011). Hadoop: The Definitive Guide 2nd Edition O'Reilly_
2. _D. P. Bertsekas and J. N. Tsitsiklis (January 1, 1997). Parallel and Distributed Computation: Numerical Methods Athena Scientific, First Edition_