<!-- Original file: C:\Users\Mark\Desktop\CMU-source\v_5_3\content\_u03_virtualizing_resources_for_cloud\_u03_m07_software_defined_datacenter\x-oli-workbook_page\_u03_m07_4_sds.xml -->

As you have already seen, different types of storage systems can be deployed based on the needs of an application. The most basic form of storage is direct-attached storage, wherein storage devices such as Magnetic disks or SSDs are directly attached to a server internally (through an internal bus connection, typically SATA or SAS). On the other hand, massive, enterprise grade storage systems can be used in the form of a Storage Area Network (SAN), which use a network (using different technologies) to connect servers to a pool of disks. 

Recall that in clouds, physical compute resources are shared among multiple tenants through virtualized servers. This approach could also apply to storage resources as well. Storage resources can be shared among multiple tenants, allowing for the lowering of cost and improving overall utilization of these resources. However, a form of isolation should be provided along with fault tolerance and other technologies to meet a specific application's SLOs. This enables clients to have predictable behavior of their applications, while sharing storage resources with other tenants. Providing end-to-end SLOs would translate into requirements on bandwidth, Input/Output Operations Per Second (IOPS), priority or control over the entire IO path (Figure 3.38). 
![Figure 3.38 : An example IO Path](../media/IO_Path.png)

_Figure 3.38 : An example IO Path_


Software Defined Storage (SDS) allows clients to specify fine-grained capacity, latency and/or bandwidth requirements, in the form of SLOs, which are mapped to abstract storage services for the client. The cloud provider can assemble and provide a scalable storage service using various storage technologies in the back-end. Client SLOs can be met by managing the storage stack at the provider's side.

> [!VIDEO https://youtube.com/embed/-JxZM7Y_8sU]

_Video 3.15: Software Defined Storage_


To better understand how this can be achieved, let's look at one of the emerging technologies that enable SDS, IOFlow . 
##  An Example of SDS: IOFlow

In data centers, the operation of an application requesting file operations and the storage system fulfilling the operation is complex involving several stages such as the host OS, the hypervisor, the network fabric, the storage server and finally a disk operation. The request also appears differently at each layer. For example, a file IO request like a read, write, or create in a VM results in a block IO request in the hypervisor. This, in turn, results in Ethernet packets across the network, and finally another file IO request and block device request at the storage server. 

The number of layers in the system means that enforcing an end-to-end flow policy (these policies are typically derived from an SLO) is hard. It requires layers along the IO path to treat requests differently based on their content and/or client SLOs. Further, policies may need to be enforced at one or more or all layers along the path. For example, prioritizing IOs from a specific VM to storage requires configuring the prioritization on all layers along the path. 

IOÂ­Flow is a software defined storage architecture, designed by Microsoft, to maintain end-to-end SLOs, bandwidth SLOs and also improve performance. The clients are VMs running on compute servers through a hypervisor. These VMs need access to the storage server which is regulated by IO-Flow. Data flow is achieved by the use of queues at each stage in the IO path. These queues are regulated by a centralized controller. The centralized controller in the IOFlow architecture assigns data rate metrics and the route at each stage in the overall architecture. 

An example of a typical VM to storage server IO path is presented in Figure 3.39 below. A compute server, which consists of a number VMs running simultaneously interacts with a storage server. Thus, the application interaction with a filesystem within a VM becomes block device requests at the virtual hard disk (VHD). In this specific example, the virtual hard disk is actually a remote storage server serving files using Microsoft's SMB protocol. Therefore, IO requests further get translated into a network packet at the hypervisor's network driver and sent across to the remote storage server through the network. In the storage server, the network packet is unwrapped at each layer, transforming into an SMB request, which in turn translates to a file system request at the storage server. At each layer in this IO path, IOFlow adds a queuing abstraction, which is then exposed to the IOFlow controller. The controller can translate these policies into lower level queuing rules at each stage. 
![Figure 3.39 : IOFlow Example (Figure from )](../media/IO_Flow.jpg)

_Figure 3.39 : IOFlow Example (Figure from )_


The IOFlow controller regulates IO traffic in the following manner: 

1. The IOFlow controller obtains a graph of the entire network under it, including where the compatible stages are located. 
1. Policies (explained below) supported by the controller are chosen by the client. 
1. Based on the client's policy, queues are formed and configured at intermediate stages. 
1. When IO traffic passes through these intermediate stages, the IO header is recognized and the respective policy is implemented. 

IOFlow implements **flow identifiers**, which are basically IO headers attached to individual IO requests, these low level headers are attached to the IO as the high level headers and cannot be recognized by the intermediate stages. Once the intermediate stages identify a header they use it as routing information, to route the IO to the next hop. 

As an example of how SDS can use policies to regulate IO traffic, IOFlow currently presents the following five flow policies: 

|Flow|Description|
|--|--|
|P1|1 VM ( **VM1**), which is guaranteed a fixed bandwidth **B**.|
|P2| When extra bandwidth is available the **VM1** is allowed to exceed **B**. |
|P3| Sanitize policy, the IO traffic is routed through a sanitization layer (eg: Malware detection). |
|P4| High priority, the data traffic of the **VM2** has high priority and must be given high priority end-to-end. |
|P5| Multiple VMs that belong to the same client that access a common pool of resources can be guaranteed a bandwidth **B**|


For P1, every VM is promised a minimum bandwidth to access one or more storage servers. However, in P2,if there is extra bandwidth available, as a result of one of the peer VMs having gone idle, the bandwidth available to VM1 can change dynamically. P3 requires traffic to be routed to a layer for sanitization. As an example, a new untrusted VM can have its traffic routed to a malware detection system. P4 can ensure low latency operations by assigning high priority to VM2 storage traffic. P5 allows a bandwidth guarantee for a set of VMs that need to access specific storage servers. The policies P1 through P4 are specified for point-to-point flows, while P5 policies are specified for multiple flows. 

SDS is a fast evolving approach to providing storage as a service which meets a client's SLOs through shared storage systems. As mentioned above, sharing without end-to-end SLO guarantees will not be readily adopted. IOFlow is an early example of an SDS technology. We expect the innovation in the SDS space to continue at a rapid pace bringing with it new solutions that will attempt to meet new cloud application requirements.

### References

1. _Thereska et al. (2013). IOFlow: A Software-Defined Storage Architecture SOSP'13: The 24th ACM Symposium on Operating Systems Principles_