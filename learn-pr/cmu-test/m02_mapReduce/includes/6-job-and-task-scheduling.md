<!-- Original file: C:\Users\Mark\Desktop\CMU-source\v_5_3\content\_u05_distributed_programming_analytics_engines\_u05_m02_mapReduce\x-oli-workbook_page\_u05_m02_6_job_task_scheduling.xml -->
##  Scheduling in MapReduce
MapReduce schedules work at both the job and task levels. Clients submit jobs, and the JobTracker (JT) partitions them into map and reduce tasks. Job scheduling (see the section _Scheduling_) determines which job should go next, and task scheduling orders tasks in a job. In Hadoop MapReduce, the JT schedules both jobs and tasks, although job schedulers are pluggable, that is, not part of the JT code. Task schedulers, on the other hand, are integrated in the JT code. The introduction of the pluggable job schedulers to the Hadoop framework is considered an evolution in cluster computing. Pluggable schedulers enable tailoring Hadoop for specific workloads and applications, which capability creates the opportunity for job schedulers optimized for the ever-increasing list of MapReduce applications. Furthermore, pluggable schedulers increase code readability and facilitate the experimentation and testing essential to research. 

Hadoop MapReduce uses several alternate job schedulers, including a default first-in, first-out (FIFO) design and the Fair and Capacity schedulers. FIFO suggests pulling jobs from a work queue in receipt order, oldest first, and launching them one after the other. Although such a strategy appears simple and easy to use, it suffers several drawbacks: 


- FIFO scheduling does not support job preemption. Running jobs cannot be interrupted to allow waiting jobs to proceed and meet their performance objectives, such as avoiding job starvations and/or sharing resources effectively. As a result, simultaneous sharing of cluster resources is infeasible. As long as one job absorbs all cluster's resources (slots), no other can proceed. So the next queued job must wait until task slots become free, and the current job has no more tasks to execute. This limitation can easily lead to a fairness conflict, in which a long-running job blocks the whole cluster, starving all small jobs.
- The FIFO scheduler does not consider job priority or size, and a job's submission time alone completely determines its importance. Thus, jobs may wait in queue for extended periods, no matter how critical or time sensitive they are, and this limitation poses additional fairness and responsiveness issues.
To address the FIFO scheduler's shortcomings, Facebook developed a more sophisticated scheduler, called the Fair Scheduler, which is now part of the Apache Hadoop distribution. The Fair Scheduler represents cluster resources in terms of map and reduce slots and suggests a way to share clusters such that all jobs get, on average, an equal share of slots over time. The scheduler assumes a set of pools into which jobs are placed. Each pool is assigned a set of shares reflecting the map and reduce slots that its constituent jobs can occupy. The greater the number of shares a pool is assigned, the greater the number of map and reduce slots its jobs can use. Jobs in a pool can be scheduled using the FIFO scheduler or the Fair Scheduler itself. Jobs across pools are always scheduled using the Fair Scheduler. 

When only a single job is submitted, the Fair Scheduler grants it all the cluster's available map and reduce slots. When new jobs are submitted, the scheduler assigns to them slots that become free. Assuming a uniform distribution of slots, each job thus gets roughly the same amount of CPU time. This strategy can obviously allow several jobs to run simultaneously on the same Hadoop cluster, "sharing in space" the cluster slots. This phrase implies that each job has exclusive access to a specific number of slots on the Hadoop cluster, an arrangement similar to sharing memory in operating systems (OSs), whereby the system allocates to each process an independent portion of the main memory. The result is that multiple processes can coexist(see the section _Resource Sharing in Space and in Time_ for more details about sharing in space). Sharing in space, as offered by the fair scheduler, allows short jobs to finish in reasonable times while not starving long jobs. Jobs that require less time are able to run and finish while the jobs that require more time continue running. To minimize congestion due to sharing and finish work in a timely manner, the Fair Scheduler permits constraining the number of jobs that can be active at one time. 

The Fair Scheduler can also accommodate job priorities by assigning to jobs weights that affect the fraction of total CPU time each can obtain. In addition, the scheduler can guarantee minimum shares to pools, thus ensuring that all jobs in a pool get sufficient map and reduce slots. Although a pool contains jobs, it gets at least its minimum share of resources, and when it becomes empty (no further jobs to schedule), the Fair Scheduler distributes its assigned slots uniformly across active pools. If a pool does not use all its guaranteed share, Fair can also spread excess map and reduce slots equally across other pools. 

To meet every pool's guaranteed minimum share, the Fair Scheduler optionally supports preempting jobs in other pools. This procedure entails preempting some or all of the foreign map and reduce tasks in a rather brutal manner. Because Hadoop MapReduce does not yet support suspending running tasks (see the section on task elasticity in _Scheduling_ ), the fair scheduler simply kills tasks in other pools that exceed their guaranteed minimum shares. Hadoop can tolerate losing tasks (see the section _Fault Tolerance_), so this strategy does not cause the preempted jobs to fail, but it can affect efficiency because killed tasks must be reexecuted, thus wasting work. To minimize such redundant computation, the Fair Scheduler picks the most recently launched tasks from overallocated jobs as kill-candidates. 

A third design, developed by Yahoo!, the Capacity Scheduler, shares some principles with the Fair Scheduler. As with Fair, the Capacity scheduler shares resources (slots) in space. However, it creates several queues, instead of pools, each with a configurable number (capacity) of map and reduce slots, and each queue can hold multiple jobs. All jobs in a queue can access the queue's allocated capacity. In a queue, scheduling occurs on a priority basis, with specific, configurable soft and hard limits, and the scheduler further adjusts priorities based on job submission times. When a slot becomes free, Capacity assigns it to the least-loaded queue and there chooses the oldest submitted job. Excess capacities among queues (unused slots) are temporarily assigned to other needy queues, even if the latter exceed their initially allocated capacities. If the original queue later experiences a demand for these reassigned slots, Capacity allows any tasks then running there to complete. Only when such tasks finish does the scheduler return the underlying slots back to their original queues/jobs (i.e., tasks are not killed). Although their reassigned slots perform "detached duty," the originating queues are delayed, but avoiding job preemption simplifies Capacity's design and eliminates wasted computation. Finally, like the Fair Scheduler, the Capacity Scheduler provides a minimum capacity guarantee to each queue. Specifically, each queue is assigned a guaranteed capacity so that the total cluster capacity is the sum of all queue capacities (no overcommitted capacity). 

Scheduling a job under either FIFO, Fair, or Capacity implies scheduling all its constituent tasks. For the latter procedure, Hadoop MapReduce uses a pull strategy (see _Symmetrical and Asymmetrical Architecture Models_). That is, after scheduling a job, J, the JobTracker does not immediately push J’s Map and Reduce tasks to TaskTrackers, but rather waits for TTs to make appropriate requests via the heartbeat mechanism (see _The Computation and Architectural Models_). On receiving requests for map tasks, JT follows a basic scheduling principle that says, "moving computation toward data is cheaper than moving data toward computation." As a consequence, seeking to reduce network traffic, JT attempts to schedule map tasks in the vicinity of relevant HDFS input blocks. This goal is easy to accomplish because a map task's input is typically hosted at a single TT. 

When scheduling reduce tasks, however, JT ignores that principle, mainly because a reduce task's input (partition(s)) usually comprises the output of many map tasks generated at multiple TTs. When a TT asks, JT assigns a reduce task, _R_, irrespective of TT's network distance locality from _R_'s feeding TTs.7 This strategy makes Hadoop's reduce task scheduler locality unaware. 

![Figure 5.19: The nodes at which native Hadoop scheduled each map task and reduce task of the WordCount benchmark]("..\media\nodes.png")
_Figure 5.19: The nodes at which native Hadoop scheduled each map task and reduce task of the WordCount benchmark_

To illustrate this locality unawareness and its implications, we deﬁne a total network distance of a reduce task, _R_, (TNDR), as sumi=0nNDiR, where _n_ is the number of partitions that are fed to _R_ from _n_ nodes, and _ND_ is the network distance required to shuffle a partition _i_ to _R_. Clearly, as TNDR increases, more time is taken to shufﬂe _R_'s partitions, and additional network bandwidth is dissipated. Figure 5.19 lists the nodes at which each map task, _Mi_, and reduce task, _Ri_, of the WordCount benchmark were scheduled by native Hadoop. In this case, every map task is feeding every reduce task, and every map task is scheduled at a distinct node. Nodes 1 through 7 are housed in one rack and the rest in another. Hadoop schedules reduce tasks _R0_, _R1_, and _R2_ at nodes 13, 12, and 3, respectively. This results in _TNDR0_ = 30, _TNDR1_ = 32, and _TNDR_ = 34. If, however, _R1_ and _R2_ are scheduled at nodes 11 and 8, respectively, this would result in _TNDR1_ = 30 and _TNDR2_ = 30. Hadoop, in its present design, cannot make such controlled scheduling decisions. 

Hadoop's current reduce task scheduler is not only locality unaware but also partitioning-skew unaware (see the section _The Data Structure and Flow_). Partitioning skew refers to a signiﬁcant variance in intermediate key frequencies and their distribution across different data nodes. Figure 5.20 demonstrates the partitioning skew phenomenon, showing partition sizes that each feeding map task delivers to each reduce task in two variants of the Sort benchmark, Sort1 and Sort2 (each with a different dataset), in WordCount and in k-Means.8 Partitioning skew causes shufﬂe skew, in which some reduce tasks receive more data than others. The shufﬂe skew problem can degrade performance because a job can get delayed while a reduce task fetches large input data, but the node at which a reduce task is scheduled can mitigate shufﬂe skew effects. In general, the reduce task scheduler's impact can extend to determining the network communication pattern, affecting the quantity of shufﬂed data, and influencing MapReduce job runtimes. 

![Figure 5.20: The sizes of partitions produced by each feeding map task to each reduce task in Sort1, Sort2, WordCount, and k-Means]("..\media\partitioning.png")
_Figure 5.20: The sizes of partitions produced by each feeding map task to each reduce task in Sort1, Sort2, WordCount, and k-Means_

To make Hadoop MapReduce's reduce task scheduler more effective, it should address data locality and partitioning skew jointly. As a specific example, Figure 5.21 demonstrates a Hadoop cluster with two racks, each including three nodes. We assume a reduce task, _R_, with two feeding nodes, TT1 and TT2. The goal is to schedule _R_ at a requesting TT, assuming TTs 1, 2, and, 4 poll JT for a reduce task. With the native Hadoop scheduler, JT can assign _R_ to any of the requesting TTs. If _R_ is assigned to TT4, _TNDR_ will evaluate to 8. On the other hand, if _R_ is assigned to TT1 or TT2, _TNDR_ will be 2. As discussed earlier, a smaller TND should produce less network trafﬁc and, accordingly, provide better performance. 

Numerous research papers have addressed the need for a task scheduler aware of both data locality and partitioning skew. The center-of-gravity reduce scheduler (CoGRS), for example, represents a locality- and skew-aware reduce task scheduler. To minimize network traffic, it attempts to schedule every reduce task, _R_, at its center-of-gravity node, determined by the network locations of _R_'s feeding nodes and the skew in _R_'s partition sizes. Specifically, CoGRS introduces a new metric called weighted total network distance ( _WTND_) and deﬁnes it for each _R_ as _WTNDR_ = sumi=0nNDiRtimeswi , where _n_ is the number of partitions needed by _R_, _ND_ is the network distance required to shufﬂe a partition, _i_, to _R_, and _wi_ is the weight of a partition, _i_. In principle, the center of gravity of _R_ is always one of _R_'s feeding nodes because it is less expensive to access data locally than to shufﬂe them over the network. Therefore, CoGRS designates the center of gravity of _R_ to be the feeding node of _R_ that provides the minimum _WTND_. 

![Figure 5.21: Options for scheduling a reduce task, R, with feeding nodes TT1 and TT2 in a cluster with two racks (CS = core switch, RS = rack switch, TT = TaskTracker, and JT = JobTracker)]("..\media\options_scheduling_Reduce_task .png")
_Figure 5.21: Options for scheduling a reduce task, R, with feeding nodes TT1 and TT2 in a cluster with two racks (CS = core switch, RS = rack switch, TT = TaskTracker, and JT = JobTracker)_

7A feeding TT of a reduce task, R, is a TT that hosts at least one of R's feeding map tasks.

8This is the Apache Mahout k-Means clustering program. K-means is a well-known clustering algorithm for knowledge discovery and data mining.