<!-- Original file: C:\Users\Mark\Desktop\CMU-source\v_5_3\content\_u05_distributed_programming_analytics_engines\_u05_m01_intro\x-oli-workbook_page\_u05_m01_5_challenges.xml -->
##  Challenges in Building Cloud Programs

Designing and implementing a distributed program involves, as we have seen, choosing a programming model and addressing issues of synchrony, parallelism, and architecture. Beyond these matters, when developing cloud programs, a designer must also pay careful attention to several other challenges critical to cloud environments. We next discuss challenges associated with scalability, communication, heterogeneity, synchronization, fault tolerance, and scheduling.
##  Scalability

A distributed program is considered to be scalable if it remains effective when the quantities of users, data, and resources increase significantly. To get a sense of the problem scope, consider the many popular applications and platforms currently offered to millions of users as Internet-based services. Along the data dimension, in this time of big data and the "era of tera" (Intel's phrase) distributed programs typically cope with Web-scale data on the order of hundreds or thousands of gigabytes, terabytes, or petabytes. Google, for example, processes 20PBs of data per day. Globally, in 2010, data sources generated approximately 1.2ZB (1.2 million petabytes), and the 2020 predictions expect an increase of nearly 44 times that amount. Internet services, such as e-commerce and social networks, handle data volumes generated by millions of users daily. Regarding resources, cloud data centers already host tens to hundreds of thousands of machines (e.g., according to estimates, Amazon EC2 hosts almost half a million machines), and projections anticipate yet another multifold scaling of machine counts. 

The reality of execution on _n_ nodes never meets the ideal of _n_-fold performance escalation. Several reasons intervene: 

1. As shown in Figure 5.13, some program parts can never be parallelized (e.g., initialization).
1. Load imbalance among tasks is highly likely, especially in distributed systems, such as clouds, in which heterogeneity (see the section Heterogeneity) is a major factor. As depicted in Figure 5.13(b), load imbalance usually delays programs so that a program becomes bound to its slowest task. Specifically, even if all tasks in a program finish, the program cannot commit before the last task finishes. 
1. Other serious overheads, such as communication and synchronization overheads, can significantly impede scalability.
![Figure 5.13: Parallel speedup: (a) ideal case and (b) real case](../media/scalability.png)

_Figure 5.13: Parallel speedup: (a) ideal case and (b) real case_


These issues are important when comparing the performance of distributed and sequential programs. A widely used expression that describes speedups and, additionally, accounts for various overheads is Amdahl's law. To illustrate the calculation, we assume that a sequential version of a program _T_ takes _T<sub>s</sub>_ time units, while a parallel/distributed version takes _T<sub>p</sub>_ time units using a cluster of _n_ nodes. In addition, we suppose that fraction _s_ of the program is not parallelizable, leaving the `(1 – s)` portion parallelizable. According to Amdahl's law, the speedup of the parallel/distributed execution of `P` versus the sequential one can be defined as follows: 
<!-- TODO fix
<formula><m:math display="block" xmlns:m="m"><m:mrow><m:msub><m:mi>Speedup</m:mi><m:mi>p</m:mi></m:msub><m:mo>=</m:mo><m:mfrac><m:msub><m:mi>T</m:mi><m:mi>s</m:mi></m:msub><m:msub><m:mi>T</m:mi><m:mi>p</m:mi></m:msub></m:mfrac><m:mo>=</m:mo><m:mfrac><m:msub><m:mi>T</m:mi><m:mi>s</m:mi></m:msub><m:mrow><m:msub><m:mi>T</m:mi><m:mi>s</m:mi></m:msub><m:mo lspace="2px" rspace="2px">times</m:mo><m:mi>s</m:mi><m:mo lspace="2px" rspace="2px">+</m:mo><m:msub><m:mi>T</m:mi><m:mi>s</m:mi></m:msub><m:mo lspace="2px" rspace="2px">times</m:mo><m:mfrac><m:mrow><m:mn>1</m:mn><m:mo>minus</m:mo><m:mi>s</m:mi></m:mrow><m:mi>n</m:mi></m:mfrac></m:mrow></m:mfrac><m:mo>=</m:mo><m:mfrac><m:mn>1</m:mn><m:mrow><m:mi>s</m:mi><m:mo>+</m:mo><m:mfrac><m:mrow><m:mn>1</m:mn><m:mo>minus</m:mo><m:mi>s</m:mi></m:mrow><m:mi>n</m:mi></m:mfrac></m:mrow></m:mfrac></m:mrow></m:math></formula>
-->


Although the formula is apparently simple, it exhibits a crucial implication: if we assume a cluster with an unlimited number of machines and a constant s, we can express the maximum achievable speedup by simply computing the speedup of `P` with an infinite number of processors as follows: 
<!-- TODO fix
<formula><m:math display="block" xmlns:m="m"><m:mrow><m:munder><m:mi>lim</m:mi><m:mrow><m:mi>n</m:mi><m:mo lspace="3px" rspace="3px">rarr</m:mo><m:mi>infin</m:mi></m:mrow></m:munder><m:msub><m:mi>Speedup</m:mi><m:mi>p</m:mi></m:msub><m:mo>=</m:mo><m:munder><m:mi>lim</m:mi><m:mrow><m:mi>n</m:mi><m:mo lspace="3px" rspace="3px">rarr</m:mo><m:mi>infin</m:mi></m:mrow></m:munder><m:mtext fontfamily="Times New Roman"></m:mtext><m:mfrac><m:mn>1</m:mn><m:mrow><m:mi>s</m:mi><m:mo>+</m:mo><m:mfrac><m:mrow><m:mn>1</m:mn><m:mo>minus</m:mo><m:mi>s</m:mi></m:mrow><m:mi>n</m:mi></m:mfrac></m:mrow></m:mfrac><m:mo>=</m:mo><m:mfrac><m:mn>1</m:mn><m:mi>s</m:mi></m:mfrac></m:mrow></m:math></formula>
-->


To understand the essence of this analysis, let us assume a serial fraction `s` of only 2%. Applying the formula with, say, an unlimited number of machines will result in a maximum speedup of only 50. Reducing `s` to 0.5% would result in a maximum speedup of 200. Consequently, we observe that attaining scalability in distributed systems is extremely challenging because it requires `s` to be almost 0, and this analysis ignores the effects of load-imbalance, synchronization, and communication overheads. In practice, synchronization overheads (e.g., performing barrier synchronization and acquiring locks) increase with an increasing number of machines, often super linearly. Communication overheads also grow dramatically in large-scale distributed systems because all machines cannot share short physical connections. Load imbalance becomes a big factor in heterogeneous environments, as we discuss shortly. Although this is truly challenging, we point out that with Web-scale input data, the overheads of synchronization and communication can be greatly reduced if they contribute much less towards overall execution time than does computation. Fortunately, with many big-data applications, this latter situation is the case. 
##  Communication

Even with distributed shared-memory systems, such as DSM, messages are passed internally between machines, albeit in a manner totally transparent to users. Hence, coordination all boils down to passing messages. We can argue, then, that the only way distributed systems can communicate is by passing messages. In fact, Coulouris and associates adopt just this definition for distributed systems. Distributed systems, such as the cloud, rely heavily on the underlying network to deliver messages rapidly enough to destination entities for three main reasons: performance, cost, and quality of service (QoS). Specifically, fast message delivery minimizes execution times, reduces costs (because cloud applications can commit earlier), and raises QoS, especially for audio and video applications. This condition makes the issue of communication a principal theme in developing cloud programs. Indeed, some might argue that communication lies at the heart of the cloud and constitutes one of its major bottlenecks. 

Distributed programs can apply two techniques to address cloud communication bottlenecks.
###  Colocation

Distributing/partitioning work across machines attempts to place highly communicating entities together. This strategy can mitigate pressure on the cloud network and subsequently improve performance. Realizing this goal, however, is not as easy as it might seem. For instance, the standard edge-cut strategy seeks to partition graph vertices into _p_ equally weighted partitions over _p_ processors so that the total weight of the edges crossing between partitions is minimized (see the section _Data Parallel and Graph Parallel Computations_). 

Carefully inspecting this strategy, we recognize a serious shortcoming that directly impacts communication. As Figure 5.10 in section _Data Parallel and Graph Parallel Computations_ shows, the minimum cut resulted from the edge-cut metric overlooks the fact that some edges may represent the same information flow. In particular, _v<sub>2</sub>_ at _P<sub>1</sub>_ in the figure sends the same message twice to _P<sub>2</sub>_ (specifically to _v<sub>4</sub>_ and _v<sub>5</sub>_ at _P<sub>2</sub>_), while it suffices to communicate the message only once because _v<sub>4</sub>_ and _v<sub>5</sub>_ will exist on the same machine. Likewise, _v<sub>4</sub>_ and _v<sub>7</sub>_ can communicate messages to P<sub>1</sub> only once, but they do it twice.

The standard edge-cut metric thus overcounts communication volume and consequently leads to superfluous network traffic. As a result, interconnection bandwidth can be potentially stressed and performance degraded. Even if the total communication volume (or the number of messages) is minimized more effectively, load imbalance can generate a bottleneck. In particular, it may happen that, although communication volume is minimized, some machines receive larger partitions (with more vertices) than others. An ideal, yet challenging, approach is to minimize communication overheads while circumventing computation skew among machines. This latter strategy strives for effective partitioning of work across machines so that highly communicating entities are colocated. 
![Figure 5.14: Effective mapping of graph partitions to cluster machines. A mapping of P1 to the other rack while P2 and P3 remain on the same rack causes more network traffic and potentially degraded performance.](../media/mapping.png)

_Figure 5.14: Effective mapping of graph partitions to cluster machines. A mapping of P1 to the other rack while P2 and P3 remain on the same rack causes more network traffic and potentially degraded performance._

###  Effective Partition Mapping

To be most effective, the strategy for mapping partitions—whether graph or data partitions—to machines should be totally aware of the underlying network topology. This goal usually requires determining the number of switches a message will hit before reaching its destination. As a specific example, Figure 5.14 demonstrates the same graph shown in Figure 5.10 and a simplified cluster with a tree-style network and six machines. The cluster network consists of two rack switches (RSs), each connecting three machines, and a core switch (CS) connecting the two RSs. Note that the bandwidth between any two machines depends on their relative locations in the network topology. For instance, machines on the same rack share a higher bandwidth connection as opposed to machines that are off-rack. Thus, it pays to minimize network traffic across racks. If _P<sub>1</sub>_, _P<sub>2</sub>_, and _P<sub>3</sub>_ are mapped to _M<sub>1</sub>_, _M<sub>2</sub>_, and _M<sub>3</sub>_, respectively, less network latency will be incurred when _P<sub>1</sub>_, _P<sub>2</sub>_, and _P<sub>3</sub>_ communicate versus if they are mapped across the two racks. More precisely, for _P<sub>1</sub>_ to communicate with _P<sub>2</sub>_ on the same rack, only one hop is incurred to route a message from _P<sub>1</sub>_ to _P<sub>2</sub>_. In contrast, for _P<sub>1</sub>_ to communicate with _P<sub>2</sub>_ on different racks, two hops are incurred per each message. Clearly, fewer hops reduce network latency and improve overall performance. Unfortunately, this objective is not as easy to achieve on clouds as it might seem for one main reason: especially on public systems, such as Amazon EC2, network topologies remain hidden. Nevertheless, the network topology can still be learned (though not effectively) using a benchmark such as Netperf to measure point-to-point TCP stream bandwidths between all pairs of cluster nodes. This approach enables estimating the relative locality of nodes and arriving at a reasonable inference regarding the cluster's rack topology. 
##  Heterogeneity

Cloud data centers comprise various collections of components, including computers, networks, operating systems (OSs), code libraries, and programming languages. In principle, if there is variety and difference in data center components, the cloud is referred to as a heterogeneous cloud. Otherwise, the cloud is denoted as a homogenous cloud. In practice, homogeneity does not always hold, mainly due to two reasons: 

1. Cloud providers typically maintain multiple generations of IT resources, purchased over different time frames. 
1. Cloud providers are increasingly applying virtualization technology on their clouds to consolidate servers, enhance system utilization, and simplify management. Public clouds are primarily virtualized data centers. Even on private clouds, virtualized environments are expected to become the norm.

Heterogeneity is a direct result of virtualized environments, and colocating virtual machines (VMs) on similar physical machines can cause heterogeneity. Consider, for example, two identical physical machines, A and B. Even assuming identical VMs running the same programs, placing one VM on machine A and 10 VMs on machine B will stress the second machine more than the first. Having dissimilar VMs and diverse, demanding programs is even more probable on the cloud, and the situation is worse there. An especially compelling setting is Amazon EC2, which offers 17 VM types (see Unit 4) for millions of users with different programs. Clearly, this situation creates even more heterogeneity. In short, heterogeneity is already and will continue to be the norm on the cloud.

Heterogeneity poses multiple challenges for running distributed programs on the cloud. Distributed programs must be designed to mask heterogeneity of the underlying hardware, networks, OSs, and programming languages. The illusion of homogeneity allows distributed tasks to communicate; otherwise message passing and the whole concept of distributed programs fail. Consider the data representation problem: messages exchanged between tasks usually contain primitive data types, such as integers. Unfortunately, not all computers store integers in the same order. In particular, some computers use the so-called big-endian order, in which the most significant byte comes first, while others use the so-called little-endian order, in which the most significant byte comes last. The floating-point numbers can also differ across computer architectures. Another issue is the set of codes used to represent characters. Some systems use ASCII characters, while others use the Unicode standard. In a word, distributed programs have to work out such heterogeneity to exist. The part that can be incorporated in distributed programs to work out heterogeneity is commonly referred to as middleware. Fortunately, most middleware are implemented over the Internet protocols, which themselves mask the differences in the underlying networks. The Simple Object Access Protocol (SOAP) is an example of middleware. SOAP defines a scheme for using Extensible Markup Language (XML), a textual self-describing format, to represent contents of messages and allow distributed tasks at diverse machines to interact. Another example is Representational State Transfer, or REST.

In general, code suitable for one machine might not be suitable for another machine on the cloud, especially when instruction-set architectures (ISAs) vary across machines. Ironically, the virtualization technology, which induces heterogeneity, can effectively serve in solving the problem. Some VMs can be initiated for a user cluster and mapped to physical machines with different underlying ISAs. Afterwards, the virtualization hypervisor will take care of emulating any difference between the ISAs of the provisioned VMs and the underlying physical machines (if any). From a user's perspective, all emulations occur transparently. Last, users can always install their own OSs and libraries on system VMs, like Amazon EC2 instances, thus ensuring homogeneity at the OS and library levels.

Another serious heterogeneity problem that requires attention from distributed programmers is performance variation on the cloud. Performance variation describes the situation in which running the same distributed program twice on the same cluster can result in different execution times. For example, execution times can vary by a factor of five for the same application on the same private cluster. Performance variation is due mostly to cloud heterogeneity, imposed by virtualized environments, and spikes and lulls in resource demand over time. As a consequence, cloud VMs rarely carry work at the same speed, thereby preventing tasks from making progress at (approximately) constant rates. Clearly, this situation can create tricky load imbalance and degrade overall performance because load imbalance makes a program's performance contingent on its slowest task. Distributed programs can attempt to provide relief by detecting slow tasks and scheduling corresponding speculative tasks on fast VMs so that the latter finish earlier. Specifically, two tasks with the same responsibility can compete by running at two different VMs, with the one that finishes earlier getting committed and the other killed. Hadoop MapReduce follows a similar strategy for solving the same problem, called _speculative execution_. Unfortunately, distinguishing between slow and fast tasks/VMs is challenging on the cloud. It could happen that a certain VM running a task is temporarily passing through a demand spike, or it could be the case that the VM is simply faulty. In theory, not every node that is detected to be slow is faulty, and differentiating between faulty and slow nodes is hard. Because of this problem, Hadoop MapReduce does not perform well in heterogeneous environments. Reasons for that and details on Hadoop's speculative execution are presented in Hadoop MapReduce section. 
##  Synchronization

To achieve maximum performance, distributed tasks need the ability to operate simultaneously on shared data without risking data corruption or inconsistency. Synchronization mechanisms address this requirement by allowing programmers to control the sequence of operations (reads and writes) that tasks perform. For instance, GraphLab allows multiple tasks to operate on different vertices of the same graph simultaneously. This capability could lead to race conditions in which two tasks try to modify data on a shared edge at the same time, resulting in a corrupted value. The solution lies in a synchronizing means to assure that distributed tasks can obtain mutually exclusive data access, the _mutual exclusion_ property.

As discussed in the section Shared-Memory Programming Model, three synchronization methods are widely used: semaphores, locks, and barriers. Applying these methods efficiently is a critical goal in developing distributed programs. For instance, although a barrier is easy to implement, a distributed program's overall execution time then becomes dependent on the slowest task. In distributed systems such as the cloud, in which heterogeneity is the norm, this situation can seriously degrade performance. The challenge is to employ synchronization methods without paying performance penalties. 

In addition to mutual exclusion, synchronization mechanisms must also guarantee other properties for distributed programs. To begin, if one task attempts to access a critical section, it should eventually succeed. If two tasks try simultaneously to access a critical section, only one should succeed. However, things may not always go as expected. For instance, if task `A` succeeds in acquiring `lock1` and, at about the same time, task `B` succeeds in acquiring `lock2`; then if task A attempts to acquire `lock2`, and task `B` attempts to acquire `lock1`, we have what is known as a _deadlock_. Avoiding such stalemates presents a significant challenge in developing distributed programs, especially when the number of tasks scales up, and any mutual exclusion mechanism must ensure the deadlock-free property. 

To build upon the example of tasks `A` and `B`, let us assume a larger set of tasks _{A, B, C…, Z}_. In ensuring mutual exclusion, task _A_ might wait on task _B_, if _B_ is holding a lock required by _A_. In return, task _B_ might wait on task _C_, if _C_ is holding a lock required by _B_. The “wait on” sequence can carry on all the way up to task _Z_. Specifically, task _C_ might wait on task _D_, and task _D_ might wait on task _E_, all the way until task _Y_, which might also wait on task _Z_. Such a “wait-on” chain is usually referred to as transitive closure. When a transitive closure occurs, a circular wait is said to arise, which situation normally leads to stark deadlocks that can bring an entire distributed program/system to a grinding halt. Lastly, we note that the wait-on relation lies at the heart of every mutual exclusion mechanism. In particular, no mutual exclusion protocol, no matter how clever, can preclude it. In normal scenarios, a task expects to “wait on” for a limited (reasonable) amount of time. But what if a task that is holding a lock/token crashes? This scenario brings us to another major challenge for distributed programs, namely, fault tolerance. 
|Did You Know?|
|--|
|Mutual exclusion in distributed systems can be categorized into two main classes, _token based_ and _permission based_. In the token-based approach, mutual exclusion is accomplished by passing a single message denoted as token between tasks of a distributed program. Obtaining the token can be deemed as acquiring the lock. As such, a task that holds the token can access the shared data, while every other task will wait until its turn arrives. Tasks using the token-based approach are usually organized logically as a ring. When a task is done, it passes the token to the next task on the ring. The next task can either choose to access the shared data or simply pass the token to the subsequent task on the ring. The token-based approach avoids starvation because it can fairly ensure that each task will have the chance to access the shared data. On the other hand, it suffers from a reliability issue. In particular, if the token is lost on the network (e.g., due to a network failure), or the task that is currently holding the token crashes, an intricate distributed procedure usually has to be involved to make sure the distributed program will continue functioning properly. Losing a token in a distributed system becomes more challenging if the system is scaled up. This problem stems from the increasing probabilities of machine and network failures in large-scale systems. In the permission-based approach, distributed mutual exclusion can be achieved by requiring tasks to ask for permissions (e.g., locks) in order to access shared data. This approach can be implemented using either _centralized_ or _decentralized_ algorithms. In centralized algorithms, a coordinator for granting permissions is employed. A task can always make requests to the coordinator, asking for permissions. The coordinator can either provide or deny permissions, depending on whether there are tasks already accessing the requested data. The coordinator ensures that only one task can write on a shared piece of data at a time, yet can allow multiple reads from multiple tasks on the same data to proceed. Centralized algorithms are easy to implement, robust to starvation, and exhibit fairness. In particular, permissions can be provided in the order that they are asked for and for specified allotted times, which ensures that every task gets a chance to make requests. Nonetheless, centralized algorithms suffer from serious problems. First, the coordinator exposes a single point of failure (SPOF). That is, if the coordinator fails, the whole system will go down. Second, the coordinator can become a performance bottleneck, especially when scaling up the quantities of nodes and users. To address these two main drawbacks of centralized algorithms, decentralized algorithms suggest splitting the central coordinator into multiple coordinators. Subsequently, for a task to acquire (write) permission, it needs to get a majority vote from coordinators (see the _Symmetrical and Asymmetrical Architectural Models_ section for an introduction on voting mechanisms). Clearly, obtaining these permissions makes the distributed program less vulnerable to an SPOF. More precisely, a distributed program with a decentralized, mutually exclusive algorithm can tolerate K out of 2K + 1 coordinator failures. Also, decentralized algorithms remove the performance bottleneck revealed in centralized algorithms. In contrary, decentralized algorithms are more complex to implement and maintain than centralized ones. In general, implementation and maintenance complexities can impede scalability, especially if the number of control messages increases dramatically.|

##  Fault Tolerance

One basic feature that distinguishes clouds and other distributed systems from uniprocessor systems is the concept of partial failures. Specifically, if one node or component fails in a distributed system, the whole system may be able to continue functioning. On the other hand, if one component (e.g., the RAM) fails in a uniprocessor system, the whole system will also fail. A crucial objective in designing distributed systems/programs is to construct them in a way that they can tolerate partial failures automatically, without seriously affecting performance. A key technique for masking faults in distributed systems is to use hardware redundancy, such as the RAID technology (see Unit 4). In most cases, however, distributed programs cannot depend solely on the underlying hardware fault-tolerance techniques of distributed systems. Among the popular techniques that the distributed programs can apply is software redundancy. 
![Figure 5.15: Two classical ways to employ task redundancy. (a) A flat group of tasks. (b) A hierarchical group of tasks with a central process (i.e., T0, whereby T1 stands for task 1.).](../media/task_redundancy.png)

_Figure 5.15: Two classical ways to employ task redundancy. (a) A flat group of tasks. (b) A hierarchical group of tasks with a central process (i.e., T0, whereby T1 stands for task 1.)._


One common type of software redundancy is task redundancy (also called resiliency, or _replication_), which protects against task failures and slowness. Tasks can be replicated as flat or hierarchical groups, exemplified in Figure 5.15. In flat groups (see Figure 5.15(a)), all tasks are identical in that they all carry the same work. Eventually, only the result of one task is accepted, and the other results are discarded. Obviously, flat groups are symmetrical and preclude SPOFs: if one task crashes, the application will stay in business, yet the group will become smaller until recovered. However, if for some applications a decision is to be made (e.g., acquiring a lock), a voting mechanism may be required. As discussed earlier, voting mechanisms incur implementation complexity, communication delays, and performance overheads. 

A hierarchical group (see Figure 5.15(b)) usually employs a coordinator task and specifies the rest of the tasks as workers. In this model, when a user request is made, it gets first forwarded to the coordinator who, in turn, decides which worker is best suited to fulfill the request. Clearly, hierarchical and flat groups reflect opposing properties. In particular, the coordinator is an SPOF and a potential performance bottleneck (especially in large-scale systems with millions of users). In contrast, as long as the coordinator is protected, the whole group remains functional. Furthermore, decisions can be easily made, solely by the coordinator without bothering any worker or incurring communication delays and performance overheads. A hybrid of flat and hierarchical task groups is adopted by Hadoop MapReduce but only for task failures. Details on that adoption are provided in the section on Hadoop MapReduce. 
![Figure 5.16: Demonstrating distributed checkpointing. D1 is a valid distributed checkpoint, while D2 is not because it is inconsistent. Specifically, D2’s checkpoint at Q indicates that m2 has been received, while D2’s checkpoint at P does not indicate that m2 has been sent.](../media/checkpointing.png)

_Figure 5.16: Demonstrating distributed checkpointing. D1 is a valid distributed checkpoint, while D2 is not because it is inconsistent. Specifically, D2’s checkpoint at Q indicates that m2 has been received, while D2’s checkpoint at P does not indicate that m2 has been sent._


In distributed programs, fault tolerance concerns not only surviving faults but also recovering from failures. The basic idea here is to replace a flawed state with a flaw-free state, and one way to achieve this goal is through backward recovery. This strategy requires that the distributed program/system is brought from a current, flawed state to a previously correct state and relies on periodically recording the system's state at each process, which is called obtaining a _checkpoint_. When a failure occurs, recovery can be started from the last recorded correct state, typically called the _recovery line_.

Checkpoints of a distributed program at different processes in a distributed system constitute a distributed checkpoint. The process of capturing a distributed checkpoint is not easy because of one main reason. Specifically, a distributed checkpoint must maintain a consistent global state; that is, it should maintain the property that if a process _P_ has recorded the receipt of a message, _m_, then there should be another process _Q_ that has recorded the sending of _m_. After all, _m_ must have come from a known process. Figure 5.16 demonstrates two distributed checkpoints, _D<sub>1</sub>_, which maintains a consistent global state, and _D<sub>2</sub>_, which does not. _D<sub>1</sub>_’s checkpoint at _Q_ indicates that _Q_ has received a message, _m<sub>1</sub>_, and the _D<sub>1</sub>_’s checkpoint at _P_ indicates that _P_ has sent _m<sub>1</sub>_, hence, making _D<sub>1</sub>_ consistent. In contrast, _D<sub>2</sub>_’s checkpoint at _Q_ indicates that message _m<sub>2</sub>_ has been received, and _D<sub>2</sub>_’s checkpoint at _P_ does not indicate that _m<sub>2</sub>_ has been sent from _P_. Therefore, _D<sub>2</sub>_ must be considered inconsistent and cannot be used as a recovery line. 
![Figure 5.17: The domino effect that might result from rolling back each process (e.g., processes P and Q) to a saved, local checkpoint in order to locate a recovery line. Neither D1, D2, nor D3 are recovery lines because they exhibit inconsistent global states.](../media/domino_effect.png)

_Figure 5.17: The domino effect that might result from rolling back each process (e.g., processes P and Q) to a saved, local checkpoint in order to locate a recovery line. Neither D1, D2, nor D3 are recovery lines because they exhibit inconsistent global states._


By rolling back each process to its most recently saved state, a distributed program/system can inspect a candidate distributed checkpoint to determine its consistency. When local states jointly form a consistent global state, a recovery line is said to be discovered. For instance, after a failure, the system exemplified in Figure 5.16 will roll back until hitting _D<sub>1</sub>_. Because _D<sub>1</sub>_ reflects a global consistent state, we have obtained a recovery line. Unfortunately, the process of cascaded rollbacks is challenging because it can lead to a domino effect. As a specific example, Figure 5.17 exhibits a case in which a recovery line cannot be found. In particular, every distributed checkpoint in Figure 5.17 is indeed inconsistent. This pitfall makes distributed checkpointing a costly operation that may not converge to an acceptable recovery solution. Many fault-tolerant distributed systems thus combine checkpointing with message logging, recording each process message before sending and after a checkpoint has been taken. This tactic solves the problem of _D<sub>2</sub>_ in Figure 5.16, for example. In particular, after _D<sub>2</sub>_’s checkpoint at _P_ is taken, the send of _m<sub>2</sub>_ will be marked in a log message at _P_, which, if merged with _D<sub>2</sub>_’s checkpoint at _Q_, can form a global consistent state. The Hadoop Distributed File System (HDFS) itself combines distributed checkpointing (the image file) and message logging (the edit file) to recover NameNode failures (see Unit 4). Pregel and GraphLab, discussed in later sections, apply only distributed checkpointing.
##  Scheduling

The effectiveness of a distributed program hinges on the manner in which its constituent tasks are scheduled over distributed machines. This scheduling is usually categorized into two main classes, one for tasks and one for jobs. Tasks are the finest unit of execution granularity, and a job can encompass one or many tasks. Multiple users can submit numerous jobs simultaneously for execution on a cluster, and job schedulers determine which should go next. Hadoop MapReduce, for instance, utilizes a first-in, first-out (FIFO) job scheduler, whereby jobs run in order of receipt, and a scheduled job will occupy the whole cluster until the job has no more tasks to schedule. Hadoop MapReduce also employs other job schedulers, such as the Capacity and Fair Schedulers (see the Hadoop MapReduce section). After a job is granted the cluster, the scheduling decision morphs into how to schedule the job's component tasks. Tasks can be scheduled either close to the data that they will process, or anywhere. When tasks are scheduled near their data, locality is considered to be exploited. For example, Hadoop MapReduce incorporates two types of tasks, map and reduce tasks. Map tasks are scheduled in the vicinity of their uniform-sized input HDFS blocks, while reduce tasks are scheduled at any cluster nodes (anywhere), irrespective of their input data locations. Pregel and GraphLab, on the other hand, do not exploit any locality when scheduling tasks. Hadoop MapReduce, Pregel, and GraphLab task schedulers are detailed in the sections Hadoop MapReduce, Pregel, and GraphLab, respectively. 

To avoid significant performance degradation, task schedulers must also account for heterogeneity in the underlying cloud system. Similar tasks that belong to the same job, for example, can be scheduled in a heterogeneous cloud at nodes of differing speed. This procedure, however, can introduce load imbalance and make jobs progress at the pace of their slowest tasks. Strategies such as Hadoop MapReduce's speculative execution can mitigate such problems (see the Hadoop MapReduce section).

In addition, task schedulers must seek to enhance system utilization and improve task parallelism. The objective here is to distribute tasks uniformly across cluster machines in a way that utilizes the available resources fairly and increases parallelism effectively, but this goal presents some contradictory priorities. To begin, by evenly distributing tasks across cluster machines, locality may be affected. Machines in a Hadoop cluster, for instance, can contain different numbers of HDFS blocks. If one machine has a significantly larger number of blocks compared to other machines, locality would imply scheduling all map tasks in that machine. This disposition might make other machines less loaded and utilized. In addition, this strategy can reduce task parallelism by accumulating many tasks on the same machine. 

If locality is relaxed somewhat, utilization could be enhanced, loads across machines could be balanced, and task parallelism could be increased. However, relaxing locality would necessitate moving data towards tasks, and, if done injudiciously, relaxation could raise communication overheads, thereby impeding scalability and potentially degrading performance. In fact, with data centers hosting thousands of machines, moving data frequently towards distant tasks might become a major bottleneck. To improve performance and reduce costs, an optimal task scheduler should strike a balance between system utilization, load balancing, task parallelism, communication overheads, and scalability. Unfortunately, in practice, this ideal is hard to realize, and, indeed, most task schedulers attempt to optimize one objective and overlook the others. 

Another major challenge when scheduling jobs and tasks is to meet what are called service-level objectives (SLOs), which reflect the performance expectations of end users. Amazon, Google, and Microsoft have identified SLO violations as a major cause of user dissatisfaction. An SLO might be expressed, for example, as a maximum acceptable latency for allocating desired resources to a job, a soft/hard deadline to finish a job, or GPU preferences for certain tasks. In multitenant, heterogeneous clusters, SLOs are hard to achieve, especially when new jobs arrive while others are executing. This situation could require suspending currently running tasks and allowing the new ones to proceed in order to meet their own specified SLOs. The capability to suspend and resume tasks is called _task elasticity_. Unfortunately, most distributed analytics engines—including Hadoop MapReduce, Pregel, and GraphLab—do not yet support task elasticity. Enabling elasticity is quite challenging and requires identifying safe points at which a task can be suspended without affecting its correctness and such that its committed work need not be repeated on resumption. Clearly, this capability resembles context switching in modern operating systems. 

### References

1. _S. Chen and S. W. Schlosser (2008). Map Reduce Meets Wider Varieties of Applications IRP-TR-08-05, Intel Research_
2. _J. Deanand and S. Ghemawat (Dec. 2004). MapReduce: Simplified Data Processing on Large Clusters OSDI_
3. _M. Zaharia, A. Konwinski, A. Joseph, R. Katz, and I. Stoica (2008). Improving Mapreduce Performance in Heterogeneous Environments OSDI_
4. _H. Liu (March 2012). Amazon Data Center Size http://huanliu.wordpress.com/2012/03/13/amazon-data-center-size/_
5. _Y. Solihin (2009). Fundamentals of Parallel Computer Architecture Solihin Books_
6. _G. Coulouris, J. Dollimore, T. Kindberg, and G. Blair (May 2011). Distributed Systems: Concepts and Design Addison-Wesley_
7. _Netperf ().  http://www.netperf.org/_
8. _M. Hammoud, M. S. Rehmanand, and M. F. Sakr (2012.). Center-of-Gravity Reduce Task Scheduling to Lower MapReduce Network Traffic CLOUD_
9. _CORBA ().  http://www.corba.org/_
10. _Java RMI ().  http://www.oracle.com/technetwork/java/javase/tech/index-jsp-138781.html_
11. _B. Farley, V. Varadarajan, K. Bowers, A. Juels, T. Ristenpart, and M. Swift (2012). More for Your Money: Exploiting Performance Heterogeneity in Public Clouds SOCC_
12. _M. S. Rehmanand and M. F. Sakr (Nov. 2010). Initial Findings for Provisioning Variation in Cloud Computing CloudCom_
13. _Amazon Elastic Compute Cloud ().  http://aws.amazon.com/ec2/_
14. _A. S. Tanenbaum and M. V. Steen (October 12, 2006). Distributed Systems: Principles and Paradigms Prentice Hall, Second Edition_
15. _T. D. Braun, H. J. Siegel, N. Beck, L. L. Blni, M. Maheswaran, A. I. Reuther, J. P. Robertson, M. D. Theys, B. Yao, D Hensgen, and R. F. Freund ( June 2001). A Comparison of Eleven Static Heuristics for Mapping a Class of Independent Tasks onto Heterogeneous Distributed Computing Systems JPDC_
16. _M. Herlihy and N. Shavit (March 14, 2008). The Art of Multiprocessor Programming Morgan Kaufmann, First Edition_
17. _J. Hamilton (2009). The Cost of Latency http://perspectives.mvdirona.com/2009/10/31/TheCostOfLatency.aspx_
18. _Amazon Elastic Compute Cloud ().  http://aws.amazon.com/ec2/_
19. _Amazon Elastic Compute Cloud ().  http://aws.amazon.com/ec2/_