<!-- Original file: C:\Users\Mark\Desktop\CMU-source\v_5_3\content\_u05_distributed_programming_analytics_engines\_u05_m01_intro\x-oli-workbook_page\_u05_m01_5_challenges_1_Scalability.xml -->
##  Challenges in Building Cloud Programs

Designing and implementing a distributed program involves, as we have seen, choosing a programming model and addressing issues of synchrony, parallelism, and architecture. Beyond these matters, when developing cloud programs, a designer must also pay careful attention to several other challenges critical to cloud environments. We next discuss challenges associated with scalability, communication, heterogeneity, synchronization, fault tolerance, and scheduling.
##  Scalability

A distributed program is considered to be scalable if it remains effective when the quantities of users, data, and resources increase significantly. To get a sense of the problem scope, consider the many popular applications and platforms currently offered to millions of users as Internet-based services. Along the data dimension, in this time of big data and the "era of tera" (Intel's phrase) distributed programs typically cope with Web-scale data on the order of hundreds or thousands of gigabytes, terabytes, or petabytes. Google, for example, processes 20PBs of data per day. Globally, in 2010, data sources generated approximately 1.2ZB (1.2 million petabytes), and the 2020 predictions expect an increase of nearly 44 times that amount. Internet services, such as e-commerce and social networks, handle data volumes generated by millions of users daily. Regarding resources, cloud data centers already host tens to hundreds of thousands of machines (e.g., according to estimates, Amazon EC2 hosts almost half a million machines), and projections anticipate yet another multifold scaling of machine counts. 

The reality of execution on _n_ nodes never meets the ideal of _n_-fold performance escalation. Several reasons intervene: 

1. As shown in Figure 5.13, some program parts can never be parallelized (e.g., initialization).
1. Load imbalance among tasks is highly likely, especially in distributed systems, such as clouds, in which heterogeneity (see the section Heterogeneity) is a major factor. As depicted in Figure 5.13(b), load imbalance usually delays programs so that a program becomes bound to its slowest task. Specifically, even if all tasks in a program finish, the program cannot commit before the last task finishes. 
1. Other serious overheads, such as communication and synchronization overheads, can significantly impede scalability.

![Figure 5.13: Parallel speedup: (a) ideal case and (b) real case](../media/scalability.png)

_Figure 5.13: Parallel speedup: (a) ideal case and (b) real case_


These issues are important when comparing the performance of distributed and sequential programs. A widely used expression that describes speedups and, additionally, accounts for various overheads is Amdahl's law. To illustrate the calculation, we assume that a sequential version of a program **T** takes **T<sub>s</sub>** time units, while a parallel/distributed version takes **T<sub>p</sub>** time units using a cluster of **n** nodes. In addition, we suppose that fraction **s** of the program is not parallelizable, leaving the `(1 â€“ s)` portion parallelizable. According to Amdahl's law, the speedup of the parallel/distributed execution of `P` versus the sequential one can be defined as follows: 
<!-- TODO fix
<formula><m:math display="block" xmlns:m="m"><m:mrow><m:msub><m:mi>Speedup</m:mi><m:mi>p</m:mi></m:msub><m:mo>=</m:mo><m:mfrac><m:msub><m:mi>T</m:mi><m:mi>s</m:mi></m:msub><m:msub><m:mi>T</m:mi><m:mi>p</m:mi></m:msub></m:mfrac><m:mo>=</m:mo><m:mfrac><m:msub><m:mi>T</m:mi><m:mi>s</m:mi></m:msub><m:mrow><m:msub><m:mi>T</m:mi><m:mi>s</m:mi></m:msub><m:mo lspace="2px" rspace="2px">times</m:mo><m:mi>s</m:mi><m:mo lspace="2px" rspace="2px">+</m:mo><m:msub><m:mi>T</m:mi><m:mi>s</m:mi></m:msub><m:mo lspace="2px" rspace="2px">times</m:mo><m:mfrac><m:mrow><m:mn>1</m:mn><m:mo>minus</m:mo><m:mi>s</m:mi></m:mrow><m:mi>n</m:mi></m:mfrac></m:mrow></m:mfrac><m:mo>=</m:mo><m:mfrac><m:mn>1</m:mn><m:mrow><m:mi>s</m:mi><m:mo>+</m:mo><m:mfrac><m:mrow><m:mn>1</m:mn><m:mo>minus</m:mo><m:mi>s</m:mi></m:mrow><m:mi>n</m:mi></m:mfrac></m:mrow></m:mfrac></m:mrow></m:math></formula>
-->


Although the formula is apparently simple, it exhibits a crucial implication: if we assume a cluster with an unlimited number of machines and a constant s, we can express the maximum achievable speedup by simply computing the speedup of `P` with an infinite number of processors as follows: 
<!-- TODO fix
<formula><m:math display="block" xmlns:m="m"><m:mrow><m:munder><m:mi>lim</m:mi><m:mrow><m:mi>n</m:mi><m:mo lspace="3px" rspace="3px">rarr</m:mo><m:mi>infin</m:mi></m:mrow></m:munder><m:msub><m:mi>Speedup</m:mi><m:mi>p</m:mi></m:msub><m:mo>=</m:mo><m:munder><m:mi>lim</m:mi><m:mrow><m:mi>n</m:mi><m:mo lspace="3px" rspace="3px">rarr</m:mo><m:mi>infin</m:mi></m:mrow></m:munder><m:mtext fontfamily="Times New Roman"></m:mtext><m:mfrac><m:mn>1</m:mn><m:mrow><m:mi>s</m:mi><m:mo>+</m:mo><m:mfrac><m:mrow><m:mn>1</m:mn><m:mo>minus</m:mo><m:mi>s</m:mi></m:mrow><m:mi>n</m:mi></m:mfrac></m:mrow></m:mfrac><m:mo>=</m:mo><m:mfrac><m:mn>1</m:mn><m:mi>s</m:mi></m:mfrac></m:mrow></m:math></formula>
-->


To understand the essence of this analysis, let us assume a serial fraction `s` of only 2%. Applying the formula with, say, an unlimited number of machines will result in a maximum speedup of only 50. Reducing `s` to 0.5% would result in a maximum speedup of 200. Consequently, we observe that attaining scalability in distributed systems is extremely challenging because it requires `s` to be almost 0, and this analysis ignores the effects of load-imbalance, synchronization, and communication overheads. In practice, synchronization overheads (e.g., performing barrier synchronization and acquiring locks) increase with an increasing number of machines, often super linearly. Communication overheads also grow dramatically in large-scale distributed systems because all machines cannot share short physical connections. Load imbalance becomes a big factor in heterogeneous environments, as we discuss shortly. Although this is truly challenging, we point out that with Web-scale input data, the overheads of synchronization and communication can be greatly reduced if they contribute much less towards overall execution time than does computation. Fortunately, with many big-data applications, this latter situation is the case. 

### References

1. _S. Chen and S. W. Schlosser (2008). Map Reduce Meets Wider Varieties of Applications IRP-TR-08-05, Intel Research_
2. _J. Deanand and S. Ghemawat (Dec. 2004). MapReduce: Simplified Data Processing on Large Clusters OSDI_
3. _M. Zaharia, A. Konwinski, A. Joseph, R. Katz, and I. Stoica (2008). Improving Mapreduce Performance in Heterogeneous Environments OSDI_
4. _Y. Solihin (2009). Fundamentals of Parallel Computer Architecture Solihin Books_
5. _Amazon Elastic Compute Cloud ().  http://aws.amazon.com/ec2/_