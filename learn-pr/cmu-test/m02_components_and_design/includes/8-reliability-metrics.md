<!-- Original file: C:\Users\Mark\Desktop\CMU\v_5_3\content\_u02_data_centers\_u02_m02_components_and_design\x-oli-workbook_page\_u02_m02_7_Design.xml -->
##  Mean Time Between Failures (MTBF)
Sometimes, when you read material referencing availability and reliability, you will see the term _nines_ used. Five nines or nine nines refers to the number of nines in a percentage of availability. Two nines is 99%, three nines is 99.9%, four is 99.99%, and so on.

You will also see the phrases mean time between failure (MTBF) and mean time to failure (MTTF) in the specifications for many individual components (e.g., hard drives, motherboards, power supplies). These are defined as the average number of hours that component is expected to last and are usually determined by the manufacturer using a sample of parts in more extreme conditions. However, reported failure rates in the field often are higher. For example, hard drives are rated 1 million hours or more, but they have been found to be 2 to 10 times higher,<!-- <link href="http://static.usenix.org/events/fast07/tech/schroeder/schroeder_html/index.html" >link</link> (added citation ~CL) --> and Google found drive failures rates to be 50% higher on average in their study.<!-- <link href="http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//archive/disk_failures.pdf" >link</link> --> The failure rate is 1/MTBF. For example, if the MTBF of a device is 100 hours, then the chances of that device failing in 1 hour is 1/100, 0.01, or 1%.

It is important to note that when determining the overall MTBF of a system that has non-redundant components, the MTBFs of each individual component add as a reciprocal. Formally, 

1MTBFsystem=1MTBFc1+1MTBFc2+ . . . +1MTBFcnOn the other hand, when a system consists of redundant components, required in both components simultaneously to have an overall system failure. The overall MTBF of the system is thus the product of the MTBFs of each individual redundant component of the system. Formally,

MTBFsystem=MTBFrc1timesMTBFrc2× . . . ×MTBFrcnOne factor that is often overlooked when considering uptime is human error. No matter how much redundancy is designed into the system, even if it is properly implemented and maintained, there is some likelihood of a mistake being made by a person. The result of which eventually leads to a service being unavailable (downtime). Some mistakes can be prevented through policy, specifying standard configurations, good documentation, and change management.

When it comes to large cloud deployments, there is little focus on the hardware resiliency of an individual server. When 10,000 or more servers are working together as part of a single application, the application itself builds in the fault tolerance (more on that in the next unit, "Resource Sharing"). In this situation, a single server failure, or even several, will not disrupt the application/service. Small and medium sized businesses, or even a large enterprise that has legacy applications, cannot afford to author these cloud-style, fully customized applications, so they rely on third-party software, most of which does not respond well to hardware failures. Instead, cloud providers will focus on server hardware that is inexpensive and as energy efficient as possible, removing unneeded parts.