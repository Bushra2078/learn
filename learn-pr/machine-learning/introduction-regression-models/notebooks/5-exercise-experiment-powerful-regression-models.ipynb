{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup chunk to install and load required packages\n",
        "knitr::opts_chunk$set(warning = FALSE, message = FALSE)\n",
        "suppressWarnings(if(!require(\"pacman\")) install.packages(\"pacman\"))\n",
        "\n",
        "pacman::p_load('tidyverse', 'tidymodels', 'glmnet',\n",
        "               'randomForest', 'xgboost','patchwork',\n",
        "               'paletteer', 'here', 'doParallel', 'summarytools')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regression - Experiment with more powerful regression models\n",
        "\n",
        "In the previous notebook, you used simple regression models to look at the relationship between features of a bike rentals dataset. In this notebook, you'll experiment with more complex models to improve your regression performance.\n",
        "\n",
        "Let's load the bicycle-sharing data as a tibble and view the first few rows. You'll also split the data into training and test datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the required packages and make them available in your current R session\n",
        "suppressPackageStartupMessages({\n",
        "  library(tidyverse)\n",
        "  library(tidymodels)\n",
        "  library(lubridate)\n",
        "  library(paletteer)\n",
        "})\n",
        "\n",
        "# Import the data into the R session\n",
        "bike_data <- read_csv(file = \"https://raw.githubusercontent.com/MicrosoftDocs/ml-basics/master/data/daily-bike-share.csv\", show_col_types = FALSE)\n",
        "\n",
        "# Parse dates then extract days\n",
        "bike_data <- bike_data %>%\n",
        "  mutate(dteday = mdy(dteday)) %>% \n",
        "  mutate(day = day(dteday))\n",
        "\n",
        "# Select desired features and labels\n",
        "bike_select <- bike_data %>% \n",
        "  select(c(season, mnth, holiday, weekday, workingday, weathersit,\n",
        "           temp, atemp, hum, windspeed, rentals)) %>% \n",
        "  mutate(across(1:6, factor))\n",
        "\n",
        "# Split 70% of the data for training and the rest for testing\n",
        "set.seed(2056)\n",
        "bike_split <- bike_select %>% \n",
        "  initial_split(prop = 0.7,\n",
        "  # splitting data evenly on the holiday variable\n",
        "                strata = holiday)\n",
        "\n",
        "# Extract the data in each split\n",
        "bike_train <- training(bike_split)\n",
        "bike_test <- testing(bike_split)\n",
        "\n",
        "# Specify multiple regression metrics\n",
        "eval_metrics <- metric_set(rmse, rsq)\n",
        "\n",
        "\n",
        "cat(\"Training Set\", nrow(bike_train), \"rows\",\n",
        "    \"\\nTest Set\", nrow(bike_test), \"rows\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result is two datasets:\n",
        "\n",
        "-   **bike_train**: A subset of the dataset used to train the model.\n",
        "-   **bike_test**: A subset of the dataset used to validate the model.\n",
        "\n",
        "Now you're ready to train a model by fitting a suitable regression algorithm to the training data.\n",
        "\n",
        "### Experiment with algorithms\n",
        "\n",
        "The linear regression algorithm you used last time to train the model has some predictive capability. There are other kinds of regression algorithms you could try:\n",
        "\n",
        "-   **Linear algorithms**: Not just the linear regression algorithm you used, which is technically an ordinary least squares algorithm, but other variants such as lasso and ridge. Lasso is an acronym for least absolute shrinkage and selection operator.\n",
        "-   **Tree-based algorithms**: Algorithms that build a decision tree to reach a prediction.\n",
        "-   **Ensemble algorithms**: Algorithms that combine the outputs of multiple base algorithms to improve generalizability.\n",
        "\n",
        "For a full list of parsnip model types and engines, see parsnip [model types and engines](https://www.tidymodels.org/find/parsnip/#models) and explore corresponding [model arguments](https://www.tidymodels.org/find/parsnip/#model-args) too.\n",
        "\n",
        "### Try another linear algorithm\n",
        "\n",
        "Let's try training the regression model by using a lasso algorithm. In Tidymodels, you change the model specification, and the rest is easy.\n",
        "\n",
        "Here, you'll set up one model specification for lasso regression. You picked a value for `penalty`. You set `mixture = 1` to specify a lasso model. When mixture = 1, it's a pure lasso model.\n",
        "\n",
        "You'll also make a model specification in a more succinct way than you did last time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build a lasso model specification\n",
        "lasso_spec <- linear_reg(\n",
        "  engine = \"glmnet\",\n",
        "  mode = \"regression\",\n",
        "  penalty = 1,\n",
        "  mixture = 1)\n",
        "\n",
        "# Train a lasso regression model\n",
        "lasso_mod <- lasso_spec %>% \n",
        "  fit(rentals ~ ., data = bike_train)\n",
        "\n",
        "# Make predictions for test data\n",
        "results <- bike_test %>% \n",
        "  bind_cols(lasso_mod %>% predict(new_data = bike_test) %>% \n",
        "              rename(predictions = .pred))\n",
        "\n",
        "# Evaluate the model\n",
        "lasso_metrics <- eval_metrics(data = results,\n",
        "                                    truth = rentals,\n",
        "                                    estimate = predictions) \n",
        "\n",
        "\n",
        "# Plot predicted vs actual\n",
        "theme_set(theme_light())\n",
        "lasso_plt <- results %>% \n",
        "  ggplot(mapping = aes(x = rentals, y = predictions)) +\n",
        "  geom_point(size = 1.6, color = 'darkorchid') +\n",
        "  # overlay regression line\n",
        "  geom_smooth(method = 'lm', color = 'black', se = F) +\n",
        "  ggtitle(\"Daily Bike Share Predictions\") +\n",
        "  xlab(\"Actual Labels\") +\n",
        "  ylab(\"Predicted Labels\") +\n",
        "  theme(plot.title = element_text(hjust = 0.5))\n",
        "\n",
        "# Return evaluations\n",
        "list(lasso_metrics, lasso_plt)\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There's not much of an improvement. To improve the performance metrics, you can estimate the right regularization hyperparameter `penalty`. This can be figured out by resampling and tuning the model, which we'll discuss.\n",
        "\n",
        "### Try a decision tree algorithm\n",
        "\n",
        "As an alternative to a linear model, a category of algorithms for machine learning uses a tree-based approach. The features in the dataset are examined in a series of evaluations. Each evaluation results in a branch in a decision tree based on the feature value. At the end of each series of branches are leaf nodes with the predicted label value based on the feature values.\n",
        "\n",
        "It's easiest to see how this process works with an example. Let's train a decision tree regression model by using the bike rental data. After you train the model, the following code prints the model definition and a text representation of the tree it uses to predict label values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build a decision tree specification\n",
        "tree_spec <- decision_tree(\n",
        "  engine = \"rpart\",\n",
        "  mode = \"regression\")\n",
        "\n",
        "# Train a decision tree model \n",
        "tree_mod <- tree_spec %>% \n",
        "  fit(rentals ~ ., data = bike_train)\n",
        "\n",
        "# Print model\n",
        "tree_mod\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you have a tree-based model. But is it any good? Let's evaluate it with the test data. You'll also try out a new function, `augment()`.\n",
        "\n",
        "The `augment()` function allows you to make and add model predictions to the given data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make and bind predictions to test data a\n",
        "results <- tree_mod %>% \n",
        "  augment(new_data = bike_test) %>% \n",
        "  rename(predictions = .pred)\n",
        "\n",
        "# Evaluate the model\n",
        "tree_metrics <- eval_metrics(data = results,\n",
        "                                  truth = rentals,\n",
        "                                  estimate = predictions)\n",
        "\n",
        "# Plot predicted vs actual\n",
        "tree_plt <- results %>% \n",
        "  ggplot(mapping = aes(x = rentals, y = predictions)) +\n",
        "  geom_point(color = 'tomato') +\n",
        "  # overlay regression line\n",
        "  geom_smooth(method = 'lm', color = 'steelblue', se = F) +\n",
        "  ggtitle(\"Daily Bike Share Predictions\") +\n",
        "  xlab(\"Actual Labels\") +\n",
        "  ylab(\"Predicted Labels\") +\n",
        "  theme(plot.title = element_text(hjust = 0.5))\n",
        "\n",
        "# Return evaluations\n",
        "list(tree_metrics, tree_plt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The tree-based model doesn't seem to have much improvement over the linear model. You can also see that it's predicting constant values for a given range of predictors. What else can you try?\n",
        "\n",
        "### Try an ensemble algorithm\n",
        "\n",
        "Ensemble algorithms work by combining multiple base estimators to produce an optimal model. They apply an aggregate function to a collection of base models, which is known as bagging. Or they leverage a sequence of models that build on one another to improve predictive performance, which is known as boosting.\n",
        "\n",
        "For example, let's try a random forest model. It applies an averaging function to multiple decision tree models for a better overall model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For reproducibility\n",
        "set.seed(2056)\n",
        "\n",
        "# Build a random forest model specification\n",
        "rf_spec <- rand_forest() %>% \n",
        "  set_engine('randomForest') %>% \n",
        "  set_mode('regression')\n",
        "\n",
        "# Train a random forest model \n",
        "rf_mod <- rf_spec %>% \n",
        "  fit(rentals ~ ., data = bike_train)\n",
        "\n",
        "# Print model\n",
        "rf_mod\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you have a random forest model. But is it any good? Let's evaluate it with the test data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make and bind predictions to test data a\n",
        "results <- rf_mod %>% \n",
        "  augment(new_data = bike_test) %>% \n",
        "  rename(predictions = .pred)\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "rf_metrics <- eval_metrics(data = results,\n",
        "                                  truth = rentals,\n",
        "                                  estimate = predictions)\n",
        "\n",
        "\n",
        "# Plot predicted vs actual\n",
        "rf_plt <- results %>% \n",
        "  ggplot(mapping = aes(x = rentals, y = predictions)) +\n",
        "  geom_point(color = '#6CBE50FF') +\n",
        "  # overlay regression line\n",
        "  geom_smooth(method = 'lm', color = '#2B7FF9FF', se = F) +\n",
        "  ggtitle(\"Daily Bike Share Predictions\") +\n",
        "  xlab(\"Actual Labels\") +\n",
        "  ylab(\"Predicted Labels\") +\n",
        "  theme(plot.title = element_text(hjust = 0.5))\n",
        "\n",
        "# Return evaluations\n",
        "list(rf_metrics, rf_plt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's a step in the right direction.\n",
        "\n",
        "Let's also try a boosting ensemble algorithm. You'll use a gradient boosting estimator. Like a random forest algorithm, it's based on multiple trees. Instead of building them all independently and taking the average result, each tree is built on the outputs of the previous one. This technique is an attempt to incrementally reduce the loss (error) in the model.\n",
        "\n",
        "In this tutorial, we'll demonstrate how to implement gradient boosting machines by using the `xgboost` engine.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For reproducibility\n",
        "set.seed(2056)\n",
        "\n",
        "# Build an xgboost model specification\n",
        "boost_spec <- boost_tree() %>% \n",
        "  set_engine('xgboost') %>% \n",
        "  set_mode('regression')\n",
        "\n",
        "# Train an xgboost model \n",
        "boost_mod <- boost_spec %>% \n",
        "  fit(rentals ~ ., data = bike_train)\n",
        "\n",
        "\n",
        "# Print model\n",
        "boost_mod\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the output, you can see that gradient boosting machines combine a series of base models. Each model is created sequentially and depends on the previous models. This technique is an attempt to incrementally reduce the error in the model.\n",
        "\n",
        "Now you have an XGBoost model. But is it any good? Again, let's evaluate it with the test data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make and bind predictions to test data a\n",
        "results <- boost_mod %>% \n",
        "  augment(new_data = bike_test) %>% \n",
        "  rename(predictions = .pred)\n",
        "\n",
        "# Evaluate the model\n",
        "boost_metrics <- eval_metrics(data = results,\n",
        "                                  truth = rentals,\n",
        "                                  estimate = predictions) \n",
        "\n",
        "# Plot predicted vs actual\n",
        "boost_plt <- results %>% \n",
        "  ggplot(mapping = aes(x = rentals, y = predictions)) +\n",
        "  geom_point(color = '#4D3161FF') +\n",
        "  # overlay regression line\n",
        "  geom_smooth(method = 'lm', color = 'black', se = F) +\n",
        "  ggtitle(\"Daily Bike Share Predictions\") +\n",
        "  xlab(\"Actual Labels\") +\n",
        "  ylab(\"Predicted Labels\") +\n",
        "  theme(plot.title = element_text(hjust = 0.5))\n",
        "\n",
        "# Return evaluations\n",
        "list(boost_metrics, boost_plt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You're definitely getting somewhere. Can you do better?\n",
        "\n",
        "### Summary\n",
        "\n",
        "You've tried some new regression algorithms to improve performance. In the next exercise unit, you'll look at tuning these algorithms to improve performance. Then you'll take a look at data preprocessing and model hyperparameters.\n",
        "\n",
        "### Further reading\n",
        "\n",
        "To learn more about Tidymodels, see the [Tidymodels documentation](https://www.tidymodels.org/).\n"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": "",
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "codemirror_mode": "r",
      "file_extension": ".r",
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.4.1"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
