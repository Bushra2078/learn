{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise - Train and evaluate a clustering model by using Tidymodels and friends\n",
        "\n",
        "### Clustering introduction\n",
        "\n",
        "In contrast to *supervised* machine learning, *unsupervised* machine learning is used when there is no \"ground truth\" from which to train and validate label predictions. The most common form of unsupervised learning is *clustering*. Clustering is similar to *classification*, except that the training data doesn't include known values for the class label to be predicted.\n",
        "\n",
        "Clustering works by separating the training cases based on similarities that can be determined from their feature values. The numeric features of a particular entity can be thought of as vector coordinates that define the entity's position in n-dimensional space. A clustering model identifies groups, or *clusters*, of entities that are close to one another, while being separated from other clusters.\n",
        "\n",
        "For example, let's take a look at a dataset that contains measurements of different species of wheat seed.\n",
        "\n",
        "> **Citation**: The seeds dataset used in this exercise was originally published by the Institute of Agrophysics of the Polish Academy of Sciences in Lublin, and can be downloaded from the UCI dataset repository (Dua, D. and Graff, C. (2019). [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml). Irvine, CA: University of California, School of Information and Computer Science).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the core tidyverse and make it available in your current R session\n",
        "library(tidyverse)\n",
        "\n",
        "# Read the csv file into a tibble\n",
        "seeds <- read_csv(file = \"https://raw.githubusercontent.com/MicrosoftDocs/ml-basics/master/data/seeds.csv\")\n",
        "\n",
        "# Print the first 5 rows of the data\n",
        "seeds %>% \n",
        "  slice_head(n = 5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sometimes, you might want more information on your data. You can look at the data, its structure, and the data type of its features by using the [*glimpse()*](https://pillar.r-lib.org/reference/glimpse.html) function, as follows:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore dimension and type of columns\n",
        "seeds %>% \n",
        "  glimpse()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also use `skimr::skim()` to take a look at the summary statistics for the data:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "library(skimr)\n",
        "\n",
        "# Obtain Summary statistics\n",
        "seeds %>% \n",
        "  skim()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now take a moment to go through the quick data exploration you just performed. Any missing values? What's the dimension of your data (rows and columns)? What are the different column types? How are the values in your columns distributed?\n",
        "\n",
        "For this module, you'll work with the first six *feature* columns. For plotting purposes, let's encode the *label* column as categorical. Tidymodels provides a neat way of excluding this variable when fitting a model to your data. Remember, you're dealing with unsupervised learning, which doesn't make use of previously known label values to train a model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Narrow down to desired features\n",
        "seeds_select <- seeds %>% \n",
        "  select(!groove_length) %>% \n",
        "  mutate(species = factor(species))\n",
        "\n",
        "# View first 5 rows of the data\n",
        "seeds_select %>% \n",
        "  slice_head(n = 5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, we now have six data points (or *features*) for each instance (*observation*) of a seed's species. So you could interpret these as coordinates that describe each seed's location in six-dimensional space.\n",
        "\n",
        "Now, of course six-dimensional space is difficult to visualize in a three-dimensional world, or on a two-dimensional plot. So you take advantage of a mathematical technique called *principal component analysis* (PCA) to analyze the relationships between the features, and to summarize each observation as coordinates for two principal components. In other words, you translate the six-dimensional feature values into two-dimensional coordinates.\n",
        "\n",
        "> PCA is a dimension reduction method that aims at reducing the feature space. Most of the information or variability in the dataset can then be explained by using fewer uncorrelated features.\n",
        "\n",
        "Let's see this in action by creating a specification of a `recipe` that estimates the principal components based on our six variables. You'll then prep and bake the recipe to apply the computations.\n",
        "\n",
        "> PCA works well when the variables are normalized (*centered* and *scaled*).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the core tidymodels and make it available in your current R session\n",
        "library(tidymodels)\n",
        "\n",
        "\n",
        "# Specify a recipe for pca\n",
        "pca_rec <- recipe(~ ., data = seeds_select) %>% \n",
        "  update_role(species, new_role = \"ID\") %>% \n",
        "  step_normalize(all_predictors()) %>% \n",
        "  step_pca(all_predictors(), num_comp = 2, id = \"pca\")\n",
        "\n",
        "# Print out recipe\n",
        "pca_rec\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compared to supervised learning techniques, there's no `outcome` variable in this recipe.\n",
        "\n",
        "By updating the role of the `species` column to `ID`, you instruct the recipe to keep the variable but not use it as either an outcome or predictor.\n",
        "\n",
        "By calling `prep()`, which estimates the statistics required by PCA, and by applying them to `seeds_select` using `bake(new_data = NULL)`, you can get the fitted PC transformation of our features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Estimate required statistcs \n",
        "pca_estimates <- prep(pca_rec)\n",
        "\n",
        "# Return preprocessed data using bake\n",
        "features_2d <- pca_estimates %>% \n",
        "  bake(new_data = NULL)\n",
        "\n",
        "# Print baked data set\n",
        "features_2d %>% \n",
        "  slice_head(n = 5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These two components capture the maximum amount of information (that is, the variance) in the original variables. From the output of the prepped recipe `pca_estimates`, you can examine how much variance each component accounts for:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine how much variance each PC accounts for\n",
        "pca_estimates %>% \n",
        "  tidy(id = \"pca\", type = \"variance\") %>% \n",
        "  filter(str_detect(terms, \"percent\"))\n",
        "\n",
        "\n",
        "theme_set(theme_light())\n",
        "# Plot how much variance each PC accounts for\n",
        "pca_estimates %>% \n",
        "  tidy(id = \"pca\", type = \"variance\") %>% \n",
        "  filter(terms == \"percent variance\") %>% \n",
        "  ggplot(mapping = aes(x = component, y = value)) +\n",
        "  geom_col(fill = \"midnightblue\", alpha = 0.7) +\n",
        "  ylab(\"% of total variance\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This output's tibbles and plots show how well each principal component is explaining the original six variables. For example, the first principal component (PC1) explains about 72 percent of the variance of the six variables. The second principal component explains an additional 16.97 percent, giving a cumulative percent variance of 89.11. This is certainly better. It means that the first two variables seem to have some power in summarizing the original six variables.\n",
        "\n",
        "Naturally, PC1 captures the most variance, followed by PC2 and PC3.\n",
        "\n",
        "Now that you have the data points translated to two dimensions PC1 and PC2, you can visualize them in a plot:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize PC scores\n",
        "features_2d %>% \n",
        "  ggplot(mapping = aes(x = PC1, y = PC2)) +\n",
        "  geom_point(size = 2, color = \"dodgerblue3\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hopefully you can see at least two, arguably three, reasonably distinct groups of data points. But here lies one of the fundamental problems with clustering: without knowning class labels, how do you know how many clusters to separate your data into?\n",
        "\n",
        "One way is to use a data sample to create a series of clustering models with an incrementing number of clusters. Then you can measure how tightly the data points are grouped within each cluster. A metric often used to measure this tightness is the *within cluster sum of squares* (WCSS), with lower values meaning that the data points are closer. You can then plot the WCSS for each model.\n",
        "\n",
        "You'll use the built-in `kmeans()` function, which accepts a data frame with all numeric columns as its primary argument to perform clustering. This approach means that you'll have to drop the *species* column. For clustering, it is recommended that the data have the same scale. You can use the recipes package to perform these transformations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop target column and normalize data\n",
        "seeds_features<- recipe(~ ., data = seeds_select) %>% \n",
        "  step_rm(species) %>% \n",
        "  step_normalize(all_predictors()) %>% \n",
        "  prep() %>% \n",
        "  bake(new_data = NULL)\n",
        "\n",
        "# Print out data\n",
        "seeds_features %>% \n",
        "  slice_head(n = 5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's explore the WCSS of different numbers of clusters.\n",
        "\n",
        "You use `map()` from the [purrr](https://purrr.tidyverse.org/) package to apply functions to each element in list.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set.seed(2056)\n",
        "# Create 10 models with 1 to 10 clusters\n",
        "kclusts <- tibble(k = 1:10) %>% \n",
        "  mutate(\n",
        "    model = map(k, ~ kmeans(x = seeds_features, centers = .x,\n",
        "                            nstart = 20)),\n",
        "    glanced = map(model, glance)) %>% \n",
        "  unnest(cols = c(glanced))\n",
        "\n",
        "\n",
        "\n",
        "# Plot Total within-cluster sum of squares (tot.withinss)\n",
        "kclusts %>% \n",
        "  ggplot(mapping = aes(x = k, y = tot.withinss)) +\n",
        "  geom_line(size = 1.2, alpha = 0.5, color = \"dodgerblue3\") +\n",
        "  geom_point(size = 2, color = \"dodgerblue3\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The plot shows a large reduction in WCSS (so, greater *tightness*) as the number of clusters increases from one to two, and a further noticeable reduction from two to three clusters. After that, the reduction is less pronounced, resulting in an *elbow* in the chart at around three clusters. This is a good indication that there are two to three reasonably well separated clusters of data points.\n",
        "\n",
        "### Summary\n",
        "\n",
        "In this notebook exercise, you looked at what clustering means, and how to determine whether clustering might be appropriate for your data. In the next notebook, you examine two ways of labeling the data automatically.\n"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": "",
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "codemirror_mode": "r",
      "file_extension": ".r",
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.4.1"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
